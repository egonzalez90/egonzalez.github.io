<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Ceph-ansible baremetal deployment</title>
  <meta name="description" content="How many times you tried to install Ceph? How many fails with no reason? All Ceph operator should agree with me when i say that Ceph installer doesn’t really works as expected so far. Yes, i’m talking about ceph-deploy and the main reason why i’m posting this guide about deploying Ceph with Ansible. At this post, i will show how to install a Ceph cluster with Ansible on baremetal servers. My configuration is as follows: 3 x ceph monitors 8GB of RAM each one 3 x OSD nodes 16GB of RAM and 3x100 GB of Disk 1 x RadosGateway node 8GB of RAM First, download Ceph-Ansible playbooks git clone https://github.com/ceph/ceph-ansible/ Cloning into &#39;ceph-ansible&#39;... remote: Counting objects: 5764, done. remote: Compressing objects: 100% (38/38), done. remote: Total 5764 (delta 7), reused 0 (delta 0), pack-reused 5726 Receiving objects: 100% (5764/5764), 1.12 MiB | 1.06 MiB/s, done. Resolving deltas: 100% (3465/3465), done. Checking connectivity... done. Move to the newly created folder called ceph-ansible cd ceph-ansible/ Copy sample vars files, we will configure our environment in these variable files. cp site.yml.sample site.yml cp group_vars/all.sample group_vars/all cp group_vars/mons.sample group_vars/mons cp group_vars/osds.sample group_vars/osds Next step is configure the inventory with our servers, i don’t really like use /etc/ansible/host file, i prefer create a new file per environment inside playbook’s folder. Create a file with the following content, use you own IPs to match your servers on the desired role inside the cluster [root@ansible ~]# vi inventory_hosts [mons] 192.168.1.48 192.168.1.49 192.168.1.52 [osds] 192.168.1.50 192.168.1.53 192.168.1.54 [rgws] 192.168.1.55 Test connectivity to you servers pinging them through Ansible ping module [root@ansible ~]# ansible -m ping -i inventory_hosts all 192.168.1.48 | success &amp;gt;&amp;gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 192.168.1.50 | success &amp;gt;&amp;gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 192.168.1.55 | success &amp;gt;&amp;gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 192.168.1.53 | success &amp;gt;&amp;gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 192.168.1.49 | success &amp;gt;&amp;gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 192.168.1.54 | success &amp;gt;&amp;gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } 192.168.1.52 | success &amp;gt;&amp;gt; { &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot; } Edit site.yml file, i will remove/comment mds nodes since i’m not going to use them. [root@ansible ~]# vi site.yml - hosts: mons become: True roles: - ceph-mon - hosts: agents become: True roles: - ceph-agent - hosts: osds become: True roles: - ceph-osd #- hosts: mdss # become: True # roles: # - ceph-mds - hosts: rgws become: True roles: - ceph-rgw - hosts: restapis become: True roles: - ceph-restapi Edit main variable file, here we are going to configure our environment [root@ansible ~]# vi group_vars/all Here we configure from where ceph packages are going to be installed, for now we use upstream code with the stable release Infernalis. ## Configure package origin ceph_origin: upstream ceph_stable: true ceph_stable_release: infernalis Configure interface on which monitor will be listening ## Monitor options monitor_interface: eth2 Here we configure some OSD options, like journal size and what networks will be used by public and cluster data replication ## OSD options journal_size: 1024 public_network: 192.168.1.0/24 cluster_network: 192.168.200.0/24 Edit osds variable file [root@ansible ~]# vi group_vars/osds I will use auto discovery option to allow ceph ansible select empy or not used devices in my servers to create OSDs. # Declare devices osd_auto_discovery: True journal_collocation: True Of course you can use other options, i’ll highly suggest you to read variable comments, as they provide valuable information about usage. We’re ready to deploy ceph with ansible with our custom inventory_hosts file. [root@ansible ~]# ansible-playbook site.yml -i inventory_hosts After a while, you will have a fully functional ceph cluster. Maybe you find some issues or bugs when running the playbooks. There is a lot of efforts to fix issues on upstream repository. If a new bug is encountered, please, post a issue right here. https://github.com/ceph/ceph-ansible/issues You can check your cluster status with ceph -s. we can see all OSDs are up and pgs active/clean. [root@ceph-mon1 ~]# ceph -s cluster 5ff692ab-2150-41a4-8b6d-001a4da21c9c health HEALTH_OK monmap e1: 3 mons at {ceph-mon1=192.168.200.141:6789/0,ceph-mon2=192.168.200.180:6789/0,ceph-mon3=192.168.200.232:6789/0} election epoch 6, quorum 0,1,2 ceph-mon1,ceph-mon2,ceph-mon3 osdmap e10: 9 osds: 9 up, 9 in flags sortbitwise pgmap v32: 64 pgs, 1 pools, 0 bytes data, 0 objects 102256 kB used, 896 GB / 896 GB avail 64 active+clean We are going to do some tests. Create a pool [root@ceph-mon1 ~]# ceph osd pool create test 128 128 pool &#39;test&#39; created Create a file big file [root@ceph-mon1 ~]# dd if=/dev/zero of=/tmp/sample.txt bs=2M count=1000 1000+0 records in 1000+0 records out 2097152000 bytes (2.1 GB) copied, 16.7386 s, 125 MB/s Upload the file to rados [root@ceph-mon1 ~]# rados -p test put sample /tmp/sample.txt Check om which placement groups your file is saved [root@ceph-mon1 ~]# ceph osd map test sample osdmap e13 pool &#39;test&#39; (1) object &#39;sample&#39; -&amp;gt; pg 1.bddbf0b9 (1.39) -&amp;gt; up ([1,0], p1) acting ([1,0], p1) Query the placement group where you file was uploaded, a similar output will prompts [root@ceph-mon1 ~]# ceph pg 1.39 query { &quot;state&quot;: &quot;active+clean&quot;, &quot;snap_trimq&quot;: &quot;[]&quot;, &quot;epoch&quot;: 13, &quot;up&quot;: [ 1, 0 ], &quot;acting&quot;: [ 1, 0 ], &quot;actingbackfill&quot;: [ &quot;0&quot;, &quot;1&quot; ], &quot;info&quot;: { &quot;pgid&quot;: &quot;1.39&quot;, &quot;last_update&quot;: &quot;13&#39;500&quot;, &quot;last_complete&quot;: &quot;13&#39;500&quot;, &quot;log_tail&quot;: &quot;0&#39;0&quot;, &quot;last_user_version&quot;: 500, &quot;last_backfill&quot;: &quot;MAX&quot;, &quot;last_backfill_bitwise&quot;: 0, &quot;purged_snaps&quot;: &quot;[]&quot;, &quot;history&quot;: { &quot;epoch_created&quot;: 11, &quot;last_epoch_started&quot;: 12, &quot;last_epoch_clean&quot;: 13, &quot;last_epoch_split&quot;: 0, &quot;last_epoch_marked_full&quot;: 0, &quot;same_up_since&quot;: 11, &quot;same_interval_since&quot;: 11, &quot;same_primary_since&quot;: 11, &quot;last_scrub&quot;: &quot;0&#39;0&quot;, &quot;last_scrub_stamp&quot;: &quot;2016-03-16 21:13:08.883121&quot;, &quot;last_deep_scrub&quot;: &quot;0&#39;0&quot;, &quot;last_deep_scrub_stamp&quot;: &quot;2016-03-16 21:13:08.883121&quot;, &quot;last_clean_scrub_stamp&quot;: &quot;0.000000&quot; }, &quot;stats&quot;: { &quot;version&quot;: &quot;13&#39;500&quot;, &quot;reported_seq&quot;: &quot;505&quot;, &quot;reported_epoch&quot;: &quot;13&quot;, &quot;state&quot;: &quot;active+clean&quot;, &quot;last_fresh&quot;: &quot;2016-03-16 21:24:40.930724&quot;, &quot;last_change&quot;: &quot;2016-03-16 21:14:09.874086&quot;, &quot;last_active&quot;: &quot;2016-03-16 21:24:40.930724&quot;, &quot;last_peered&quot;: &quot;2016-03-16 21:24:40.930724&quot;, &quot;last_clean&quot;: &quot;2016-03-16 21:24:40.930724&quot;, &quot;last_became_active&quot;: &quot;0.000000&quot;, &quot;last_became_peered&quot;: &quot;0.000000&quot;, &quot;last_unstale&quot;: &quot;2016-03-16 21:24:40.930724&quot;, &quot;last_undegraded&quot;: &quot;2016-03-16 21:24:40.930724&quot;, &quot;last_fullsized&quot;: &quot;2016-03-16 21:24:40.930724&quot;, &quot;mapping_epoch&quot;: 11, &quot;log_start&quot;: &quot;0&#39;0&quot;, &quot;ondisk_log_start&quot;: &quot;0&#39;0&quot;, &quot;created&quot;: 11, &quot;last_epoch_clean&quot;: 13, &quot;parent&quot;: &quot;0.0&quot;, &quot;parent_split_bits&quot;: 0, &quot;last_scrub&quot;: &quot;0&#39;0&quot;, &quot;last_scrub_stamp&quot;: &quot;2016-03-16 21:13:08.883121&quot;, &quot;last_deep_scrub&quot;: &quot;0&#39;0&quot;, &quot;last_deep_scrub_stamp&quot;: &quot;2016-03-16 21:13:08.883121&quot;, &quot;last_clean_scrub_stamp&quot;: &quot;0.000000&quot;, &quot;log_size&quot;: 500, &quot;ondisk_log_size&quot;: 500, &quot;stats_invalid&quot;: &quot;0&quot;, &quot;stat_sum&quot;: { &quot;num_bytes&quot;: 2097152000, &quot;num_objects&quot;: 1, &quot;num_object_clones&quot;: 0, &quot;num_object_copies&quot;: 2, &quot;num_objects_missing_on_primary&quot;: 0, &quot;num_objects_degraded&quot;: 0, &quot;num_objects_misplaced&quot;: 0, &quot;num_objects_unfound&quot;: 0, &quot;num_objects_dirty&quot;: 1, &quot;num_whiteouts&quot;: 0, &quot;num_read&quot;: 0, &quot;num_read_kb&quot;: 0, &quot;num_write&quot;: 500, &quot;num_write_kb&quot;: 2048000, &quot;num_scrub_errors&quot;: 0, &quot;num_shallow_scrub_errors&quot;: 0, &quot;num_deep_scrub_errors&quot;: 0, &quot;num_objects_recovered&quot;: 0, &quot;num_bytes_recovered&quot;: 0, &quot;num_keys_recovered&quot;: 0, &quot;num_objects_omap&quot;: 0, &quot;num_objects_hit_set_archive&quot;: 0, &quot;num_bytes_hit_set_archive&quot;: 0, &quot;num_flush&quot;: 0, &quot;num_flush_kb&quot;: 0, &quot;num_evict&quot;: 0, &quot;num_evict_kb&quot;: 0, &quot;num_promote&quot;: 0, &quot;num_flush_mode_high&quot;: 0, &quot;num_flush_mode_low&quot;: 0, &quot;num_evict_mode_some&quot;: 0, &quot;num_evict_mode_full&quot;: 0 }, &quot;up&quot;: [ 1, 0 ], &quot;acting&quot;: [ 1, 0 ], &quot;blocked_by&quot;: [], &quot;up_primary&quot;: 1, &quot;acting_primary&quot;: 1 }, &quot;empty&quot;: 0, &quot;dne&quot;: 0, &quot;incomplete&quot;: 0, &quot;last_epoch_started&quot;: 12, &quot;hit_set_history&quot;: { &quot;current_last_update&quot;: &quot;0&#39;0&quot;, &quot;history&quot;: [] } }, &quot;peer_info&quot;: [ { &quot;peer&quot;: &quot;0&quot;, &quot;pgid&quot;: &quot;1.39&quot;, &quot;last_update&quot;: &quot;13&#39;500&quot;, &quot;last_complete&quot;: &quot;13&#39;500&quot;, &quot;log_tail&quot;: &quot;0&#39;0&quot;, &quot;last_user_version&quot;: 0, &quot;last_backfill&quot;: &quot;MAX&quot;, &quot;last_backfill_bitwise&quot;: 0, &quot;purged_snaps&quot;: &quot;[]&quot;, &quot;history&quot;: { &quot;epoch_created&quot;: 11, &quot;last_epoch_started&quot;: 12, &quot;last_epoch_clean&quot;: 13, &quot;last_epoch_split&quot;: 0, &quot;last_epoch_marked_full&quot;: 0, &quot;same_up_since&quot;: 0, &quot;same_interval_since&quot;: 0, &quot;same_primary_since&quot;: 0, &quot;last_scrub&quot;: &quot;0&#39;0&quot;, &quot;last_scrub_stamp&quot;: &quot;2016-03-16 21:13:08.883121&quot;, &quot;last_deep_scrub&quot;: &quot;0&#39;0&quot;, &quot;last_deep_scrub_stamp&quot;: &quot;2016-03-16 21:13:08.883121&quot;, &quot;last_clean_scrub_stamp&quot;: &quot;0.000000&quot; }, &quot;stats&quot;: { &quot;version&quot;: &quot;0&#39;0&quot;, &quot;reported_seq&quot;: &quot;0&quot;, &quot;reported_epoch&quot;: &quot;0&quot;, &quot;state&quot;: &quot;inactive&quot;, &quot;last_fresh&quot;: &quot;0.000000&quot;, &quot;last_change&quot;: &quot;0.000000&quot;, &quot;last_active&quot;: &quot;0.000000&quot;, &quot;last_peered&quot;: &quot;0.000000&quot;, &quot;last_clean&quot;: &quot;0.000000&quot;, &quot;last_became_active&quot;: &quot;0.000000&quot;, &quot;last_became_peered&quot;: &quot;0.000000&quot;, &quot;last_unstale&quot;: &quot;0.000000&quot;, &quot;last_undegraded&quot;: &quot;0.000000&quot;, &quot;last_fullsized&quot;: &quot;0.000000&quot;, &quot;mapping_epoch&quot;: 0, &quot;log_start&quot;: &quot;0&#39;0&quot;, &quot;ondisk_log_start&quot;: &quot;0&#39;0&quot;, &quot;created&quot;: 0, &quot;last_epoch_clean&quot;: 0, &quot;parent&quot;: &quot;0.0&quot;, &quot;parent_split_bits&quot;: 0, &quot;last_scrub&quot;: &quot;0&#39;0&quot;, &quot;last_scrub_stamp&quot;: &quot;0.000000&quot;, &quot;last_deep_scrub&quot;: &quot;0&#39;0&quot;, &quot;last_deep_scrub_stamp&quot;: &quot;0.000000&quot;, &quot;last_clean_scrub_stamp&quot;: &quot;0.000000&quot;, &quot;log_size&quot;: 0, &quot;ondisk_log_size&quot;: 0, &quot;stats_invalid&quot;: &quot;0&quot;, &quot;stat_sum&quot;: { &quot;num_bytes&quot;: 0, &quot;num_objects&quot;: 0, &quot;num_object_clones&quot;: 0, &quot;num_object_copies&quot;: 0, &quot;num_objects_missing_on_primary&quot;: 0, &quot;num_objects_degraded&quot;: 0, &quot;num_objects_misplaced&quot;: 0, &quot;num_objects_unfound&quot;: 0, &quot;num_objects_dirty&quot;: 0, &quot;num_whiteouts&quot;: 0, &quot;num_read&quot;: 0, &quot;num_read_kb&quot;: 0, &quot;num_write&quot;: 0, &quot;num_write_kb&quot;: 0, &quot;num_scrub_errors&quot;: 0, &quot;num_shallow_scrub_errors&quot;: 0, &quot;num_deep_scrub_errors&quot;: 0, &quot;num_objects_recovered&quot;: 0, &quot;num_bytes_recovered&quot;: 0, &quot;num_keys_recovered&quot;: 0, &quot;num_objects_omap&quot;: 0, &quot;num_objects_hit_set_archive&quot;: 0, &quot;num_bytes_hit_set_archive&quot;: 0, &quot;num_flush&quot;: 0, &quot;num_flush_kb&quot;: 0, &quot;num_evict&quot;: 0, &quot;num_evict_kb&quot;: 0, &quot;num_promote&quot;: 0, &quot;num_flush_mode_high&quot;: 0, &quot;num_flush_mode_low&quot;: 0, &quot;num_evict_mode_some&quot;: 0, &quot;num_evict_mode_full&quot;: 0 }, &quot;up&quot;: [], &quot;acting&quot;: [], &quot;blocked_by&quot;: [], &quot;up_primary&quot;: -1, &quot;acting_primary&quot;: -1 }, &quot;empty&quot;: 0, &quot;dne&quot;: 0, &quot;incomplete&quot;: 0, &quot;last_epoch_started&quot;: 12, &quot;hit_set_history&quot;: { &quot;current_last_update&quot;: &quot;0&#39;0&quot;, &quot;history&quot;: [] } } ], &quot;recovery_state&quot;: [ { &quot;name&quot;: &quot;Started\/Primary\/Active&quot;, &quot;enter_time&quot;: &quot;2016-03-16 21:13:36.769083&quot;, &quot;might_have_unfound&quot;: [], &quot;recovery_progress&quot;: { &quot;backfill_targets&quot;: [], &quot;waiting_on_backfill&quot;: [], &quot;last_backfill_started&quot;: &quot;MIN&quot;, &quot;backfill_info&quot;: { &quot;begin&quot;: &quot;MIN&quot;, &quot;end&quot;: &quot;MIN&quot;, &quot;objects&quot;: [] }, &quot;peer_backfill_info&quot;: [], &quot;backfills_in_flight&quot;: [], &quot;recovering&quot;: [], &quot;pg_backend&quot;: { &quot;pull_from_peer&quot;: [], &quot;pushing&quot;: [] } }, &quot;scrub&quot;: { &quot;scrubber.epoch_start&quot;: &quot;0&quot;, &quot;scrubber.active&quot;: 0, &quot;scrubber.waiting_on&quot;: 0, &quot;scrubber.waiting_on_whom&quot;: [] } }, { &quot;name&quot;: &quot;Started&quot;, &quot;enter_time&quot;: &quot;2016-03-16 21:13:09.216260&quot; } ], &quot;agent_state&quot;: {} } That’s all for now. Regards, Eduardo Gonzalez">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://217.182.136.201:8080/ceph-ansible-baremetal-deployment/">
  
  
  <link rel="alternate" type="application/rss+xml" title="OpenStack things" href="http://217.182.136.201:8080/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="egongu90">
  <meta name="twitter:title" content="Ceph-ansible baremetal deployment">
  <meta name="twitter:description" content="How many times you tried to install Ceph? How many fails with no reason? All Ceph operator should agree with me when i say that Ceph installer doesn’t really works as expected so far. Yes, i’m talk...">
  
    <meta name="twitter:creator" content="egongu90">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">OpenStack things</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/yous/whiteglass">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
      <h1 class="post-title" itemprop="name headline">Ceph-ansible baremetal deployment</h1>
    
    <p class="post-meta"><time datetime="2016-03-17T23:47:17+01:00" itemprop="datePublished">Mar 17, 2016</time> • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Editor</span></span> • 
  
  
    
      <a href="/categories/linux/">Linux</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
      <a href="/categories/various/">Various</a>
    
  
    
  
    
  
    
  
    
  

</p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>How many times you tried to install Ceph? How many fails with no reason?
All Ceph operator should agree with me when i say that Ceph installer doesn’t really works as expected so far.
Yes, i’m talking about ceph-deploy and the main reason why i’m posting this guide about deploying Ceph with Ansible.</p>

<p>At this post, i will show how to install a Ceph cluster with Ansible on baremetal servers.
My configuration is as follows:</p>
<ol>
<li type="disc">3 x ceph monitors 8GB of RAM each one</li>
<li type="disc">3 x OSD nodes 16GB of RAM and 3x100 GB of Disk</li>
<li type="disc">1 x RadosGateway node 8GB of RAM</li></ol>

<p>First, download Ceph-Ansible playbooks</p>
<pre>
git clone https://github.com/ceph/ceph-ansible/
Cloning into 'ceph-ansible'...
remote: Counting objects: 5764, done.
remote: Compressing objects: 100% (38/38), done.
remote: Total 5764 (delta 7), reused 0 (delta 0), pack-reused 5726
Receiving objects: 100% (5764/5764), 1.12 MiB | 1.06 MiB/s, done.
Resolving deltas: 100% (3465/3465), done.
Checking connectivity... done.
</pre>
<p>Move to the newly created folder called ceph-ansible</p>
<pre>
cd ceph-ansible/
</pre>
<p>Copy sample vars files, we will configure our environment in these variable files.</p>
<pre>
cp site.yml.sample site.yml
cp group_vars/all.sample group_vars/all
cp group_vars/mons.sample group_vars/mons
cp group_vars/osds.sample group_vars/osds
</pre>
<p>Next step is configure the inventory with our servers, i don’t really like use /etc/ansible/host file, i prefer create a new file per environment inside playbook’s folder.</p>

<p>Create a file with the following content, use you own IPs to match your servers on the desired role inside the cluster</p>
<pre>
[root@ansible ~]# vi inventory_hosts

[mons]
192.168.1.48
192.168.1.49
192.168.1.52

[osds]
192.168.1.50
192.168.1.53
192.168.1.54

[rgws]
192.168.1.55

</pre>
<p>Test connectivity to you servers pinging them through Ansible ping module</p>
<pre>
[root@ansible ~]# ansible -m ping -i inventory_hosts all
192.168.1.48 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.50 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.55 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.53 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.49 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.54 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.52 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}
</pre>
<p>Edit site.yml file, i will remove/comment mds nodes since i’m not going to use them.</p>
<pre>
[root@ansible ~]# vi site.yml

- hosts: mons
  become: True
  roles:
  - ceph-mon

- hosts: agents
  become: True
  roles:
  - ceph-agent

- hosts: osds
  become: True
  roles:
  - ceph-osd

#- hosts: mdss
#  become: True
#  roles:
#  - ceph-mds

- hosts: rgws
  become: True
  roles:
  - ceph-rgw

- hosts: restapis
  become: True
  roles:
  - ceph-restapi

</pre>
<p>Edit main variable file, here we are going to configure our environment</p>
<pre>
[root@ansible ~]# vi group_vars/all
</pre>
<p>Here we configure from where ceph packages are going to be installed, for now we use upstream code with the stable release Infernalis.</p>
<pre>
## Configure package origin
ceph_origin: upstream
ceph_stable: true
ceph_stable_release: infernalis
</pre>
<p>Configure interface on which monitor will be listening</p>
<pre>
## Monitor options
monitor_interface: eth2
</pre>
<p>Here we configure some OSD options, like journal size and what networks will be used by public and cluster data replication</p>
<pre>
## OSD options
journal_size: 1024
public_network: 192.168.1.0/24
cluster_network: 192.168.200.0/24
</pre>
<p>Edit osds variable file</p>
<pre>
[root@ansible ~]# vi group_vars/osds
</pre>
<p>I will use auto discovery option to allow ceph ansible select empy or not used devices in my servers to create OSDs.</p>
<pre>
# Declare devices
osd_auto_discovery: True
journal_collocation: True
</pre>
<p>Of course you can use other options, i’ll highly suggest you to read variable comments, as they provide valuable information about usage.
We’re ready to deploy  ceph with ansible with our custom inventory_hosts file.</p>
<pre>
[root@ansible ~]# ansible-playbook site.yml -i inventory_hosts
</pre>

<p>After a while, you will have a fully functional ceph cluster.</p>

<p>Maybe you find some issues or bugs when running the playbooks. 
There is a lot of efforts to fix issues on upstream repository. If a new bug is encountered, please, post a issue right here.
https://github.com/ceph/ceph-ansible/issues</p>

<p>You can check your cluster status with ceph -s. we can see all OSDs are up and pgs active/clean.</p>
<pre>
[root@ceph-mon1 ~]# ceph -s
    cluster 5ff692ab-2150-41a4-8b6d-001a4da21c9c
     health HEALTH_OK
     monmap e1: 3 mons at {ceph-mon1=192.168.200.141:6789/0,ceph-mon2=192.168.200.180:6789/0,ceph-mon3=192.168.200.232:6789/0}
            election epoch 6, quorum 0,1,2 ceph-mon1,ceph-mon2,ceph-mon3
     osdmap e10: 9 osds: 9 up, 9 in
            flags sortbitwise
      pgmap v32: 64 pgs, 1 pools, 0 bytes data, 0 objects
            102256 kB used, 896 GB / 896 GB avail
                  64 active+clean
</pre>
<p>We are going to do some tests.
Create a pool</p>
<pre>
[root@ceph-mon1 ~]# ceph osd pool create test 128 128
pool 'test' created
</pre>
<p>Create a file big file</p>
<pre>
[root@ceph-mon1 ~]# dd if=/dev/zero of=/tmp/sample.txt bs=2M count=1000
1000+0 records in
1000+0 records out
2097152000 bytes (2.1 GB) copied, 16.7386 s, 125 MB/s
</pre>
<p>Upload the file to rados</p>
<pre>
[root@ceph-mon1 ~]# rados -p test put sample /tmp/sample.txt 
</pre>
<p>Check om which placement groups your file is saved</p>
<pre>
[root@ceph-mon1 ~]# ceph osd map test sample
osdmap e13 pool 'test' (1) object 'sample' -&gt; pg 1.bddbf0b9 (1.39) -&gt; up ([1,0], p1) acting ([1,0], p1)
</pre>
<p>Query the placement group where you file was uploaded, a similar output will prompts</p>
<pre>
[root@ceph-mon1 ~]# ceph pg 1.39 query
{
    "state": "active+clean",
    "snap_trimq": "[]",
    "epoch": 13,
    "up": [
        1,
        0
    ],
    "acting": [
        1,
        0
    ],
    "actingbackfill": [
        "0",
        "1"
    ],
    "info": {
        "pgid": "1.39",
        "last_update": "13'500",
        "last_complete": "13'500",
        "log_tail": "0'0",
        "last_user_version": 500,
        "last_backfill": "MAX",
        "last_backfill_bitwise": 0,
        "purged_snaps": "[]",
        "history": {
            "epoch_created": 11,
            "last_epoch_started": 12,
            "last_epoch_clean": 13,
            "last_epoch_split": 0,
            "last_epoch_marked_full": 0,
            "same_up_since": 11,
            "same_interval_since": 11,
            "same_primary_since": 11,
            "last_scrub": "0'0",
            "last_scrub_stamp": "2016-03-16 21:13:08.883121",
            "last_deep_scrub": "0'0",
            "last_deep_scrub_stamp": "2016-03-16 21:13:08.883121",
            "last_clean_scrub_stamp": "0.000000"
        },
        "stats": {
            "version": "13'500",
            "reported_seq": "505",
            "reported_epoch": "13",
            "state": "active+clean",
            "last_fresh": "2016-03-16 21:24:40.930724",
            "last_change": "2016-03-16 21:14:09.874086",
            "last_active": "2016-03-16 21:24:40.930724",
            "last_peered": "2016-03-16 21:24:40.930724",
            "last_clean": "2016-03-16 21:24:40.930724",
            "last_became_active": "0.000000",
            "last_became_peered": "0.000000",
            "last_unstale": "2016-03-16 21:24:40.930724",
            "last_undegraded": "2016-03-16 21:24:40.930724",
            "last_fullsized": "2016-03-16 21:24:40.930724",
            "mapping_epoch": 11,
            "log_start": "0'0",
            "ondisk_log_start": "0'0",
            "created": 11,
            "last_epoch_clean": 13,
            "parent": "0.0",
            "parent_split_bits": 0,
            "last_scrub": "0'0",
            "last_scrub_stamp": "2016-03-16 21:13:08.883121",
            "last_deep_scrub": "0'0",
            "last_deep_scrub_stamp": "2016-03-16 21:13:08.883121",
            "last_clean_scrub_stamp": "0.000000",
            "log_size": 500,
            "ondisk_log_size": 500,
            "stats_invalid": "0",
            "stat_sum": {
                "num_bytes": 2097152000,
                "num_objects": 1,
                "num_object_clones": 0,
                "num_object_copies": 2,
                "num_objects_missing_on_primary": 0,
                "num_objects_degraded": 0,
                "num_objects_misplaced": 0,
                "num_objects_unfound": 0,
                "num_objects_dirty": 1,
                "num_whiteouts": 0,
                "num_read": 0,
                "num_read_kb": 0,
                "num_write": 500,
                "num_write_kb": 2048000,
                "num_scrub_errors": 0,
                "num_shallow_scrub_errors": 0,
                "num_deep_scrub_errors": 0,
                "num_objects_recovered": 0,
                "num_bytes_recovered": 0,
                "num_keys_recovered": 0,
                "num_objects_omap": 0,
                "num_objects_hit_set_archive": 0,
                "num_bytes_hit_set_archive": 0,
                "num_flush": 0,
                "num_flush_kb": 0,
                "num_evict": 0,
                "num_evict_kb": 0,
                "num_promote": 0,
                "num_flush_mode_high": 0,
                "num_flush_mode_low": 0,
                "num_evict_mode_some": 0,
                "num_evict_mode_full": 0
            },
            "up": [
                1,
                0
            ],
            "acting": [
                1,
                0
            ],
            "blocked_by": [],
            "up_primary": 1,
            "acting_primary": 1
        },
        "empty": 0,
        "dne": 0,
        "incomplete": 0,
        "last_epoch_started": 12,
        "hit_set_history": {
            "current_last_update": "0'0",
            "history": []
        }
    },
    "peer_info": [
        {
            "peer": "0",
            "pgid": "1.39",
            "last_update": "13'500",
            "last_complete": "13'500",
            "log_tail": "0'0",
            "last_user_version": 0,
            "last_backfill": "MAX",
            "last_backfill_bitwise": 0,
            "purged_snaps": "[]",
            "history": {
                "epoch_created": 11,
                "last_epoch_started": 12,
                "last_epoch_clean": 13,
                "last_epoch_split": 0,
                "last_epoch_marked_full": 0,
                "same_up_since": 0,
                "same_interval_since": 0,
                "same_primary_since": 0,
                "last_scrub": "0'0",
                "last_scrub_stamp": "2016-03-16 21:13:08.883121",
                "last_deep_scrub": "0'0",
                "last_deep_scrub_stamp": "2016-03-16 21:13:08.883121",
                "last_clean_scrub_stamp": "0.000000"
            },
            "stats": {
                "version": "0'0",
                "reported_seq": "0",
                "reported_epoch": "0",
                "state": "inactive",
                "last_fresh": "0.000000",
                "last_change": "0.000000",
                "last_active": "0.000000",
                "last_peered": "0.000000",
                "last_clean": "0.000000",
                "last_became_active": "0.000000",
                "last_became_peered": "0.000000",
                "last_unstale": "0.000000",
                "last_undegraded": "0.000000",
                "last_fullsized": "0.000000",
                "mapping_epoch": 0,
                "log_start": "0'0",
                "ondisk_log_start": "0'0",
                "created": 0,
                "last_epoch_clean": 0,
                "parent": "0.0",
                "parent_split_bits": 0,
                "last_scrub": "0'0",
                "last_scrub_stamp": "0.000000",
                "last_deep_scrub": "0'0",
                "last_deep_scrub_stamp": "0.000000",
                "last_clean_scrub_stamp": "0.000000",
                "log_size": 0,
                "ondisk_log_size": 0,
                "stats_invalid": "0",
                "stat_sum": {
                    "num_bytes": 0,
                    "num_objects": 0,
                    "num_object_clones": 0,
                    "num_object_copies": 0,
                    "num_objects_missing_on_primary": 0,
                    "num_objects_degraded": 0,
                    "num_objects_misplaced": 0,
                    "num_objects_unfound": 0,
                    "num_objects_dirty": 0,
                    "num_whiteouts": 0,
                    "num_read": 0,
                    "num_read_kb": 0,
                    "num_write": 0,
                    "num_write_kb": 0,
                    "num_scrub_errors": 0,
                    "num_shallow_scrub_errors": 0,
                    "num_deep_scrub_errors": 0,
                    "num_objects_recovered": 0,
                    "num_bytes_recovered": 0,
                    "num_keys_recovered": 0,
                    "num_objects_omap": 0,
                    "num_objects_hit_set_archive": 0,
                    "num_bytes_hit_set_archive": 0,
                    "num_flush": 0,
                    "num_flush_kb": 0,
                    "num_evict": 0,
                    "num_evict_kb": 0,
                    "num_promote": 0,
                    "num_flush_mode_high": 0,
                    "num_flush_mode_low": 0,
                    "num_evict_mode_some": 0,
                    "num_evict_mode_full": 0
                },
                "up": [],
                "acting": [],
                "blocked_by": [],
                "up_primary": -1,
                "acting_primary": -1
            },
            "empty": 0,
            "dne": 0,
            "incomplete": 0,
            "last_epoch_started": 12,
            "hit_set_history": {
                "current_last_update": "0'0",
                "history": []
            }
        }
    ],
    "recovery_state": [
        {
            "name": "Started\/Primary\/Active",
            "enter_time": "2016-03-16 21:13:36.769083",
            "might_have_unfound": [],
            "recovery_progress": {
                "backfill_targets": [],
                "waiting_on_backfill": [],
                "last_backfill_started": "MIN",
                "backfill_info": {
                    "begin": "MIN",
                    "end": "MIN",
                    "objects": []
                },
                "peer_backfill_info": [],
                "backfills_in_flight": [],
                "recovering": [],
                "pg_backend": {
                    "pull_from_peer": [],
                    "pushing": []
                }
            },
            "scrub": {
                "scrubber.epoch_start": "0",
                "scrubber.active": 0,
                "scrubber.waiting_on": 0,
                "scrubber.waiting_on_whom": []
            }
        },
        {
            "name": "Started",
            "enter_time": "2016-03-16 21:13:09.216260"
        }
    ],
    "agent_state": {}
}
</pre>

<p>That’s all for now.
Regards, Eduardo Gonzalez</p>


  </div>

</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy;  - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://217.182.136.201:8080/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
