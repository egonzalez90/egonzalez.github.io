<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://217.182.136.201:5000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://217.182.136.201:5000/" rel="alternate" type="text/html" /><updated>2017-08-25T12:57:00+00:00</updated><id>http://217.182.136.201:5000/</id><title type="html">OpenStack things</title><subtitle>OpenStack, Docker, Ansible, Ceph, Linux</subtitle><entry><title type="html">Welcome to Jekyll!</title><link href="http://217.182.136.201:5000/jekyll/update/2017/08/25/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2017-08-25T10:03:33+00:00</published><updated>2017-08-25T10:03:33+00:00</updated><id>http://217.182.136.201:5000/jekyll/update/2017/08/25/welcome-to-jekyll</id><content type="html" xml:base="http://217.182.136.201:5000/jekyll/update/2017/08/25/welcome-to-jekyll.html">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry><entry><title type="html">Deploy OpenStack Designate with Kolla</title><link href="http://217.182.136.201:5000/deploy-openstack-designate-with-kolla-ansible/" rel="alternate" type="text/html" title="Deploy OpenStack Designate with Kolla" /><published>2017-02-22T15:39:03+00:00</published><updated>2017-02-22T15:39:03+00:00</updated><id>http://217.182.136.201:5000/deploy-openstack-designate-with-kolla-ansible</id><content type="html" xml:base="http://217.182.136.201:5000/deploy-openstack-designate-with-kolla-ansible/">&lt;p&gt;During Ocata release, OpenStack DNS-as-a-Service (Designate) support was implemented in OpenStack kolla project.&lt;/p&gt;

&lt;p&gt;This post will guide you through a basic deployment and tests of designate service.&lt;/p&gt;

&lt;p&gt;Install required dependencies and tools for kolla-ansible and designate.&lt;/p&gt;
&lt;pre&gt;
# yum install -y epel-release
# yum install -y python-pip python-devel libffi-devel gcc openssl-devel ansible ntp wget bind-utils
# pip install -U pip
&lt;/pre&gt;
&lt;p&gt;Install Docker and downgrade to 1.12.6. At the time of writing this post libvirt had issues to connect with D-Bus due SElinux issues with Docker 1.13.&lt;/p&gt;
&lt;pre&gt;
# curl -sSL https://get.docker.io | bash
# yum downgrade docker-engine-1.12.6 docker-engine-selinux-1.12.6
# yum install -y python-docker-py
&lt;/pre&gt;
&lt;p&gt;Configure Docker daemon to allow insecure-registry (Use the IP where your remote registry will be located).&lt;/p&gt;
&lt;pre&gt;
# mkdir -p /etc/systemd/system/docker.service.d
# tee /etc/systemd/system/docker.service.d/kolla.conf &amp;lt;&amp;lt;-'EOF'
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd --insecure-registry 172.28.128.3:4000
MountFlags=shared
EOF
&lt;/pre&gt;
&lt;p&gt;Reload systemd daemons and start/stop/disable/enable the following services.&lt;/p&gt;
&lt;pre&gt;
# systemctl daemon-reload
# systemctl stop libvirtd
# systemctl disable libvirtd
# systemctl enable ntpd docker
# systemctl start ntpd docker
&lt;/pre&gt;
&lt;p&gt;Download Ocata registry created in tarballs.openstack.org, skip this step if images used are custom builds or downloaded from DockerHub.
Create kolla registry from downloaded tarball.&lt;/p&gt;
&lt;pre&gt;
# wget https://tarballs.openstack.org/kolla/images/centos-binary-registry-ocata.tar.gz
# mkdir /opt/kolla_registry
# sudo tar xzf centos-binary-registry-ocata.tar.gz -C /opt/kolla_registry
# docker run -d -p 4000:5000 --restart=always -v /opt/kolla_registry/:/var/lib/registry --name registry registry:2
&lt;/pre&gt;
&lt;p&gt;Install kolla-ansible.&lt;/p&gt;
&lt;pre&gt;
# pip install kolla-ansible
# cp -r /usr/share/kolla-ansible/etc_examples/kolla /etc/kolla/
# cp /usr/share/kolla-ansible/ansible/inventory/* .
&lt;/pre&gt;
&lt;p&gt;Configure kolla globals.yml configuration file with the following content.
Change values when necessary (IP addresses, interface names).
This is a sample minimal configuration.&lt;/p&gt;
&lt;pre&gt;
# vi /etc/kolla/globals.yml
---
kolla_internal_vip_address: &quot;172.28.128.10&quot;
kolla_base_distro: &quot;centos&quot;
kolla_install_type: &quot;binary&quot;
docker_registry: &quot;172.28.128.3:4000&quot;
docker_namespace: &quot;lokolla&quot;
network_interface: &quot;enp0s8&quot;
neutron_external_interface: &quot;enp0s9&quot;
&lt;/pre&gt;
&lt;p&gt;Configure designate options in globals.yml.
dns_interface must be network reachable from nova instances if internal DNS resolution is needed.&lt;/p&gt;
&lt;pre&gt;
enable_designate: &quot;yes&quot;
dns_interface: &quot;enp0s8&quot;
designate_backend: &quot;bind9&quot;
designate_ns_record: &quot;sample.openstack.org&quot;
&lt;/pre&gt;
&lt;p&gt;Configure inventory, add the nodes in their respective groups.&lt;/p&gt;
&lt;pre&gt;
# vi ~/multinode
&lt;/pre&gt;
&lt;p&gt;Generate passwords.&lt;/p&gt;
&lt;pre&gt;
# kolla-genpwd
&lt;/pre&gt;
&lt;p&gt;Ensure the environment is ready to deploy with prechecks.
Until prechecks does not succeed do not start deployment.
Fix what is necessary.&lt;/p&gt;
&lt;pre&gt;
# kolla-ansible prechecks -i ~/multinode
&lt;/pre&gt;
&lt;p&gt;Pull Docker images on the servers, this can be skipped because will be made in deploy step, but doing it first will ensure all the nodes have the images you need and will minimize the deployment time.&lt;/p&gt;
&lt;pre&gt;
# kolla-ansible pull -i ~/multinode
&lt;/pre&gt;
&lt;p&gt;Deploy kolla-ansible and do a woot for kolla ;)&lt;/p&gt;
&lt;pre&gt;
# kolla-ansible deploy -i ~/multinode
&lt;/pre&gt;
&lt;p&gt;Create credentials file and source it.&lt;/p&gt;
&lt;pre&gt;
# kolla-ansible post-deploy -i ~/multinode
# source /etc/kolla/admin-openrc.sh
&lt;/pre&gt;
&lt;p&gt;Check that all containers are running and none of them  are restarting or exiting.&lt;/p&gt;
&lt;pre&gt;
# docker ps -a --filter status=exited --filter status=restarting
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
&lt;/pre&gt;
&lt;p&gt;Install required python clients&lt;/p&gt;
&lt;pre&gt;
# pip install python-openstackclient python-designateclient python-neutronclient
&lt;/pre&gt;
&lt;p&gt;Execute a base OpenStack configuration (public and internal networks, cirros image).
Do no execute this script if custom networks are going to be used.&lt;/p&gt;
&lt;pre&gt;
# sh /usr/share/kolla-ansible/init-runonce
&lt;/pre&gt;
&lt;p&gt;Create a sample designate zone.&lt;/p&gt;
&lt;pre&gt;
# openstack zone create --email admin@sample.openstack.org sample.openstack.org.
+----------------+--------------------------------------+
| Field          | Value                                |
+----------------+--------------------------------------+
| action         | CREATE                               |
| attributes     |                                      |
| created_at     | 2017-02-22T13:14:39.000000           |
| description    | None                                 |
| email          | admin@sample.openstack.org           |
| id             | 4a44b0c9-bd07-4f5c-8908-523f453f269d |
| masters        |                                      |
| name           | sample.openstack.org.                |
| pool_id        | 85d18aec-453e-45ae-9eb3-748841a1da12 |
| project_id     | 937d49af6cfe4ef080a79f9a833d7c7d     |
| serial         | 1487769279                           |
| status         | PENDING                              |
| transferred_at | None                                 |
| ttl            | 3600                                 |
| type           | PRIMARY                              |
| updated_at     | None                                 |
| version        | 1                                    |
+----------------+--------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Configure designate sink to make use of the previously created zone, sink will need zone_id to automatically create neutron and nova records into designate.&lt;/p&gt;
&lt;pre&gt;
# mkdir -p /etc/kolla/config/designate/designate-sink/
# vi /etc/kolla/config/designate/designate-sink.conf
[handler:nova_fixed]
zone_id = 4a44b0c9-bd07-4f5c-8908-523f453f269d
[handler:neutron_floatingip]
zone_id = 4a44b0c9-bd07-4f5c-8908-523f453f269d
&lt;/pre&gt;
&lt;p&gt;After configure designate-sink.conf, reconfigure designate to make use of this configuration.&lt;/p&gt;
&lt;pre&gt;
# kolla-ansible reconfigure -i ~/multinode --tags designate
&lt;/pre&gt;
&lt;p&gt;List networks.&lt;/p&gt;
&lt;pre&gt;
# neutron net-list
+--------------------------------------+----------+----------------------------------+--------------------------------------------------+
| id                                   | name     | tenant_id                        | subnets                                          |
+--------------------------------------+----------+----------------------------------+--------------------------------------------------+
| 3b56c605-5a01-45be-9ed6-e4c3285e4366 | demo-net | 937d49af6cfe4ef080a79f9a833d7c7d | 7f28f050-77b2-426e-b963-35b682077993 10.0.0.0/24 |
| 6954d495-fb8c-4b0b-98a9-9672a7f65b7c | public1  | 937d49af6cfe4ef080a79f9a833d7c7d | 9bd9feca-40a7-4e82-b912-e51b726ad746 10.0.2.0/24 |
+--------------------------------------+----------+----------------------------------+--------------------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Update the network with a dns_domain.&lt;/p&gt;
&lt;pre&gt;
# neutron net-update 3b56c605-5a01-45be-9ed6-e4c3285e4366 --dns_domain sample.openstack.org.
Updated network: 3b56c605-5a01-45be-9ed6-e4c3285e4366
&lt;/pre&gt;
&lt;p&gt;Ensure dns_domain is properly applied.&lt;/p&gt;
&lt;pre&gt;
# neutron net-show 3b56c605-5a01-45be-9ed6-e4c3285e4366
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| availability_zone_hints   |                                      |
| availability_zones        | nova                                 |
| created_at                | 2017-02-22T13:13:06Z                 |
| description               |                                      |
| dns_domain                | sample.openstack.org.                |
| id                        | 3b56c605-5a01-45be-9ed6-e4c3285e4366 |
| ipv4_address_scope        |                                      |
| ipv6_address_scope        |                                      |
| mtu                       | 1450                                 |
| name                      | demo-net                             |
| port_security_enabled     | True                                 |
| project_id                | 937d49af6cfe4ef080a79f9a833d7c7d     |
| provider:network_type     | vxlan                                |
| provider:physical_network |                                      |
| provider:segmentation_id  | 27                                   |
| revision_number           | 6                                    |
| router:external           | False                                |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   | 7f28f050-77b2-426e-b963-35b682077993 |
| tags                      |                                      |
| tenant_id                 | 937d49af6cfe4ef080a79f9a833d7c7d     |
| updated_at                | 2017-02-22T13:25:16Z                 |
+---------------------------+--------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Create a new instance in the previously updated network.&lt;/p&gt;
&lt;pre&gt;
# openstack server create \
    --image cirros \
    --flavor m1.tiny \
    --key-name mykey \
    --nic net-id=3b56c605-5a01-45be-9ed6-e4c3285e4366 \
    demo1
&lt;/pre&gt;
&lt;p&gt;Once the instance is ACTIVE, check the IP associated.&lt;/p&gt;
&lt;pre&gt;
# openstack server list
+--------------------------------------+-------+--------+-------------------+------------+
| ID                                   | Name  | Status | Networks          | Image Name |
+--------------------------------------+-------+--------+-------------------+------------+
| d483e4ee-58c2-4e1e-9384-85174630428e | demo1 | ACTIVE | demo-net=10.0.0.3 | cirros     |
+--------------------------------------+-------+--------+-------------------+------------+
&lt;/pre&gt;
&lt;p&gt;List records in the designate zone.
As you can see there is a record in designate associated with the instance IP.&lt;/p&gt;
&lt;pre&gt;
# openstack recordset list sample.openstack.org.
+--------------------------------------+----------------------------------+------+-------------------------------------------+--------+--------+
| id                                   | name                             | type | records                                   | status | action |
+--------------------------------------+----------------------------------+------+-------------------------------------------+--------+--------+
| 4f70531e-c325-4ffd-a8d3-8172bd5163b8 | sample.openstack.org.            | SOA  | sample.openstack.org.                     | ACTIVE | NONE   |
|                                      |                                  |      | admin.sample.openstack.org. 1487770304    |        |        |
|                                      |                                  |      | 3586 600 86400 3600                       |        |        |
| a9a09c5f-ccf1-4b52-8400-f36e8faa9549 | sample.openstack.org.            | NS   | sample.openstack.org.                     | ACTIVE | NONE   |
| aa6cd25d-186e-425b-9153-699d8b0811de | 10-0-0-3.sample.openstack.org.   | A    | 10.0.0.3                                  | ACTIVE | NONE   |
| 713650a5-a45e-470b-9539-74e110b15115 | demo1.None.sample.openstack.org. | A    | 10.0.0.3                                  | ACTIVE | NONE   |
| 6506e6f6-f535-45eb-9bfb-4ac1f16c5c9b | demo1.sample.openstack.org.      | A    | 10.0.0.3                                  | ACTIVE | NONE   |
+--------------------------------------+----------------------------------+------+-------------------------------------------+--------+--------+
&lt;/pre&gt;
&lt;p&gt;Validate that designate resolves the DNS record.
You can use designate mDNS service or directly to bind9 servers to validate the test.&lt;/p&gt;
&lt;pre&gt;
# dig +short -p 5354 @172.28.128.3 demo1.sample.openstack.org. A
10.0.0.3
# dig +short -p 53 @172.28.128.3 demo1.sample.openstack.org. A
10.0.0.3
&lt;/pre&gt;
&lt;p&gt;If you find any issue with designate in kolla-ansible or kolla, please fill a bug &lt;a href=&quot;https://bugs.launchpad.net/kolla-ansible/+filebug&quot;&gt;https://bugs.launchpad.net/kolla-ansible/+filebug&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Regards,
Eduardo Gonzalez&lt;/p&gt;</content><author><name>Editor</name></author><category term="--configure" /><category term="ansible" /><category term="deploy" /><category term="designate" /><category term="DNS" /><category term="kolla" /><category term="kolla-ansible" /><category term="ocata" /><category term="openstack" /><summary type="html">During Ocata release, OpenStack DNS-as-a-Service (Designate) support was implemented in OpenStack kolla project. This post will guide you through a basic deployment and tests of designate service. Install required dependencies and tools for kolla-ansible and designate. # yum install -y epel-release # yum install -y python-pip python-devel libffi-devel gcc openssl-devel ansible ntp wget bind-utils # pip install -U pip Install Docker and downgrade to 1.12.6. At the time of writing this post libvirt had issues to connect with D-Bus due SElinux issues with Docker 1.13. # curl -sSL https://get.docker.io | bash # yum downgrade docker-engine-1.12.6 docker-engine-selinux-1.12.6 # yum install -y python-docker-py Configure Docker daemon to allow insecure-registry (Use the IP where your remote registry will be located). # mkdir -p /etc/systemd/system/docker.service.d # tee /etc/systemd/system/docker.service.d/kolla.conf &amp;lt;&amp;lt;-'EOF' [Service] ExecStart= ExecStart=/usr/bin/dockerd --insecure-registry 172.28.128.3:4000 MountFlags=shared EOF Reload systemd daemons and start/stop/disable/enable the following services. # systemctl daemon-reload # systemctl stop libvirtd # systemctl disable libvirtd # systemctl enable ntpd docker # systemctl start ntpd docker Download Ocata registry created in tarballs.openstack.org, skip this step if images used are custom builds or downloaded from DockerHub. Create kolla registry from downloaded tarball. # wget https://tarballs.openstack.org/kolla/images/centos-binary-registry-ocata.tar.gz # mkdir /opt/kolla_registry # sudo tar xzf centos-binary-registry-ocata.tar.gz -C /opt/kolla_registry # docker run -d -p 4000:5000 --restart=always -v /opt/kolla_registry/:/var/lib/registry --name registry registry:2 Install kolla-ansible. # pip install kolla-ansible # cp -r /usr/share/kolla-ansible/etc_examples/kolla /etc/kolla/ # cp /usr/share/kolla-ansible/ansible/inventory/* . Configure kolla globals.yml configuration file with the following content. Change values when necessary (IP addresses, interface names). This is a sample minimal configuration. # vi /etc/kolla/globals.yml --- kolla_internal_vip_address: &quot;172.28.128.10&quot; kolla_base_distro: &quot;centos&quot; kolla_install_type: &quot;binary&quot; docker_registry: &quot;172.28.128.3:4000&quot; docker_namespace: &quot;lokolla&quot; network_interface: &quot;enp0s8&quot; neutron_external_interface: &quot;enp0s9&quot; Configure designate options in globals.yml. dns_interface must be network reachable from nova instances if internal DNS resolution is needed. enable_designate: &quot;yes&quot; dns_interface: &quot;enp0s8&quot; designate_backend: &quot;bind9&quot; designate_ns_record: &quot;sample.openstack.org&quot; Configure inventory, add the nodes in their respective groups. # vi ~/multinode Generate passwords. # kolla-genpwd Ensure the environment is ready to deploy with prechecks. Until prechecks does not succeed do not start deployment. Fix what is necessary. # kolla-ansible prechecks -i ~/multinode Pull Docker images on the servers, this can be skipped because will be made in deploy step, but doing it first will ensure all the nodes have the images you need and will minimize the deployment time. # kolla-ansible pull -i ~/multinode Deploy kolla-ansible and do a woot for kolla ;) # kolla-ansible deploy -i ~/multinode Create credentials file and source it. # kolla-ansible post-deploy -i ~/multinode # source /etc/kolla/admin-openrc.sh Check that all containers are running and none of them are restarting or exiting. # docker ps -a --filter status=exited --filter status=restarting CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Install required python clients # pip install python-openstackclient python-designateclient python-neutronclient Execute a base OpenStack configuration (public and internal networks, cirros image). Do no execute this script if custom networks are going to be used. # sh /usr/share/kolla-ansible/init-runonce Create a sample designate zone. # openstack zone create --email admin@sample.openstack.org sample.openstack.org. +----------------+--------------------------------------+ | Field | Value | +----------------+--------------------------------------+ | action | CREATE | | attributes | | | created_at | 2017-02-22T13:14:39.000000 | | description | None | | email | admin@sample.openstack.org | | id | 4a44b0c9-bd07-4f5c-8908-523f453f269d | | masters | | | name | sample.openstack.org. | | pool_id | 85d18aec-453e-45ae-9eb3-748841a1da12 | | project_id | 937d49af6cfe4ef080a79f9a833d7c7d | | serial | 1487769279 | | status | PENDING | | transferred_at | None | | ttl | 3600 | | type | PRIMARY | | updated_at | None | | version | 1 | +----------------+--------------------------------------+ Configure designate sink to make use of the previously created zone, sink will need zone_id to automatically create neutron and nova records into designate. # mkdir -p /etc/kolla/config/designate/designate-sink/ # vi /etc/kolla/config/designate/designate-sink.conf [handler:nova_fixed] zone_id = 4a44b0c9-bd07-4f5c-8908-523f453f269d [handler:neutron_floatingip] zone_id = 4a44b0c9-bd07-4f5c-8908-523f453f269d After configure designate-sink.conf, reconfigure designate to make use of this configuration. # kolla-ansible reconfigure -i ~/multinode --tags designate List networks. # neutron net-list +--------------------------------------+----------+----------------------------------+--------------------------------------------------+ | id | name | tenant_id | subnets | +--------------------------------------+----------+----------------------------------+--------------------------------------------------+ | 3b56c605-5a01-45be-9ed6-e4c3285e4366 | demo-net | 937d49af6cfe4ef080a79f9a833d7c7d | 7f28f050-77b2-426e-b963-35b682077993 10.0.0.0/24 | | 6954d495-fb8c-4b0b-98a9-9672a7f65b7c | public1 | 937d49af6cfe4ef080a79f9a833d7c7d | 9bd9feca-40a7-4e82-b912-e51b726ad746 10.0.2.0/24 | +--------------------------------------+----------+----------------------------------+--------------------------------------------------+ Update the network with a dns_domain. # neutron net-update 3b56c605-5a01-45be-9ed6-e4c3285e4366 --dns_domain sample.openstack.org. Updated network: 3b56c605-5a01-45be-9ed6-e4c3285e4366 Ensure dns_domain is properly applied. # neutron net-show 3b56c605-5a01-45be-9ed6-e4c3285e4366 +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | True | | availability_zone_hints | | | availability_zones | nova | | created_at | 2017-02-22T13:13:06Z | | description | | | dns_domain | sample.openstack.org. | | id | 3b56c605-5a01-45be-9ed6-e4c3285e4366 | | ipv4_address_scope | | | ipv6_address_scope | | | mtu | 1450 | | name | demo-net | | port_security_enabled | True | | project_id | 937d49af6cfe4ef080a79f9a833d7c7d | | provider:network_type | vxlan | | provider:physical_network | | | provider:segmentation_id | 27 | | revision_number | 6 | | router:external | False | | shared | False | | status | ACTIVE | | subnets | 7f28f050-77b2-426e-b963-35b682077993 | | tags | | | tenant_id | 937d49af6cfe4ef080a79f9a833d7c7d | | updated_at | 2017-02-22T13:25:16Z | +---------------------------+--------------------------------------+ Create a new instance in the previously updated network. # openstack server create \ --image cirros \ --flavor m1.tiny \ --key-name mykey \ --nic net-id=3b56c605-5a01-45be-9ed6-e4c3285e4366 \ demo1 Once the instance is ACTIVE, check the IP associated. # openstack server list +--------------------------------------+-------+--------+-------------------+------------+ | ID | Name | Status | Networks | Image Name | +--------------------------------------+-------+--------+-------------------+------------+ | d483e4ee-58c2-4e1e-9384-85174630428e | demo1 | ACTIVE | demo-net=10.0.0.3 | cirros | +--------------------------------------+-------+--------+-------------------+------------+ List records in the designate zone. As you can see there is a record in designate associated with the instance IP. # openstack recordset list sample.openstack.org. +--------------------------------------+----------------------------------+------+-------------------------------------------+--------+--------+ | id | name | type | records | status | action | +--------------------------------------+----------------------------------+------+-------------------------------------------+--------+--------+ | 4f70531e-c325-4ffd-a8d3-8172bd5163b8 | sample.openstack.org. | SOA | sample.openstack.org. | ACTIVE | NONE | | | | | admin.sample.openstack.org. 1487770304 | | | | | | | 3586 600 86400 3600 | | | | a9a09c5f-ccf1-4b52-8400-f36e8faa9549 | sample.openstack.org. | NS | sample.openstack.org. | ACTIVE | NONE | | aa6cd25d-186e-425b-9153-699d8b0811de | 10-0-0-3.sample.openstack.org. | A | 10.0.0.3 | ACTIVE | NONE | | 713650a5-a45e-470b-9539-74e110b15115 | demo1.None.sample.openstack.org. | A | 10.0.0.3 | ACTIVE | NONE | | 6506e6f6-f535-45eb-9bfb-4ac1f16c5c9b | demo1.sample.openstack.org. | A | 10.0.0.3 | ACTIVE | NONE | +--------------------------------------+----------------------------------+------+-------------------------------------------+--------+--------+ Validate that designate resolves the DNS record. You can use designate mDNS service or directly to bind9 servers to validate the test. # dig +short -p 5354 @172.28.128.3 demo1.sample.openstack.org. A 10.0.0.3 # dig +short -p 53 @172.28.128.3 demo1.sample.openstack.org. A 10.0.0.3 If you find any issue with designate in kolla-ansible or kolla, please fill a bug https://bugs.launchpad.net/kolla-ansible/+filebug Regards, Eduardo Gonzalez</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://217.182.136.201:5000/wp-content/uploads/2015/09/learn-about-openstack-badge.png" /></entry><entry><title type="html">OpenStack Keystone Zero-Downtime upgrade process (N to O)</title><link href="http://217.182.136.201:5000/openstack-keystone-zero-downtime-upgrade-process-n-to-o/" rel="alternate" type="text/html" title="OpenStack Keystone Zero-Downtime upgrade process (N to O)" /><published>2017-01-31T17:00:17+00:00</published><updated>2017-01-31T17:00:17+00:00</updated><id>http://217.182.136.201:5000/openstack-keystone-zero-downtime-upgrade-process-n-to-o</id><content type="html" xml:base="http://217.182.136.201:5000/openstack-keystone-zero-downtime-upgrade-process-n-to-o/">&lt;p&gt;This blog post will show Keystone upgrade procedure from OpenStack Newton to Ocata release with zero-downtime.&lt;/p&gt;

&lt;p&gt;In the case of doing this in production, please read release notes, ensure a proper configuration, do database backups and test the upgrade a thousand times.&lt;/p&gt;

&lt;p&gt;Keystone upgrade will need to stop one node in order to use it as upgrade server.
In the case of a PoC this is not an issue, but in a production environment, Keystone loads may be intensive and stopping a node for a while may decrease other nodes performance more than expected.
For this reason I prefer orchestrate the upgrade from an external Docker container. With this method all nodes will be fully running almost all the time.&lt;/p&gt;
&lt;ul&gt;
 	&lt;li&gt;New container won't start any service, just will sync the database schema with new Keystone version avoiding stop a node to orchestrate the upgrade.&lt;/li&gt;
 	&lt;li&gt;The Docker image is provided by OpenStack Kolla project, if already using Kolla this upgrade won't be needed as kolla-ansible already provide an upgrade method.&lt;/li&gt;
 	&lt;li&gt;At the moment of writing of this blog, Ocata packages were not released into stable repositories. For this reason I use DLRN repositories.&lt;/li&gt;
 	&lt;li&gt;If Ocata is released please do not use DLRN, use stable packages instead.&lt;/li&gt;
 	&lt;li&gt;Use stable Ocata Docker image if available with tag 4.0.x and will avoid repository configuration and package upgrades.&lt;/li&gt;
 	&lt;li&gt;NOTE: Upgrade may need more steps depending of your own configuration, i.e, if using fernet token more steps are necessary during the upgrade.&lt;/li&gt;
        &lt;li&gt;All Keystone nodes are behind HAproxy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Prepare the upgrade&lt;/h3&gt;
&lt;p&gt;Start Keystone Docker container with host networking (needed to communicate with database nodes directly) and root user (needed to install packages).&lt;/p&gt;
&lt;pre&gt;(host)# docker run -ti --net host -u 0 kolla/centos-binary-keystone:3.0.2 bash
&lt;/pre&gt;
&lt;p&gt;Download Delorean CentOS trunk repositories&lt;/p&gt;
&lt;pre&gt;(keystone-upgrade)# curl -Lo /etc/yum.repos.d/delorean.repo http://buildlogs.centos.org/centos/7/cloud/x86_64/rdo-trunk-master-tested/delorean.repo
(keystone-upgrade)# curl -Lo /etc/yum.repos.d/delorean-deps.repo http://trunk.rdoproject.org/centos7/delorean-deps.repo
&lt;/pre&gt;
&lt;p&gt;Disable Newton repository&lt;/p&gt;
&lt;pre&gt;(keystone-upgrade)# yum-config-manager --disable centos-openstack-newton
&lt;/pre&gt;
&lt;p&gt;Ensure Newton repository is not longer used by the system&lt;/p&gt;
&lt;pre&gt;(keystone-upgrade)# yum repolist | grep -i openstack
delorean                        delorean-openstack-glance-0bf9d805886c2  565+255
&lt;/pre&gt;
&lt;p&gt;Update all packages in the Docker container to bump keystone version to Ocata.&lt;/p&gt;
&lt;pre&gt;(keystone-upgrade)# yum clean all &amp;amp;&amp;amp; yum update -y
&lt;/pre&gt;
&lt;p&gt;Configure keystone.conf file, this are my settings. Review you configuration and ensure all is correctly, otherwise may cause issues in the database.
An important option is default_domain_id, this value is for backward compatible with users created under default domain.&lt;/p&gt;
&lt;pre&gt;(keystone-upgrade)# egrep ^[^#] /etc/keystone/keystone.conf 
[DEFAULT]
debug = False
log_file = /var/log/keystone/keystone.log
secure_proxy_ssl_header = HTTP_X_FORWARDED_PROTO
[database]
connection = mysql+pymysql://keystone:ickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB@192.168.100.10:3306/keystone
max_retries = -1
[cache]
backend = oslo_cache.memcache_pool
enabled = True
memcache_servers = 192.168.100.215:11211,192.168.100.170:11211
[identity]
default_domain_id = default
[token]
provider = uuid
&lt;/pre&gt;
&lt;p&gt;Check migrate version in the database.
As you will notice, contract/data_migrate/expand are in the same version&lt;/p&gt;
&lt;pre&gt;(mariadb)# mysql -ukeystone -pickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB -h192.168.100.10 keystone -e &quot;select * from migrate_version;&quot; 
Warning: Using a password on the command line interface can be insecure.
+-----------------------+--------------------------------------------------------------------------+---------+
| repository_id         | repository_path                                                          | version |
+-----------------------+--------------------------------------------------------------------------+---------+
| keystone              | /usr/lib/python2.7/site-packages/keystone/common/sql/migrate_repo        |     109 |
| keystone_contract     | /usr/lib/python2.7/site-packages/keystone/common/sql/contract_repo       |       4 |
| keystone_data_migrate | /usr/lib/python2.7/site-packages/keystone/common/sql/data_migration_repo |       4 |
| keystone_expand       | /usr/lib/python2.7/site-packages/keystone/common/sql/expand_repo         |       4 |
+-----------------------+--------------------------------------------------------------------------+---------+
&lt;/pre&gt;
&lt;p&gt;Before start upgrading the database schema, you will need add SUPER privileges in the database to keystone user or set log_bin_trust_function_creators to True.
In my opinion is safer set the value to True, I don’t want keystone with SUPER privileges.&lt;/p&gt;
&lt;pre&gt;(mariadb)# mysql -uroot -pnkLMrBibfMTRqOGBAP3UAxdO4kOFfEaPptGM5UDL -h192.168.100.10 keystone -e &quot;set global log_bin_trust_function_creators=1;&quot;
&lt;/pre&gt;
&lt;p&gt;Now use Rally, tempest or some tool to test/benchmarch keystone service during upgrade.
If don’t want to use one of those tools, just use this for command.&lt;/p&gt;
&lt;pre&gt;(host)# for i in {1000..6000} ; do openstack user create --password $i $i; done
&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Start Upgrade&lt;/h3&gt;
&lt;p&gt;Check database status before upgrade using Doctor, this may raise issues in the configuration. Some of them may be ignored(Please, ensure is not an issue before ignoring).
As example, I’m not using fernet tokens and errors appear about missing folder.&lt;/p&gt;
&lt;pre&gt;(keystone-upgrade)# keystone-manage doctor
&lt;/pre&gt;
&lt;p&gt;Remove obsoleted tokens&lt;/p&gt;
&lt;pre&gt;(keystone-upgrade)# keystone-manage token_flush
&lt;/pre&gt;
&lt;p&gt;Now, expand the database schema to latest version, in keystone.log can see the status.
Check in the logs if some error is raised before jump to the next step.&lt;/p&gt;
&lt;pre&gt;(keystone-upgrade)# keystone-manage db_sync --expand


2017-01-31 13:42:02.772 306 INFO migrate.versioning.api [-] 4 -&amp;gt; 5... 
2017-01-31 13:42:03.004 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:03.005 306 INFO migrate.versioning.api [-] 5 -&amp;gt; 6... 
2017-01-31 13:42:03.310 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:03.310 306 INFO migrate.versioning.api [-] 6 -&amp;gt; 7... 
2017-01-31 13:42:03.670 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:03.671 306 INFO migrate.versioning.api [-] 7 -&amp;gt; 8... 
2017-01-31 13:42:03.984 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:03.985 306 INFO migrate.versioning.api [-] 8 -&amp;gt; 9... 
2017-01-31 13:42:04.185 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:04.185 306 INFO migrate.versioning.api [-] 9 -&amp;gt; 10... 
2017-01-31 13:42:07.202 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:07.202 306 INFO migrate.versioning.api [-] 10 -&amp;gt; 11... 
2017-01-31 13:42:07.481 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:07.481 306 INFO migrate.versioning.api [-] 11 -&amp;gt; 12... 
2017-01-31 13:42:11.334 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:11.334 306 INFO migrate.versioning.api [-] 12 -&amp;gt; 13... 
2017-01-31 13:42:11.560 306 INFO migrate.versioning.api [-] done
&lt;/pre&gt;
&lt;p&gt;After expand the database, migrate it to latest version.
Ensure there are not errors in Keystone logs.&lt;/p&gt;
&lt;pre&gt;(keystone-upgrade)# keystone-manage db_sync --migrate

#keystone.log
2017-01-31 13:42:58.771 314 INFO migrate.versioning.api [-] 4 -&amp;gt; 5... 
2017-01-31 13:42:58.943 314 INFO migrate.versioning.api [-] done
2017-01-31 13:42:58.943 314 INFO migrate.versioning.api [-] 5 -&amp;gt; 6... 
2017-01-31 13:42:59.143 314 INFO migrate.versioning.api [-] done
2017-01-31 13:42:59.143 314 INFO migrate.versioning.api [-] 6 -&amp;gt; 7... 
2017-01-31 13:42:59.340 314 INFO migrate.versioning.api [-] done
2017-01-31 13:42:59.341 314 INFO migrate.versioning.api [-] 7 -&amp;gt; 8... 
2017-01-31 13:42:59.698 314 INFO migrate.versioning.api [-] done
2017-01-31 13:42:59.699 314 INFO migrate.versioning.api [-] 8 -&amp;gt; 9... 
2017-01-31 13:42:59.852 314 INFO migrate.versioning.api [-] done
2017-01-31 13:42:59.852 314 INFO migrate.versioning.api [-] 9 -&amp;gt; 10... 
2017-01-31 13:43:00.135 314 INFO migrate.versioning.api [-] done
2017-01-31 13:43:00.135 314 INFO migrate.versioning.api [-] 10 -&amp;gt; 11... 
2017-01-31 13:43:00.545 314 INFO migrate.versioning.api [-] done
2017-01-31 13:43:00.545 314 INFO migrate.versioning.api [-] 11 -&amp;gt; 12... 
2017-01-31 13:43:00.703 314 INFO migrate.versioning.api [-] done
2017-01-31 13:43:00.703 314 INFO migrate.versioning.api [-] 12 -&amp;gt; 13... 
2017-01-31 13:43:00.854 314 INFO migrate.versioning.api [-] done
&lt;/pre&gt;
&lt;p&gt;Now, see migrate_version table, you will notice that expand and data_migrate are in the latest version, but contract still in the previous version.&lt;/p&gt;
&lt;pre&gt;(mariadb)# mysql -ukeystone -pickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB -h192.168.100.10 keystone -e &quot;select * from migrate_version;&quot;
+-----------------------+--------------------------------------------------------------------------+---------+
| repository_id         | repository_path                                                          | version |
+-----------------------+--------------------------------------------------------------------------+---------+
| keystone              | /usr/lib/python2.7/site-packages/keystone/common/sql/migrate_repo        |     109 |
| keystone_contract     | /usr/lib/python2.7/site-packages/keystone/common/sql/contract_repo       |       4 |
| keystone_data_migrate | /usr/lib/python2.7/site-packages/keystone/common/sql/data_migration_repo |      13 |
| keystone_expand       | /usr/lib/python2.7/site-packages/keystone/common/sql/expand_repo         |      13 |
+-----------------------+--------------------------------------------------------------------------+---------+
&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Every Keystone node, one by one&lt;/h3&gt;
&lt;p&gt;Go to keystone nodes.
Stop Keystone services, in my case using wsgi inside Apache&lt;/p&gt;
&lt;pre&gt;(keystone_nodes)# systemctl stop httpd
&lt;/pre&gt;
&lt;p&gt;Configure Ocata repositories as made in the Docker container.
Update packages, if you have Keystone sharing the node with other OpenStack service, do not update all packages as it will break other services.
Update only required packages.&lt;/p&gt;
&lt;pre&gt;(keystone_nodes)# yum clean all &amp;amp;&amp;amp; yum update -y
&lt;/pre&gt;
&lt;p&gt;Configure Keystone configuration file to the desired state. Your configuration may change.&lt;/p&gt;
&lt;pre&gt;(keystone_nodes)# egrep ^[^#] /etc/keystone/keystone.conf 
[DEFAULT]
debug = False
log_file = /var/log/keystone/keystone.log
secure_proxy_ssl_header = HTTP_X_FORWARDED_PROTO
[database]
connection = mysql+pymysql://keystone:ickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB@192.168.100.10:3306/keystone
max_retries = -1
[cache]
backend = oslo_cache.memcache_pool
enabled = True
memcache_servers = 192.168.100.215:11211,192.168.100.170:11211
[identity]
default_domain_id = default
[token]
provider = uuid
&lt;/pre&gt;
&lt;p&gt;Start Keystone service.&lt;/p&gt;
&lt;pre&gt;(keystone_nodes)# systemctl start httpd
&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Finish Upgrade&lt;/h3&gt;
&lt;p&gt;After all the nodes are updated to latest version (please ensure all nodes are using latest packages, if not will fail).
Contract Keystone database schema.
Look at keystone.log for errors.&lt;/p&gt;
&lt;pre&gt;(keystone-upgrade)# keystone-manage db_sync --contract

keystone.log

2017-01-31 13:57:52.164 322 INFO migrate.versioning.api [-] 4 -&amp;gt; 5... 
2017-01-31 13:57:52.379 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:52.379 322 INFO migrate.versioning.api [-] 5 -&amp;gt; 6... 
2017-01-31 13:57:52.969 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:52.969 322 INFO migrate.versioning.api [-] 6 -&amp;gt; 7... 
2017-01-31 13:57:53.462 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:53.462 322 INFO migrate.versioning.api [-] 7 -&amp;gt; 8... 
2017-01-31 13:57:53.793 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:53.793 322 INFO migrate.versioning.api [-] 8 -&amp;gt; 9... 
2017-01-31 13:57:53.957 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:53.957 322 INFO migrate.versioning.api [-] 9 -&amp;gt; 10... 
2017-01-31 13:57:54.111 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:54.112 322 INFO migrate.versioning.api [-] 10 -&amp;gt; 11... 
2017-01-31 13:57:54.853 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:54.853 322 INFO migrate.versioning.api [-] 11 -&amp;gt; 12... 
2017-01-31 13:57:56.727 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:56.728 322 INFO migrate.versioning.api [-] 12 -&amp;gt; 13... 
2017-01-31 13:57:59.529 322 INFO migrate.versioning.api [-] done
&lt;/pre&gt;
&lt;p&gt;Now if we look at migrate_version table, will see that contract version is latest and match with the other version (Ensure all are in the same version).
This means the database upgrade has been successfully implemented.&lt;/p&gt;
&lt;pre&gt;(mariadb)# mysql -ukeystone -pickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB -h192.168.100.10 keystone -e &quot;select * from migrate_version;&quot;
+-----------------------+--------------------------------------------------------------------------+---------+
| repository_id         | repository_path                                                          | version |
+-----------------------+--------------------------------------------------------------------------+---------+
| keystone              | /usr/lib/python2.7/site-packages/keystone/common/sql/migrate_repo        |     109 |
| keystone_contract     | /usr/lib/python2.7/site-packages/keystone/common/sql/contract_repo       |      13 |
| keystone_data_migrate | /usr/lib/python2.7/site-packages/keystone/common/sql/data_migration_repo |      13 |
| keystone_expand       | /usr/lib/python2.7/site-packages/keystone/common/sql/expand_repo         |      13 |
+-----------------------+--------------------------------------------------------------------------+---------+
&lt;/pre&gt;
&lt;p&gt;Remove log_bin_trust_function_creators value.&lt;/p&gt;
&lt;pre&gt;(mariadb)# mysql -uroot -pnkLMrBibfMTRqOGBAP3UAxdO4kOFfEaPptGM5UDL -h192.168.100.10 keystone -e &quot;set global log_bin_trust_function_creators=0;&quot;
&lt;/pre&gt;
&lt;p&gt;After finish the upgrade, Rally tests should not have any error. **If using HAproxy for load balance Keystone service, some errors may happen due a connection drop while stopping Keystone service and re-balance to other Keystone node. This can be avoided putting the node to update in Maintenance Mode in HAproxy backend.&lt;/p&gt;

&lt;p&gt;Have to thank Keystone team in #openstack-keystone IRC channel for the help provided with a couple of issues.&lt;/p&gt;

&lt;p&gt;Regards, Eduardo Gonzalez&lt;/p&gt;</content><author><name>Editor</name></author><category term="contract" /><category term="database" /><category term="docker" /><category term="Downtime" /><category term="expand" /><category term="keystone" /><category term="kolla" /><category term="migrate" /><category term="newton" /><category term="ocata" /><category term="openstack" /><category term="schema" /><category term="Upgrade" /><category term="Zero" /><category term="zero-downtime" /><summary type="html">This blog post will show Keystone upgrade procedure from OpenStack Newton to Ocata release with zero-downtime. In the case of doing this in production, please read release notes, ensure a proper configuration, do database backups and test the upgrade a thousand times. Keystone upgrade will need to stop one node in order to use it as upgrade server. In the case of a PoC this is not an issue, but in a production environment, Keystone loads may be intensive and stopping a node for a while may decrease other nodes performance more than expected. For this reason I prefer orchestrate the upgrade from an external Docker container. With this method all nodes will be fully running almost all the time. New container won't start any service, just will sync the database schema with new Keystone version avoiding stop a node to orchestrate the upgrade. The Docker image is provided by OpenStack Kolla project, if already using Kolla this upgrade won't be needed as kolla-ansible already provide an upgrade method. At the moment of writing of this blog, Ocata packages were not released into stable repositories. For this reason I use DLRN repositories. If Ocata is released please do not use DLRN, use stable packages instead. Use stable Ocata Docker image if available with tag 4.0.x and will avoid repository configuration and package upgrades. NOTE: Upgrade may need more steps depending of your own configuration, i.e, if using fernet token more steps are necessary during the upgrade. All Keystone nodes are behind HAproxy.   Prepare the upgrade Start Keystone Docker container with host networking (needed to communicate with database nodes directly) and root user (needed to install packages). (host)# docker run -ti --net host -u 0 kolla/centos-binary-keystone:3.0.2 bash Download Delorean CentOS trunk repositories (keystone-upgrade)# curl -Lo /etc/yum.repos.d/delorean.repo http://buildlogs.centos.org/centos/7/cloud/x86_64/rdo-trunk-master-tested/delorean.repo (keystone-upgrade)# curl -Lo /etc/yum.repos.d/delorean-deps.repo http://trunk.rdoproject.org/centos7/delorean-deps.repo Disable Newton repository (keystone-upgrade)# yum-config-manager --disable centos-openstack-newton Ensure Newton repository is not longer used by the system (keystone-upgrade)# yum repolist | grep -i openstack delorean delorean-openstack-glance-0bf9d805886c2 565+255 Update all packages in the Docker container to bump keystone version to Ocata. (keystone-upgrade)# yum clean all &amp;amp;&amp;amp; yum update -y Configure keystone.conf file, this are my settings. Review you configuration and ensure all is correctly, otherwise may cause issues in the database. An important option is default_domain_id, this value is for backward compatible with users created under default domain. (keystone-upgrade)# egrep ^[^#] /etc/keystone/keystone.conf [DEFAULT] debug = False log_file = /var/log/keystone/keystone.log secure_proxy_ssl_header = HTTP_X_FORWARDED_PROTO [database] connection = mysql+pymysql://keystone:ickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB@192.168.100.10:3306/keystone max_retries = -1 [cache] backend = oslo_cache.memcache_pool enabled = True memcache_servers = 192.168.100.215:11211,192.168.100.170:11211 [identity] default_domain_id = default [token] provider = uuid Check migrate version in the database. As you will notice, contract/data_migrate/expand are in the same version (mariadb)# mysql -ukeystone -pickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB -h192.168.100.10 keystone -e &quot;select * from migrate_version;&quot; Warning: Using a password on the command line interface can be insecure. +-----------------------+--------------------------------------------------------------------------+---------+ | repository_id | repository_path | version | +-----------------------+--------------------------------------------------------------------------+---------+ | keystone | /usr/lib/python2.7/site-packages/keystone/common/sql/migrate_repo | 109 | | keystone_contract | /usr/lib/python2.7/site-packages/keystone/common/sql/contract_repo | 4 | | keystone_data_migrate | /usr/lib/python2.7/site-packages/keystone/common/sql/data_migration_repo | 4 | | keystone_expand | /usr/lib/python2.7/site-packages/keystone/common/sql/expand_repo | 4 | +-----------------------+--------------------------------------------------------------------------+---------+ Before start upgrading the database schema, you will need add SUPER privileges in the database to keystone user or set log_bin_trust_function_creators to True. In my opinion is safer set the value to True, I don’t want keystone with SUPER privileges. (mariadb)# mysql -uroot -pnkLMrBibfMTRqOGBAP3UAxdO4kOFfEaPptGM5UDL -h192.168.100.10 keystone -e &quot;set global log_bin_trust_function_creators=1;&quot; Now use Rally, tempest or some tool to test/benchmarch keystone service during upgrade. If don’t want to use one of those tools, just use this for command. (host)# for i in {1000..6000} ; do openstack user create --password $i $i; done   Start Upgrade Check database status before upgrade using Doctor, this may raise issues in the configuration. Some of them may be ignored(Please, ensure is not an issue before ignoring). As example, I’m not using fernet tokens and errors appear about missing folder. (keystone-upgrade)# keystone-manage doctor Remove obsoleted tokens (keystone-upgrade)# keystone-manage token_flush Now, expand the database schema to latest version, in keystone.log can see the status. Check in the logs if some error is raised before jump to the next step. (keystone-upgrade)# keystone-manage db_sync --expand 2017-01-31 13:42:02.772 306 INFO migrate.versioning.api [-] 4 -&amp;gt; 5... 2017-01-31 13:42:03.004 306 INFO migrate.versioning.api [-] done 2017-01-31 13:42:03.005 306 INFO migrate.versioning.api [-] 5 -&amp;gt; 6... 2017-01-31 13:42:03.310 306 INFO migrate.versioning.api [-] done 2017-01-31 13:42:03.310 306 INFO migrate.versioning.api [-] 6 -&amp;gt; 7... 2017-01-31 13:42:03.670 306 INFO migrate.versioning.api [-] done 2017-01-31 13:42:03.671 306 INFO migrate.versioning.api [-] 7 -&amp;gt; 8... 2017-01-31 13:42:03.984 306 INFO migrate.versioning.api [-] done 2017-01-31 13:42:03.985 306 INFO migrate.versioning.api [-] 8 -&amp;gt; 9... 2017-01-31 13:42:04.185 306 INFO migrate.versioning.api [-] done 2017-01-31 13:42:04.185 306 INFO migrate.versioning.api [-] 9 -&amp;gt; 10... 2017-01-31 13:42:07.202 306 INFO migrate.versioning.api [-] done 2017-01-31 13:42:07.202 306 INFO migrate.versioning.api [-] 10 -&amp;gt; 11... 2017-01-31 13:42:07.481 306 INFO migrate.versioning.api [-] done 2017-01-31 13:42:07.481 306 INFO migrate.versioning.api [-] 11 -&amp;gt; 12... 2017-01-31 13:42:11.334 306 INFO migrate.versioning.api [-] done 2017-01-31 13:42:11.334 306 INFO migrate.versioning.api [-] 12 -&amp;gt; 13... 2017-01-31 13:42:11.560 306 INFO migrate.versioning.api [-] done After expand the database, migrate it to latest version. Ensure there are not errors in Keystone logs. (keystone-upgrade)# keystone-manage db_sync --migrate #keystone.log 2017-01-31 13:42:58.771 314 INFO migrate.versioning.api [-] 4 -&amp;gt; 5... 2017-01-31 13:42:58.943 314 INFO migrate.versioning.api [-] done 2017-01-31 13:42:58.943 314 INFO migrate.versioning.api [-] 5 -&amp;gt; 6... 2017-01-31 13:42:59.143 314 INFO migrate.versioning.api [-] done 2017-01-31 13:42:59.143 314 INFO migrate.versioning.api [-] 6 -&amp;gt; 7... 2017-01-31 13:42:59.340 314 INFO migrate.versioning.api [-] done 2017-01-31 13:42:59.341 314 INFO migrate.versioning.api [-] 7 -&amp;gt; 8... 2017-01-31 13:42:59.698 314 INFO migrate.versioning.api [-] done 2017-01-31 13:42:59.699 314 INFO migrate.versioning.api [-] 8 -&amp;gt; 9... 2017-01-31 13:42:59.852 314 INFO migrate.versioning.api [-] done 2017-01-31 13:42:59.852 314 INFO migrate.versioning.api [-] 9 -&amp;gt; 10... 2017-01-31 13:43:00.135 314 INFO migrate.versioning.api [-] done 2017-01-31 13:43:00.135 314 INFO migrate.versioning.api [-] 10 -&amp;gt; 11... 2017-01-31 13:43:00.545 314 INFO migrate.versioning.api [-] done 2017-01-31 13:43:00.545 314 INFO migrate.versioning.api [-] 11 -&amp;gt; 12... 2017-01-31 13:43:00.703 314 INFO migrate.versioning.api [-] done 2017-01-31 13:43:00.703 314 INFO migrate.versioning.api [-] 12 -&amp;gt; 13... 2017-01-31 13:43:00.854 314 INFO migrate.versioning.api [-] done Now, see migrate_version table, you will notice that expand and data_migrate are in the latest version, but contract still in the previous version. (mariadb)# mysql -ukeystone -pickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB -h192.168.100.10 keystone -e &quot;select * from migrate_version;&quot; +-----------------------+--------------------------------------------------------------------------+---------+ | repository_id | repository_path | version | +-----------------------+--------------------------------------------------------------------------+---------+ | keystone | /usr/lib/python2.7/site-packages/keystone/common/sql/migrate_repo | 109 | | keystone_contract | /usr/lib/python2.7/site-packages/keystone/common/sql/contract_repo | 4 | | keystone_data_migrate | /usr/lib/python2.7/site-packages/keystone/common/sql/data_migration_repo | 13 | | keystone_expand | /usr/lib/python2.7/site-packages/keystone/common/sql/expand_repo | 13 | +-----------------------+--------------------------------------------------------------------------+---------+   Every Keystone node, one by one Go to keystone nodes. Stop Keystone services, in my case using wsgi inside Apache (keystone_nodes)# systemctl stop httpd Configure Ocata repositories as made in the Docker container. Update packages, if you have Keystone sharing the node with other OpenStack service, do not update all packages as it will break other services. Update only required packages. (keystone_nodes)# yum clean all &amp;amp;&amp;amp; yum update -y Configure Keystone configuration file to the desired state. Your configuration may change. (keystone_nodes)# egrep ^[^#] /etc/keystone/keystone.conf [DEFAULT] debug = False log_file = /var/log/keystone/keystone.log secure_proxy_ssl_header = HTTP_X_FORWARDED_PROTO [database] connection = mysql+pymysql://keystone:ickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB@192.168.100.10:3306/keystone max_retries = -1 [cache] backend = oslo_cache.memcache_pool enabled = True memcache_servers = 192.168.100.215:11211,192.168.100.170:11211 [identity] default_domain_id = default [token] provider = uuid Start Keystone service. (keystone_nodes)# systemctl start httpd   Finish Upgrade After all the nodes are updated to latest version (please ensure all nodes are using latest packages, if not will fail). Contract Keystone database schema. Look at keystone.log for errors. (keystone-upgrade)# keystone-manage db_sync --contract keystone.log 2017-01-31 13:57:52.164 322 INFO migrate.versioning.api [-] 4 -&amp;gt; 5... 2017-01-31 13:57:52.379 322 INFO migrate.versioning.api [-] done 2017-01-31 13:57:52.379 322 INFO migrate.versioning.api [-] 5 -&amp;gt; 6... 2017-01-31 13:57:52.969 322 INFO migrate.versioning.api [-] done 2017-01-31 13:57:52.969 322 INFO migrate.versioning.api [-] 6 -&amp;gt; 7... 2017-01-31 13:57:53.462 322 INFO migrate.versioning.api [-] done 2017-01-31 13:57:53.462 322 INFO migrate.versioning.api [-] 7 -&amp;gt; 8... 2017-01-31 13:57:53.793 322 INFO migrate.versioning.api [-] done 2017-01-31 13:57:53.793 322 INFO migrate.versioning.api [-] 8 -&amp;gt; 9... 2017-01-31 13:57:53.957 322 INFO migrate.versioning.api [-] done 2017-01-31 13:57:53.957 322 INFO migrate.versioning.api [-] 9 -&amp;gt; 10... 2017-01-31 13:57:54.111 322 INFO migrate.versioning.api [-] done 2017-01-31 13:57:54.112 322 INFO migrate.versioning.api [-] 10 -&amp;gt; 11... 2017-01-31 13:57:54.853 322 INFO migrate.versioning.api [-] done 2017-01-31 13:57:54.853 322 INFO migrate.versioning.api [-] 11 -&amp;gt; 12... 2017-01-31 13:57:56.727 322 INFO migrate.versioning.api [-] done 2017-01-31 13:57:56.728 322 INFO migrate.versioning.api [-] 12 -&amp;gt; 13... 2017-01-31 13:57:59.529 322 INFO migrate.versioning.api [-] done Now if we look at migrate_version table, will see that contract version is latest and match with the other version (Ensure all are in the same version). This means the database upgrade has been successfully implemented. (mariadb)# mysql -ukeystone -pickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB -h192.168.100.10 keystone -e &quot;select * from migrate_version;&quot; +-----------------------+--------------------------------------------------------------------------+---------+ | repository_id | repository_path | version | +-----------------------+--------------------------------------------------------------------------+---------+ | keystone | /usr/lib/python2.7/site-packages/keystone/common/sql/migrate_repo | 109 | | keystone_contract | /usr/lib/python2.7/site-packages/keystone/common/sql/contract_repo | 13 | | keystone_data_migrate | /usr/lib/python2.7/site-packages/keystone/common/sql/data_migration_repo | 13 | | keystone_expand | /usr/lib/python2.7/site-packages/keystone/common/sql/expand_repo | 13 | +-----------------------+--------------------------------------------------------------------------+---------+ Remove log_bin_trust_function_creators value. (mariadb)# mysql -uroot -pnkLMrBibfMTRqOGBAP3UAxdO4kOFfEaPptGM5UDL -h192.168.100.10 keystone -e &quot;set global log_bin_trust_function_creators=0;&quot; After finish the upgrade, Rally tests should not have any error. **If using HAproxy for load balance Keystone service, some errors may happen due a connection drop while stopping Keystone service and re-balance to other Keystone node. This can be avoided putting the node to update in Maintenance Mode in HAproxy backend. Have to thank Keystone team in #openstack-keystone IRC channel for the help provided with a couple of issues. Regards, Eduardo Gonzalez</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://217.182.136.201:5000/wp-content/uploads/2015/09/learn-about-openstack-badge.png" /></entry><entry><title type="html">To Conditional or To Skip, that is the Ansible question</title><link href="http://217.182.136.201:5000/to-conditional-or-to-skip-that-is-the-ansible-question/" rel="alternate" type="text/html" title="To Conditional or To Skip, that is the Ansible question" /><published>2016-10-31T20:49:37+00:00</published><updated>2016-10-31T20:49:37+00:00</updated><id>http://217.182.136.201:5000/to-conditional-or-to-skip-that-is-the-ansible-question</id><content type="html" xml:base="http://217.182.136.201:5000/to-conditional-or-to-skip-that-is-the-ansible-question/">&lt;p&gt;Have you ever think about if an Ansible task should be skipped with a conditional or without (hidden skip)?.
Well, this post will analyse both methods.&lt;/p&gt;

&lt;p&gt;Let’s use a example to create the same result and analyse the both methods:&lt;/p&gt;

&lt;p&gt;In OpenStack Kolla we found that sometimes operators need to customise policy.json files. That’s fine, but the problem is that policy.json files are installed with the services by default and we don’t need/want to maintain policy.json files in our repository because for sure will cause bugs from outdated policy files in the future.&lt;/p&gt;

&lt;p&gt;What’s the proposed solution to this? Allow operators use their own policy files only when their files exists in a custom configuration folder. If custom files are not present, default policy.json files are already present as part of the software installation. ( Actually this change is under review )&lt;/p&gt;
&lt;h2&gt;To Conditional method&lt;/h2&gt;
&lt;p&gt;Code snippet:&lt;/p&gt;
&lt;pre&gt;  - name: Check if file exists
    stat:
      path: &quot;/tmp/custom_file.json&quot;
    register: check_custom_file_exist

  - name: Copy custom policy when exist
    template:
      src: &quot;/tmp/custom_file.json&quot;
      dest: &quot;/tmp/destination_file.json&quot;
    when: &quot;&quot;
&lt;/pre&gt;
&lt;p&gt;The first task checks if the file is present and register the stat result.
The second task, copy the file only when the registered result of the previous task is True. (exists == True)&lt;/p&gt;

&lt;p&gt;Outputs the following when the file is not present:&lt;/p&gt;
&lt;pre&gt;PLAY [localhost] ***************************************************************

TASK [Check if file exists] ****************************************************
ok: [localhost]

TASK [Copy custom policy when exist] *******************************************
skipping: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=1    changed=0    unreachable=0    failed=0  
&lt;/pre&gt;
&lt;p&gt;We can see the copy file task is skipped with a skipping message.&lt;/p&gt;
&lt;h2&gt;To Skip method&lt;/h2&gt;
&lt;p&gt;Code snippet:&lt;/p&gt;
&lt;pre&gt;  - name: Copy custom policy when exist
    template:
      src: &quot;&quot;
      dest: &quot;/tmp/destination_file.json&quot;
    with_first_found:
    - files:
      - custom_file.json
      skip: True
&lt;/pre&gt;
&lt;p&gt;This playbook contains a single task, this task will use the first found file in a list of files. If no file is present will skip the task.&lt;/p&gt;

&lt;p&gt;Output from this execution:&lt;/p&gt;
&lt;pre&gt;PLAY [localhost] ***************************************************************

TASK [Copy custom policy when exist] *******************************************

PLAY RECAP *********************************************************************
localhost                  : ok=0    changed=0    unreachable=0    failed=0 
&lt;/pre&gt;
&lt;p&gt;We can see that no task is executed when custom files are not present, no output from the task (hidden skip).&lt;/p&gt;
&lt;h2&gt;Analysis and own opinion&lt;/h2&gt;
&lt;p&gt;Both methods do the same, both copy custom files when are present and both skip copy task when are not present.
What are the differences between both methods?&lt;/p&gt;

&lt;p&gt;To_skip method is simpler to read and unified in a single task, to_conditional is created within two tasks.
To_conditional method takes longer to be executed as it has to check the existence of a file and then evaluate a conditional.&lt;/p&gt;

&lt;p&gt;You may think that to_skip method is better than to_conditional method, that’s right in terms of code syntax and execution times. But…
As both, operator and infrastructure developer, I always use to_conditional method because when I’m deploying something new, I want to know what is executed and what not. In to_skip method you don’t know because there is no output provided from the task(not really true) but in to_conditional method it clearly says Skipping.&lt;/p&gt;

&lt;p&gt;Execution times are not a problem in most use cases, as is not commonly used this kind of tasks in CM systems, only a few tasks will need this type of logic.&lt;/p&gt;

&lt;p&gt;Regards, Eduardo Gonzalez&lt;/p&gt;</content><author><name>Editor</name></author><category term="analysis" /><category term="ansible" /><category term="code" /><category term="conditional" /><category term="logic" /><category term="opinion" /><category term="skip" /><category term="syntax" /><category term="True" /><category term="when" /><category term="with_first_found" /><summary type="html">Have you ever think about if an Ansible task should be skipped with a conditional or without (hidden skip)?. Well, this post will analyse both methods. Let’s use a example to create the same result and analyse the both methods: In OpenStack Kolla we found that sometimes operators need to customise policy.json files. That’s fine, but the problem is that policy.json files are installed with the services by default and we don’t need/want to maintain policy.json files in our repository because for sure will cause bugs from outdated policy files in the future. What’s the proposed solution to this? Allow operators use their own policy files only when their files exists in a custom configuration folder. If custom files are not present, default policy.json files are already present as part of the software installation. ( Actually this change is under review ) To Conditional method Code snippet: - name: Check if file exists stat: path: &quot;/tmp/custom_file.json&quot; register: check_custom_file_exist - name: Copy custom policy when exist template: src: &quot;/tmp/custom_file.json&quot; dest: &quot;/tmp/destination_file.json&quot; when: &quot;&quot; The first task checks if the file is present and register the stat result. The second task, copy the file only when the registered result of the previous task is True. (exists == True) Outputs the following when the file is not present: PLAY [localhost] *************************************************************** TASK [Check if file exists] **************************************************** ok: [localhost] TASK [Copy custom policy when exist] ******************************************* skipping: [localhost] PLAY RECAP ********************************************************************* localhost : ok=1 changed=0 unreachable=0 failed=0 We can see the copy file task is skipped with a skipping message. To Skip method Code snippet: - name: Copy custom policy when exist template: src: &quot;&quot; dest: &quot;/tmp/destination_file.json&quot; with_first_found: - files: - custom_file.json skip: True This playbook contains a single task, this task will use the first found file in a list of files. If no file is present will skip the task. Output from this execution: PLAY [localhost] *************************************************************** TASK [Copy custom policy when exist] ******************************************* PLAY RECAP ********************************************************************* localhost : ok=0 changed=0 unreachable=0 failed=0 We can see that no task is executed when custom files are not present, no output from the task (hidden skip). Analysis and own opinion Both methods do the same, both copy custom files when are present and both skip copy task when are not present. What are the differences between both methods? To_skip method is simpler to read and unified in a single task, to_conditional is created within two tasks. To_conditional method takes longer to be executed as it has to check the existence of a file and then evaluate a conditional. You may think that to_skip method is better than to_conditional method, that’s right in terms of code syntax and execution times. But… As both, operator and infrastructure developer, I always use to_conditional method because when I’m deploying something new, I want to know what is executed and what not. In to_skip method you don’t know because there is no output provided from the task(not really true) but in to_conditional method it clearly says Skipping. Execution times are not a problem in most use cases, as is not commonly used this kind of tasks in CM systems, only a few tasks will need this type of logic. Regards, Eduardo Gonzalez</summary></entry><entry><title type="html">Spacewalk (Red Hat Satellite v5) in a Docker container PoC</title><link href="http://217.182.136.201:5000/spacewalk-red-hat-satellite-v5-in-a-docker-container-poc/" rel="alternate" type="text/html" title="Spacewalk (Red Hat Satellite v5) in a Docker container PoC" /><published>2016-07-12T20:57:17+00:00</published><updated>2016-07-12T20:57:17+00:00</updated><id>http://217.182.136.201:5000/spacewalk-red-hat-satellite-v5-in-a-docker-container-poc</id><content type="html" xml:base="http://217.182.136.201:5000/spacewalk-red-hat-satellite-v5-in-a-docker-container-poc/">&lt;p&gt;Spacewalk was the upstream project to provide a Linux systems management layer on which Red Hat Satellite was based, was based at least until RH Satellite version 5. Newer versions are not anymore based on Spacewalk, instead Satellite is a federation of several upstream open source projects, including Katello, Foreman, Pulp, and Candlepin.&lt;/p&gt;

&lt;p&gt;Some weeks ago, a friend asked me if I knew a Docker container image for Satellite.
I have not found any image. What I found was some Spacewalk images, but sadly none of them worked for me.
I decided to create an image for this purpose.&lt;/p&gt;

&lt;p&gt;While developing the image, I found serious troubles to make it run with systemd (I’m a fan of systemd, but not inside containers yet).
The result was a semi functional working image. I said semi functional because some Spacewalk features are not working (probably an issue with systemd again).
The main problem was that spacewalk-setup script starts and uses systemd to configure the database and the other needed services, that’s OK in a VM but not in a container.
So i needed to hack into postgres setup and start the services with the typical &lt;code&gt;command --config-file file.conf&lt;/code&gt; executed from supervisord as Docker entrypoint.
Currently there is an issue with &lt;code&gt;osa-dispatcher&lt;/code&gt;, on which I can’t find a fix to make it run.&lt;/p&gt;

&lt;p&gt;This image is primarily created just for test Spacewalk interface and be more comfortable with it aka testing/development purposes, or just to have fun hacking with Docker containers.&lt;/p&gt;

&lt;p&gt;Now, I’m going to make a short description of what the Dockerfile makes and then start the container.
Have fun.&lt;/p&gt;

&lt;p&gt;I used centos as image base for this PoC&lt;/p&gt;
&lt;pre&gt;
FROM centos:7 
&lt;/pre&gt;
&lt;p&gt;Typical Maintainer line&lt;/p&gt;
&lt;pre&gt;
MAINTAINER Eduardo Gonzalez Gutierrez &amp;lt;dabarren@gmail.com&amp;gt;
&lt;/pre&gt;
&lt;p&gt;Add jpackage repo which provides Java packages for Linux&lt;/p&gt;
&lt;pre&gt;
COPY jpackage-generic.repo /etc/yum.repos.d/jpackage-generic.repo
&lt;/pre&gt;
&lt;p&gt;Install EPEL and Spacewalk repositories, after install, clean all stored cache to minimize image size&lt;/p&gt;
&lt;pre&gt;
RUN yum install -y http://yum.spacewalkproject.org/2.5/RHEL/7/x86_64/spacewalk-repo-2.5-3.el7.noarch.rpm \
        epel-release &amp;amp;&amp;amp; \
        yum clean all
&lt;/pre&gt;
&lt;p&gt;Import Keys to allow installation from these repositories&lt;/p&gt;
&lt;pre&gt;
RUN rpm --import http://www.jpackage.org/jpackage.asc &amp;amp;&amp;amp; \
    rpm --import https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7 &amp;amp;&amp;amp; \
    rpm --import http://yum.spacewalkproject.org/RPM-GPG-KEY-spacewalk-2015 &amp;amp;&amp;amp; \
    yum clean all
&lt;/pre&gt;
&lt;p&gt;Install spacewalk and supervisord packages&lt;/p&gt;
&lt;pre&gt;
RUN yum -y install \
        spacewalk-setup-postgresql \
        spacewalk-postgresql \
        supervisor  \
        yum clean all
&lt;/pre&gt;
&lt;p&gt;Copy the example file used to sync spacewalk database in a later step&lt;/p&gt;
&lt;pre&gt;
COPY answerfile.txt /tmp/answerfile.txt
&lt;/pre&gt;
&lt;p&gt;Open necessary ports&lt;/p&gt;
&lt;pre&gt;
EXPOSE 80 443 5222 68 69
&lt;/pre&gt;
&lt;p&gt;Change to postgres user&lt;/p&gt;
&lt;pre&gt;
USER postgres
&lt;/pre&gt;
&lt;p&gt;Initialize the database&lt;/p&gt;
&lt;pre&gt;
RUN /usr/bin/pg_ctl initdb  -D /var/lib/pgsql/data/
&lt;/pre&gt;
&lt;p&gt;Create spacewalk database, user, role and create pltclu language&lt;/p&gt;
&lt;pre&gt;
RUN /usr/bin/pg_ctl start -D /var/lib/pgsql/data/  -w -t 300 &amp;amp;&amp;amp; \
     psql -c 'CREATE DATABASE spaceschema' &amp;amp;&amp;amp; \
     psql -c &quot;CREATE USER spaceuser WITH PASSWORD 'spacepw'&quot; &amp;amp;&amp;amp; \
     psql -c 'ALTER ROLE spaceuser SUPERUSER' &amp;amp;&amp;amp; \
     createlang pltclu spaceschema
&lt;/pre&gt;
&lt;p&gt;Change to root user&lt;/p&gt;
&lt;pre&gt;
USER root
&lt;/pre&gt;
&lt;p&gt;Start the database and execute spacewalk configuration script&lt;/p&gt;
&lt;pre&gt;
RUN su -c &quot;/usr/bin/pg_ctl start -D /var/lib/pgsql/data/  -w -t 300&quot; postgres &amp;amp;&amp;amp; \
    su -c &quot;spacewalk-setup --answer-file=/tmp/answerfile.txt --skip-db-diskspace-check --skip-db-install&quot; root ; exit 0
&lt;/pre&gt;
&lt;p&gt;Copy supervisord configuration&lt;/p&gt;
&lt;pre&gt;
ADD supervisord.conf /etc/supervisord.d/supervisord.conf
&lt;/pre&gt;
&lt;p&gt;Use supervisord command to start all services at container launch time&lt;/p&gt;
&lt;pre&gt;
ENTRYPOINT supervisord -c /etc/supervisord.d/supervisord.conf
&lt;/pre&gt;
&lt;p&gt;You can check or download the source code at GitHub &lt;a href=&quot;https://github.com/egonzalez90/docker-spacewalk&quot; target=&quot;_blank&quot;&gt;https://github.com/egonzalez90/docker-spacewalk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I uploaded the image to DockerHub, which is auto-build from my GitHub repository, you can find it with the following command.&lt;/p&gt;
&lt;pre&gt;
[egonzalez@localhost ~]$ docker search spacewalk
INDEX       NAME                                       DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED
docker.io   docker.io/ruo91/spacewalk                  Spacewalk is an open source Linux systems ...   3                    [OK]
docker.io   docker.io/jamesnetherton/spacewalk         Spacewalk running under Docker                  1                    
docker.io   docker.io/coffmant/spacewalk-docker        Spacewalk                                       0                    [OK]
docker.io   docker.io/csabakollar/spacewalk            Spacewalk 2.4 in a CentOS6 container            0                    
docker.io   docker.io/egonzalez90/spacewalk            Spacewalk docker image                          0                    [OK]
docker.io   docker.io/jdostal/spacewalk-clients        Repository containing spacewalk-clients         0                    
docker.io   docker.io/jhutar/spacewalk-client                                                          0                    
docker.io   docker.io/norus/spacewalk-reposync                                                         0                    
docker.io   docker.io/pajinek/spacewalk-client                                                         0                    [OK]
docker.io   docker.io/perfectweb/spacewalk             spacewalk                                       0                    [OK]
docker.io   docker.io/researchiteng/docker-spacewalk   spacewalk is the open source version of Re...   0                    [OK]
docker.io   docker.io/varhoo/spacewalk-proxy                                                           0                    [OK]
&lt;/pre&gt;
&lt;p&gt;To start the container use the following command. If you don’t have the image locally, it will download the image from DockerHub&lt;/p&gt;
&lt;pre&gt;
[egonzalez@localhost ~]$ docker run -d --privileged=True egonzalez90/spacewalk
Unable to find image 'egonzalez90/spacewalk:latest' locally
Trying to pull repository docker.io/egonzalez90/spacewalk ... 
latest: Pulling from docker.io/egonzalez90/spacewalk
a3ed95caeb02: Already exists 
da71393503ec: Already exists 
519093688e2c: Pull complete 
97bbffaa9fc9: Pull complete 
63bfb115f62d: Pull complete 
929bbb68aff9: Pull complete 
532bc4af8e1a: Pull complete 
3eb667dda9ee: Pull complete 
275894897aa4: Pull complete 
93bcddf9cedb: Pull complete 
266c3b70754f: Pull complete 
Digest: sha256:a4dd98548f9dbb405fb4c6bb4a2a07b83d5f2bf730f29f71913b72876b1a61ab
Status: Downloaded newer image for docker.io/egonzalez90/spacewalk:latest
ded4a8b7eb1ee61fecc8ddc2eb1b092917a361bc36f7f752b32d76e79501d70a
&lt;/pre&gt;
&lt;p&gt;Now you have the container running, check if all the ports are properly exposed&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  [egonzalez@localhost ~]$ docker ps --latest --format 'table \t\t'
  CONTAINER ID        IMAGE                   PORTS
  ded4a8b7eb1e        egonzalez90/spacewalk   68-69/tcp, 80/tcp, 443/tcp, 5222/tcp
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Get the container IP address in order to enter from a Web Browser&lt;/p&gt;
&lt;pre&gt;
[egonzalez@localhost ~]$ docker inspect ded4a8b7eb1e | egrep IPAddress
            &quot;SecondaryIPAddresses&quot;: null,
            &quot;IPAddress&quot;: &quot;172.17.0.3&quot;,
                    &quot;IPAddress&quot;: &quot;172.17.0.3&quot;,
&lt;/pre&gt;
&lt;p&gt;Open A browser and go to the container IP address, if you use HTTP, by default it will redirect you to HTTPS.
The container uses an auto-signed SSL certificate, you have to add an exception in the Browser you use to allow connections to Spacewalk.
Once in the Welcome page, create an Organization.&lt;/p&gt;

&lt;p&gt;Now you are in Spacewalk and can play/test some features.
&lt;img src=&quot;http://egonzalez.org/wp-content/uploads/2016/07/Selection_003-1024x483.png&quot; alt=&quot;Selection_003&quot; width=&quot;900&quot; height=&quot;425&quot; class=&quot;alignnone size-large wp-image-1287&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is an issue I was not able to fix, so osa-dispatcher and some other features will not work with this image.
If someone can give me an input to fix the issue it will appreciated.&lt;/p&gt;
&lt;pre&gt;
[egonzalez@localhost ~]$ docker logs ded4a8b7eb1e | egrep FATAL
2016-07-12 18:13:32,220 INFO gave up: osa-dispatcher entered FATAL state, too many start retries too quickly
&lt;/pre&gt;

&lt;p&gt;Thanks for your time and hopes this image at least serves you to learn and play with the interface.&lt;/p&gt;

&lt;p&gt;Regards, Eduardo Gonzalez&lt;/p&gt;</content><author><name>Editor</name></author><category term="container" /><category term="docker" /><category term="dockerfile" /><category term="github" /><category term="image" /><category term="poc" /><category term="proofofconcept" /><category term="redhat" /><category term="satellite" /><category term="spacewalk" /><summary type="html">Spacewalk was the upstream project to provide a Linux systems management layer on which Red Hat Satellite was based, was based at least until RH Satellite version 5. Newer versions are not anymore based on Spacewalk, instead Satellite is a federation of several upstream open source projects, including Katello, Foreman, Pulp, and Candlepin. Some weeks ago, a friend asked me if I knew a Docker container image for Satellite. I have not found any image. What I found was some Spacewalk images, but sadly none of them worked for me. I decided to create an image for this purpose. While developing the image, I found serious troubles to make it run with systemd (I’m a fan of systemd, but not inside containers yet). The result was a semi functional working image. I said semi functional because some Spacewalk features are not working (probably an issue with systemd again). The main problem was that spacewalk-setup script starts and uses systemd to configure the database and the other needed services, that’s OK in a VM but not in a container. So i needed to hack into postgres setup and start the services with the typical command --config-file file.conf executed from supervisord as Docker entrypoint. Currently there is an issue with osa-dispatcher, on which I can’t find a fix to make it run. This image is primarily created just for test Spacewalk interface and be more comfortable with it aka testing/development purposes, or just to have fun hacking with Docker containers. Now, I’m going to make a short description of what the Dockerfile makes and then start the container. Have fun. I used centos as image base for this PoC FROM centos:7 Typical Maintainer line MAINTAINER Eduardo Gonzalez Gutierrez &amp;lt;dabarren@gmail.com&amp;gt; Add jpackage repo which provides Java packages for Linux COPY jpackage-generic.repo /etc/yum.repos.d/jpackage-generic.repo Install EPEL and Spacewalk repositories, after install, clean all stored cache to minimize image size RUN yum install -y http://yum.spacewalkproject.org/2.5/RHEL/7/x86_64/spacewalk-repo-2.5-3.el7.noarch.rpm \ epel-release &amp;amp;&amp;amp; \ yum clean all Import Keys to allow installation from these repositories RUN rpm --import http://www.jpackage.org/jpackage.asc &amp;amp;&amp;amp; \ rpm --import https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7 &amp;amp;&amp;amp; \ rpm --import http://yum.spacewalkproject.org/RPM-GPG-KEY-spacewalk-2015 &amp;amp;&amp;amp; \ yum clean all Install spacewalk and supervisord packages RUN yum -y install \ spacewalk-setup-postgresql \ spacewalk-postgresql \ supervisor \ yum clean all Copy the example file used to sync spacewalk database in a later step COPY answerfile.txt /tmp/answerfile.txt Open necessary ports EXPOSE 80 443 5222 68 69 Change to postgres user USER postgres Initialize the database RUN /usr/bin/pg_ctl initdb -D /var/lib/pgsql/data/ Create spacewalk database, user, role and create pltclu language RUN /usr/bin/pg_ctl start -D /var/lib/pgsql/data/ -w -t 300 &amp;amp;&amp;amp; \ psql -c 'CREATE DATABASE spaceschema' &amp;amp;&amp;amp; \ psql -c &quot;CREATE USER spaceuser WITH PASSWORD 'spacepw'&quot; &amp;amp;&amp;amp; \ psql -c 'ALTER ROLE spaceuser SUPERUSER' &amp;amp;&amp;amp; \ createlang pltclu spaceschema Change to root user USER root Start the database and execute spacewalk configuration script RUN su -c &quot;/usr/bin/pg_ctl start -D /var/lib/pgsql/data/ -w -t 300&quot; postgres &amp;amp;&amp;amp; \ su -c &quot;spacewalk-setup --answer-file=/tmp/answerfile.txt --skip-db-diskspace-check --skip-db-install&quot; root ; exit 0 Copy supervisord configuration ADD supervisord.conf /etc/supervisord.d/supervisord.conf Use supervisord command to start all services at container launch time ENTRYPOINT supervisord -c /etc/supervisord.d/supervisord.conf You can check or download the source code at GitHub https://github.com/egonzalez90/docker-spacewalk I uploaded the image to DockerHub, which is auto-build from my GitHub repository, you can find it with the following command. [egonzalez@localhost ~]$ docker search spacewalk INDEX NAME DESCRIPTION STARS OFFICIAL AUTOMATED docker.io docker.io/ruo91/spacewalk Spacewalk is an open source Linux systems ... 3 [OK] docker.io docker.io/jamesnetherton/spacewalk Spacewalk running under Docker 1 docker.io docker.io/coffmant/spacewalk-docker Spacewalk 0 [OK] docker.io docker.io/csabakollar/spacewalk Spacewalk 2.4 in a CentOS6 container 0 docker.io docker.io/egonzalez90/spacewalk Spacewalk docker image 0 [OK] docker.io docker.io/jdostal/spacewalk-clients Repository containing spacewalk-clients 0 docker.io docker.io/jhutar/spacewalk-client 0 docker.io docker.io/norus/spacewalk-reposync 0 docker.io docker.io/pajinek/spacewalk-client 0 [OK] docker.io docker.io/perfectweb/spacewalk spacewalk 0 [OK] docker.io docker.io/researchiteng/docker-spacewalk spacewalk is the open source version of Re... 0 [OK] docker.io docker.io/varhoo/spacewalk-proxy 0 [OK] To start the container use the following command. If you don’t have the image locally, it will download the image from DockerHub [egonzalez@localhost ~]$ docker run -d --privileged=True egonzalez90/spacewalk Unable to find image 'egonzalez90/spacewalk:latest' locally Trying to pull repository docker.io/egonzalez90/spacewalk ... latest: Pulling from docker.io/egonzalez90/spacewalk a3ed95caeb02: Already exists da71393503ec: Already exists 519093688e2c: Pull complete 97bbffaa9fc9: Pull complete 63bfb115f62d: Pull complete 929bbb68aff9: Pull complete 532bc4af8e1a: Pull complete 3eb667dda9ee: Pull complete 275894897aa4: Pull complete 93bcddf9cedb: Pull complete 266c3b70754f: Pull complete Digest: sha256:a4dd98548f9dbb405fb4c6bb4a2a07b83d5f2bf730f29f71913b72876b1a61ab Status: Downloaded newer image for docker.io/egonzalez90/spacewalk:latest ded4a8b7eb1ee61fecc8ddc2eb1b092917a361bc36f7f752b32d76e79501d70a Now you have the container running, check if all the ports are properly exposed</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://217.182.136.201:5000/wp-content/uploads/2016/07/Selection_003-e1468353022760.png" /></entry><entry><title type="html">MidoNet Integration with OpenStack Mitaka</title><link href="http://217.182.136.201:5000/midonet-integration-with-openstack-mitaka/" rel="alternate" type="text/html" title="MidoNet Integration with OpenStack Mitaka" /><published>2016-07-04T21:48:34+00:00</published><updated>2016-07-04T21:48:34+00:00</updated><id>http://217.182.136.201:5000/midonet-integration-with-openstack-mitaka</id><content type="html" xml:base="http://217.182.136.201:5000/midonet-integration-with-openstack-mitaka/">&lt;p&gt;MidoNet is an Open Source network virtualization software for IaaS infrastructure. 
It decouples your IaaS cloud from your network hardware, creating an intelligent software abstraction layer between your end hosts and your physical network.
This network abstraction layer allows the cloud operator to move what has traditionally been hardware-based network appliances into a software-based multi-tenant virtual domain.&lt;/p&gt;

&lt;p&gt;This definition from MidoNet documentation explains what MidoNet is and what MidoNet does.&lt;/p&gt;

&lt;p&gt;At this I will post cover my experiences integrating MidoNet with OpenStack.
I used the following configurations:&lt;/p&gt;

&lt;p&gt;All servers have CentOS 7.2 installed
OpenStack has been previously installed from RDO packages with multinode Packstack&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;x3 NSDB nodes (Casandra and Zookeeper services)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;x2 Gateway Nodes (Midolman Agent)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;x1 OpenStack Controller (MidoNet Cluster)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;x1 OpenStack compute node (Midolman Agent)&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;NSDB NODES&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Disable SElinux&lt;/p&gt;
&lt;pre&gt;
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux
&lt;/pre&gt;
&lt;p&gt;Install OpenStack Mitaka release repository&lt;/p&gt;
&lt;pre&gt;
sudo yum install -y centos-release-openstack-mitaka
&lt;/pre&gt;
&lt;p&gt;Add Cassandra repository&lt;/p&gt;
&lt;pre&gt;
cat &amp;lt;&amp;lt;EOF&amp;gt;/etc/yum.repos.d/datastax.repo
[datastax]
name = DataStax Repo for Apache Cassandra
baseurl = http://rpm.datastax.com/community
enabled = 1
gpgcheck = 1
gpgkey = https://rpm.datastax.com/rpm/repo_key
EOF
&lt;/pre&gt;
&lt;p&gt;Add Midonet repository&lt;/p&gt;
&lt;pre&gt;
cat &amp;lt;&amp;lt;EOF&amp;gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
&lt;/pre&gt;
&lt;p&gt;Clean repo cache and update packages&lt;/p&gt;
&lt;pre&gt;
yum clean all
yum update
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Zookeeper Configuration&lt;/strong&gt;
Install Zookeeper, java and dependencies&lt;/p&gt;
&lt;pre&gt;
yum install -y java-1.7.0-openjdk-headless zookeeper zkdump nmap-ncat
&lt;/pre&gt;
&lt;p&gt;Edit zookeeper configuration file&lt;/p&gt;
&lt;pre&gt;
vi /etc/zookeeper/zoo.cfg
&lt;/pre&gt;
&lt;p&gt;Add all NSDB nodes at the configuration file&lt;/p&gt;
&lt;pre&gt;
server.1=nsdb1:2888:3888
server.2=nsdb2:2888:3888
server.3=nsdb3:2888:3888
autopurge.snapRetainCount=10
autopurge.purgeInterval =12
&lt;/pre&gt;
&lt;p&gt;Create zookeeper folder on which zookeeper will store data, change the owner to zookeeper user&lt;/p&gt;
&lt;pre&gt;
mkdir /var/lib/zookeeper/data
chown zookeeper:zookeeper /var/lib/zookeeper/data
&lt;/pre&gt;
&lt;p&gt;Create myid file at zookeeper data folder, the ID should match with the NSDB node number, insert that number as follows:&lt;/p&gt;
&lt;pre&gt;
#NSDB1
echo 1 &amp;gt; /var/lib/zookeeper/data/myid
#NSDB2
echo 2 &amp;gt; /var/lib/zookeeper/data/myid
#NSDB3
echo 3 &amp;gt; /var/lib/zookeeper/data/myid
&lt;/pre&gt;
&lt;p&gt;Create java folder and create a softlink to it&lt;/p&gt;
&lt;pre&gt;
mkdir -p /usr/java/default/bin/
ln -s /usr/lib/jvm/jre-1.7.0-openjdk/bin/java /usr/java/default/bin/java
&lt;/pre&gt;
&lt;p&gt;Start and enable Zookeeper service&lt;/p&gt;
&lt;pre&gt;
systemctl enable zookeeper.service
systemctl start zookeeper.service
&lt;/pre&gt;
&lt;p&gt;Test if zookeeper is working locally&lt;/p&gt;
&lt;pre&gt;
echo ruok | nc 127.0.0.1 2181
imok
&lt;/pre&gt;
&lt;p&gt;Test if zookeeper is working at NSDB remote nodes&lt;/p&gt;
&lt;pre&gt;
echo stat | nc nsdb3 2181

Zookeeper version: 3.4.5--1, built on 02/08/2013 12:25 GMT
Clients:
 /192.168.100.172:35306[0](queued=0,recved=1,sent=0)

Latency min/avg/max: 0/0/0
Received: 1
Sent: 0
Connections: 1
Outstanding: 0
Zxid: 0x100000000
Mode: follower
Node count: 4
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Cassandra configuration&lt;/strong&gt;
Install Java and Cassandra dependencies&lt;/p&gt;
&lt;pre&gt;
yum install -y java-1.8.0-openjdk-headless dsc22
&lt;/pre&gt;
&lt;p&gt;Edit cassandra yaml file&lt;/p&gt;
&lt;pre&gt;
vi /etc/cassandra/conf/cassandra.yaml
&lt;/pre&gt;
&lt;p&gt;Change cluster_name to midonet
Configure seed_provider seeds to match all NSDB nodes
Configure listen_address and rpc_address to match the hostname of the self node&lt;/p&gt;
&lt;pre&gt;
cluster_name: 'midonet'
....
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: &quot;nsdb1,nsdb2,nsdb3&quot;

listen_address: nsdb1
rpc_address: nsdb1
&lt;/pre&gt;
&lt;p&gt;Edit cassandra’s init script in order to fix a bug in the init script&lt;/p&gt;
&lt;pre&gt;
vi /etc/init.d/cassandra
&lt;/pre&gt;
&lt;p&gt;Add the next two lines after #Casandra startup&lt;/p&gt;
&lt;pre&gt;
case &quot;$1&quot; in
    start)
        # Cassandra startup
        echo -n &quot;Starting Cassandra: &quot;
        mkdir -p /var/run/cassandra #Add this line
        chown cassandra:cassandra /var/run/cassandra #Add this line
        su $CASSANDRA_OWNR -c &quot;$CASSANDRA_PROG -p $pid_file&quot; &amp;gt; $log_file 2&amp;gt;&amp;amp;1
        retval=$?
        [ $retval -eq 0 ] &amp;amp;&amp;amp; touch $lock_file
        echo &quot;OK&quot;
        ;;
&lt;/pre&gt;
&lt;p&gt;Start and enable Cassandra service&lt;/p&gt;
&lt;pre&gt;
systemctl enable cassandra.service
systemctl start cassandra.service
&lt;/pre&gt;
&lt;p&gt;Check if all NSDB nodes join the cluster&lt;/p&gt;
&lt;pre&gt;
nodetool --host 127.0.0.1 status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address          Load       Tokens       Owns (effective)  Host ID                               Rack
UN  192.168.100.172  89.1 KB    256          70.8%             3f1ecedd-8caf-4938-84ad-8614d2134557  rack1
UN  192.168.100.224  67.64 KB   256          60.7%             cb36c999-a6e1-4d98-a4dd-d4230b41df08  rack1
UN  192.168.100.195  25.78 KB   256          68.6%             4758bae8-9300-4e57-9a61-5b1107082964  rack1
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Configure OpenStack Controller Nodes (On which Neutron Server is running)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Disable SElinux&lt;/p&gt;
&lt;pre&gt;
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux
&lt;/pre&gt;
&lt;p&gt;Install OpenStack Mitaka release repository&lt;/p&gt;
&lt;pre&gt;
sudo yum install -y centos-release-openstack-mitaka
&lt;/pre&gt;
&lt;p&gt;Add Midonet Repository&lt;/p&gt;
&lt;pre&gt;
cat &amp;lt;&amp;lt;EOF&amp;gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
&lt;/pre&gt;
&lt;p&gt;Clean repos cache and update the system&lt;/p&gt;
&lt;pre&gt;
yum clean all
yum update
&lt;/pre&gt;
&lt;p&gt;Create an OpenStack user for MidoNet, change the password to match your own&lt;/p&gt;
&lt;pre&gt;
# openstack user create --password temporal midonet
+----------+----------------------------------+
| Field    | Value                            |
+----------+----------------------------------+
| email    | None                             |
| enabled  | True                             |
| id       | ac25c5a77e7c4e4598ccadea89e09969 |
| name     | midonet                          |
| username | midonet                          |
+----------+----------------------------------+
&lt;/pre&gt;
&lt;p&gt;Add admin role at tenant services to Midonet user&lt;/p&gt;
&lt;pre&gt;
# openstack role add --project services --user midonet admin
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | bca2c6e1f3da42b0ba82aee401398a8a |
| name      | admin                            |
+-----------+----------------------------------+
&lt;/pre&gt;
&lt;p&gt;Create MidoNet service at Keystone&lt;/p&gt;
&lt;pre&gt;
# openstack service create --name midonet --description &quot;MidoNet API Service&quot; midonet
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | MidoNet API Service              |
| enabled     | True                             |
| id          | 499059c4a3a040cfb632411408a2be4c |
| name        | midonet                          |
| type        | midonet                          |
+-------------+----------------------------------+
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Clean up neutron server&lt;/strong&gt;
Stop neutron services&lt;/p&gt;
&lt;pre&gt;
openstack-service stop neutron
&lt;/pre&gt;
&lt;p&gt;Remove neutron database and recreate it again&lt;/p&gt;
&lt;pre&gt;
mysql -u root -p
DROP DATABASE neutron;
Query OK, 157 rows affected (11.50 sec)

MariaDB [(none)]&amp;gt; CREATE DATABASE neutron;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&amp;gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'ab4f81b1040a495e';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&amp;gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'ab4f81b1040a495e';
Query OK, 0 rows affected (0.00 sec)
MariaDB [(none)]&amp;gt; exit
Bye
&lt;/pre&gt;
&lt;p&gt;Remove plugin.ini symbolic link to ml2_conf.ini&lt;/p&gt;
&lt;pre&gt;
#rm /etc/neutron/plugin.ini 
rm: remove symbolic link ‘/etc/neutron/plugin.ini’? y
&lt;/pre&gt;
&lt;p&gt;Remove br-tun tunnel used by neutron in all the nodes&lt;/p&gt;
&lt;pre&gt;
ovs-vsctl del-br br-tun
&lt;/pre&gt;
&lt;p&gt;Install MidoNet packages and remove ml2 package&lt;/p&gt;
&lt;pre&gt;
yum install -y openstack-neutron python-networking-midonet python-neutronclient
yum remove openstack-neutron-ml2
&lt;/pre&gt;
&lt;p&gt;Make a backup of neutron configuration file&lt;/p&gt;
&lt;pre&gt;
cp /etc/neutron/neutron.conf neutron.conf.bak
&lt;/pre&gt;
&lt;p&gt;Edit neutron configuration file&lt;/p&gt;
&lt;pre&gt;
vi /etc/neutron/neutron.conf
&lt;/pre&gt;
&lt;p&gt;Most of the options are already configured by our older neutron configuration, change the ones who apply to match this configuration&lt;/p&gt;
&lt;pre&gt;
[DEFAULT]
core_plugin = midonet.neutron.plugin_v2.MidonetPluginV2
service_plugins = midonet.neutron.services.l3.l3_midonet.MidonetL3ServicePlugin
dhcp_agent_notification = False
allow_overlapping_ips = True
rpc_backend = rabbit
auth_strategy = keystone
notify_nova_on_port_status_changes = true
notify_nova_on_port_data_changes = true
nova_url = http://controller:8774/v2

[database]
connection = mysql+pymysql://neutron:ab4f81b1040a495e@controller/neutron

[oslo_messaging_rabbit]
rabbit_host = controller
rabbit_userid = guest
rabbit_password = guest

[keystone_authtoken]
auth_uri = http://controller:5000/v2.0
admin_user=neutron
admin_tenant_name=services
identity_uri=http://controller:35357
admin_password=d88f0bd060d64c33

[nova]
region_name = RegionOne
auth_url = http://controller:35357
auth_type = password
password = 9ca36d15e4824d93
project_domain_id = default
project_name = services
tenant_name = services
user_domain_id = default
username = nova

[oslo_concurrency]
lock_path = /var/lib/neutron/tmp
&lt;/pre&gt;
&lt;p&gt;At my deployment these are the options I had to change to configure midonet&lt;/p&gt;
&lt;pre&gt;
diff /etc/neutron/neutron.conf neutron.conf.bak 
33c33
&amp;lt; core_plugin = midonet.neutron.plugin_v2.MidonetPluginV2
---
&amp;gt; core_plugin = neutron.plugins.ml2.plugin.Ml2Plugin
37c37
&amp;lt; service_plugins = midonet.neutron.services.l3.l3_midonet.MidonetL3ServicePlugin
---
&amp;gt; service_plugins =router
120c120
&amp;lt; dhcp_agent_notification = False
---
&amp;gt; #dhcp_agent_notification = true
1087c1087,1088
&amp;lt; lock_path = /var/lib/neutron/tmp
---
&amp;gt; lock_path = $state_path/lock
&amp;gt; 
&lt;/pre&gt;
&lt;p&gt;Create midonet plugins folder&lt;/p&gt;
&lt;pre&gt;
mkdir /etc/neutron/plugins/midonet
&lt;/pre&gt;
&lt;p&gt;Create a file called midonet.ini&lt;/p&gt;
&lt;pre&gt;
vi /etc/neutron/plugins/midonet/midonet.ini
&lt;/pre&gt;
&lt;p&gt;Configure midonet.ini file to match your own configuration options&lt;/p&gt;
&lt;pre&gt;
[MIDONET]
# MidoNet API URL
midonet_uri = http://controller:8181/midonet-api
# MidoNet administrative user in Keystone
username = midonet
password = temporal
# MidoNet administrative user's tenant
project_id = services
&lt;/pre&gt;
&lt;p&gt;Create a symbolic link from midonet.ini to plugin.ini&lt;/p&gt;
&lt;pre&gt;
ln -s /etc/neutron/plugins/midonet/midonet.ini /etc/neutron/plugin.ini
&lt;/pre&gt;
&lt;p&gt;Sync and populate database tables with Midonet plugin&lt;/p&gt;
&lt;pre&gt;
su -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/midonet/midonet.ini upgrade head&quot; neutron
su -s /bin/sh -c &quot;neutron-db-manage --subproject networking-midonet upgrade head&quot; neutron
&lt;/pre&gt;
&lt;p&gt;Restart nova api and neutron server services&lt;/p&gt;
&lt;pre&gt;
systemctl restart openstack-nova-api.service
systemctl restart neutron-server
&lt;/pre&gt;
&lt;p&gt;Install midonet cluster package&lt;/p&gt;
&lt;pre&gt;
yum install -y midonet-cluster
&lt;/pre&gt;
&lt;p&gt;Configure midonet.conf file&lt;/p&gt;
&lt;pre&gt;
vi /etc/midonet/midonet.conf
&lt;/pre&gt;
&lt;p&gt;Add all NSDB nodes at zookeeper_hosts&lt;/p&gt;
&lt;pre&gt;
[zookeeper]
zookeeper_hosts = nsdb1:2181,nsdb2:2181,nsdb3:2181
&lt;/pre&gt;
&lt;p&gt;Configure midonet to make use of NSDB nodes as Zookeeper and cassandra hosts&lt;/p&gt;
&lt;pre&gt;
cat &amp;lt;&amp;lt; EOF | mn-conf set -t default
zookeeper {
    zookeeper_hosts = &quot;nsdb1:2181,nsdb2:2181,nsdb3:2181&quot;
}

cassandra {
    servers = &quot;nsdb1,nsdb2,nsdb3&quot;
}
EOF
&lt;/pre&gt;
&lt;p&gt;Set cassandra replication factor to 3&lt;/p&gt;
&lt;pre&gt;
echo &quot;cassandra.replication_factor : 3&quot; | mn-conf set -t default
&lt;/pre&gt;
&lt;p&gt;Grab your admin token&lt;/p&gt;
&lt;pre&gt;
#egrep ^admin_token /etc/keystone/keystone.conf 
admin_token = 7b84d89b32c34b71a697eb1a270807ab
&lt;/pre&gt;
&lt;p&gt;Configure Midonet to auth with keystone&lt;/p&gt;
&lt;pre&gt;
cat &amp;lt;&amp;lt; EOF | mn-conf set -t default
cluster.auth {
    provider_class = &quot;org.midonet.cluster.auth.keystone.KeystoneService&quot;
    admin_role = &quot;admin&quot;
    keystone.tenant_name = &quot;admin&quot;
    keystone.admin_token = &quot;7b84d89b32c34b71a697eb1a270807ab&quot;
    keystone.host = controller
    keystone.port = 35357
}
EOF
&lt;/pre&gt;
&lt;p&gt;Start and enable midonet cluster service&lt;/p&gt;
&lt;pre&gt;
systemctl enable midonet-cluster.service
systemctl start midonet-cluster.service
&lt;/pre&gt;
&lt;p&gt;Install midonet CLI&lt;/p&gt;
&lt;pre&gt;
yum install -y python-midonetclient
&lt;/pre&gt;
&lt;p&gt;Create a file at you home directory with midonet auth info&lt;/p&gt;
&lt;pre&gt;
#vi ~/.midonetrc

[cli]
api_url = http://controller:8181/midonet-api
username = admin
password = temporal
project_id = admin
&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Configure Compute nodes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Disable SElinux&lt;/p&gt;
&lt;pre&gt;
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux
&lt;/pre&gt;
&lt;p&gt;Install OpenStack Mitaka release repository&lt;/p&gt;
&lt;pre&gt;
sudo yum install -y centos-release-openstack-mitaka
&lt;/pre&gt;
&lt;p&gt;Add Midonet repository&lt;/p&gt;
&lt;pre&gt;
cat &amp;lt;&amp;lt;EOF&amp;gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
&lt;/pre&gt;
&lt;p&gt;Clean repos cache and update the system&lt;/p&gt;
&lt;pre&gt;
yum clean all
yum update
&lt;/pre&gt;
&lt;p&gt;Edit qemu.conf&lt;/p&gt;
&lt;pre&gt;
vi /etc/libvirt/qemu.conf
&lt;/pre&gt;
&lt;p&gt;Configure with the following options, by default all these options are commented, you can paste it all wherever you want&lt;/p&gt;
&lt;pre&gt;
user = &quot;root&quot;
group = &quot;root&quot;

cgroup_device_acl = [
    &quot;/dev/null&quot;, &quot;/dev/full&quot;, &quot;/dev/zero&quot;,
    &quot;/dev/random&quot;, &quot;/dev/urandom&quot;,
    &quot;/dev/ptmx&quot;, &quot;/dev/kvm&quot;, &quot;/dev/kqemu&quot;,
    &quot;/dev/rtc&quot;,&quot;/dev/hpet&quot;, &quot;/dev/vfio/vfio&quot;,
    &quot;/dev/net/tun&quot;
]
&lt;/pre&gt;
&lt;p&gt;Restart libvirtd service&lt;/p&gt;
&lt;pre&gt;
systemctl restart libvirtd.service
&lt;/pre&gt;
&lt;p&gt;Install nova-network package&lt;/p&gt;
&lt;pre&gt;
yum install -y openstack-nova-network
&lt;/pre&gt;
&lt;p&gt;Disable Nova Network service and restart Nova compute service&lt;/p&gt;
&lt;pre&gt;
systemctl disable openstack-nova-network.service
systemctl restart openstack-nova-compute.service
&lt;/pre&gt;
&lt;p&gt;Install Midolman agent and java packages&lt;/p&gt;
&lt;pre&gt;
yum install -y java-1.8.0-openjdk-headless midolman
&lt;/pre&gt;
&lt;p&gt;Configure midolman.conf&lt;/p&gt;
&lt;pre&gt;
vi /etc/midolman/midolman.conf
&lt;/pre&gt;
&lt;p&gt;Add all nsdb nodes as zookeeper hosts&lt;/p&gt;
&lt;pre&gt;
[zookeeper]
zookeeper_hosts = nsdb1:2181,nsdb2:2181,nsdb3:2181
&lt;/pre&gt;
&lt;p&gt;Configure each compute node with an appropiate flavor located at /etc/midolman/ folder, the have different hardware resources configured, use the one that better match your compute host capabilities&lt;/p&gt;
&lt;pre&gt;
mn-conf template-set -h local -t agent-compute-medium
cp /etc/midolman/midolman-env.sh.compute.medium /etc/midolman/midolman-env.sh
&lt;/pre&gt;
&lt;p&gt;Configure metadata, issue the following commands only once, it will automatically populate the configuration to all midonet agents&lt;/p&gt;
&lt;pre&gt;
echo &quot;agent.openstack.metadata.nova_metadata_url : \&quot;http://controller:8775\&quot;&quot; | mn-conf set -t default
echo &quot;agent.openstack.metadata.shared_secret : 2bfeb930a90d435d&quot; | mn-conf set -t default
echo &quot;agent.openstack.metadata.enabled : true&quot; | mn-conf set -t default
&lt;/pre&gt;
&lt;p&gt;Allow metadata trafic at iptables&lt;/p&gt;
&lt;pre&gt;
iptables -I INPUT 1 -i metadata -j ACCEPT
&lt;/pre&gt;
&lt;p&gt;Remove br-tun bridge&lt;/p&gt;
&lt;pre&gt;
ovs-vsctl del-br br-tun
&lt;/pre&gt;
&lt;p&gt;Start and enable midolman agent service&lt;/p&gt;
&lt;pre&gt;
systemctl enable midolman.service
systemctl start midolman.service
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Gateway nodes configuration&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Disable SElinux&lt;/p&gt;
&lt;pre&gt;
setenforce 0
sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux
&lt;/pre&gt;
&lt;p&gt;Install OpenStack Mitaka release repository&lt;/p&gt;
&lt;pre&gt;
sudo yum install -y centos-release-openstack-mitaka
&lt;/pre&gt;
&lt;p&gt;Add Midonet repository&lt;/p&gt;
&lt;pre&gt;
cat &amp;lt;&amp;lt;EOF&amp;gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
&lt;/pre&gt;
&lt;p&gt;Clean repos cache and update the system&lt;/p&gt;
&lt;pre&gt;
yum clean all
yum update
&lt;/pre&gt;
&lt;p&gt;Install Midolman agent and java packages&lt;/p&gt;
&lt;pre&gt;
yum install -y java-1.8.0-openjdk-headless midolman
&lt;/pre&gt;
&lt;p&gt;Configure midolman.conf&lt;/p&gt;
&lt;pre&gt;
vi /etc/midolman/midolman.conf
&lt;/pre&gt;
&lt;p&gt;Add all nsdb nodes as zookeeper hosts&lt;/p&gt;
&lt;pre&gt;
[zookeeper]
zookeeper_hosts = nsdb1:2181,nsdb2:2181,nsdb3:2181
&lt;/pre&gt;
&lt;p&gt;Configure each gateway node with an appropiate flavor located at /etc/midolman/ folder, the have different hardware resources configured, use the one that better match your gateway host capabilities&lt;/p&gt;
&lt;pre&gt;
mn-conf template-set -h local -t agent-gateway-medium
cp /etc/midolman/midolman-env.sh.gateway.medium /etc/midolman/midolman-env.sh
&lt;/pre&gt;
&lt;p&gt;Grab the metadata shared secret located at nova.conf at any of your nova nodes&lt;/p&gt;
&lt;pre&gt;
# egrep ^metadata_proxy_shared_secret /etc/nova/nova.conf 
metadata_proxy_shared_secret =2bfeb930a90d435d
&lt;/pre&gt;
&lt;p&gt;Allow metadata trafic at iptables&lt;/p&gt;
&lt;pre&gt;
iptables -I INPUT 1 -i metadata -j ACCEPT
&lt;/pre&gt;
&lt;p&gt;Start and enable midolman agent service&lt;/p&gt;
&lt;pre&gt;
systemctl enable midolman.service
systemctl start midolman.service
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Configure encapsulation and register nodes&lt;/strong&gt;
Enter to midonet CLI from a controller node&lt;/p&gt;
&lt;pre&gt;
midonet-cli
&lt;/pre&gt;
&lt;p&gt;Create the tunnel zone with VXLAN encapsulation&lt;/p&gt;
&lt;pre&gt;
midonet&amp;gt; tunnel-zone create name tz type vxlan
tzone0
midonet&amp;gt; list tunnel-zone
tzone tzone0 name tz type vxlan
&lt;/pre&gt;
&lt;p&gt;List hosts discovered by midonet, should be all the nodes where you configured midonet agents(midolman)&lt;/p&gt;
&lt;pre&gt;
midonet&amp;gt; list host
host host0 name gateway2 alive true addresses fe80:0:0:0:0:11ff:fe00:1102,169.254.123.1,fe80:0:0:0:0:11ff:fe00:1101,127.0.0.1,0:0:0:0:0:0:0:1,192.168.200.176,fe80:0:0:0:5054:ff:fef9:b2a0,169.254.169.254,fe80:0:0:0:7874:d6ff:fe5b:dea8,192.168.100.227,fe80:0:0:0:5054:ff:fed9:9cc0,fe80:0:0:0:5054:ff:fe4a:e39b,192.168.1.86 flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
host host1 name gateway1 alive true addresses 169.254.169.254,fe80:0:0:0:3cd1:23ff:feac:a3c2,192.168.1.87,fe80:0:0:0:5054:ff:fea8:da91,127.0.0.1,0:0:0:0:0:0:0:1,fe80:0:0:0:5054:ff:feec:92c1,192.168.200.232,fe80:0:0:0:0:11ff:fe00:1102,169.254.123.1,fe80:0:0:0:0:11ff:fe00:1101,192.168.100.141,fe80:0:0:0:5054:ff:fe20:30fb flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
host host2 name compute1 alive true addresses fe80:0:0:0:0:11ff:fe00:1101,169.254.123.1,127.0.0.1,0:0:0:0:0:0:0:1,fe80:0:0:0:0:11ff:fe00:1102,192.168.100.173,fe80:0:0:0:5054:ff:fe06:161,fe80:0:0:0:5054:ff:fee3:eb48,192.168.200.251,fe80:0:0:0:5054:ff:fe8d:d22,192.168.1.93,169.254.169.254,fe80:0:0:0:48cb:adff:fe69:f07b flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
&lt;/pre&gt;
&lt;p&gt;Register each of the nodes at the VXLAN zone we created before&lt;/p&gt;
&lt;pre&gt;
midonet&amp;gt; tunnel-zone tzone0 add member host host0 address 192.168.100.227
zone tzone0 host host0 address 192.168.100.227
midonet&amp;gt; tunnel-zone tzone0 add member host host1 address 192.168.100.141
zone tzone0 host host1 address 192.168.100.141
midonet&amp;gt; tunnel-zone tzone0 add member host host2 address 192.168.100.173
zone tzone0 host host2 address 192.168.100.173
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Create Networks at Neutron&lt;/strong&gt;
Create an external network&lt;/p&gt;
&lt;pre&gt;
# neutron net-create ext-net --router:external
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:47:30                  |
| description           |                                      |
| id                    | dc15245e-4391-4514-b489-8976373046a3 |
| is_default            | False                                |
| name                  | ext-net                              |
| port_security_enabled | True                                 |
| provider:network_type | midonet                              |
| router:external       | True                                 |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
| updated_at            | 2016-07-03T14:47:30                  |
+-----------------------+--------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Create an external subnet in the network we created before, use you own IP ranges to match your environment&lt;/p&gt;
&lt;pre&gt;
# neutron subnet-create ext-net 192.168.200.0/24 --name ext-subnet \
  --allocation-pool start=192.168.200.225,end=192.168.200.240 \
  --disable-dhcp --gateway 192.168.200.1
Created a new subnet:
+-------------------+--------------------------------------------------------+
| Field             | Value                                                  |
+-------------------+--------------------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;192.168.200.225&quot;, &quot;end&quot;: &quot;192.168.200.240&quot;} |
| cidr              | 192.168.200.0/24                                       |
| created_at        | 2016-07-03T14:50:46                                    |
| description       |                                                        |
| dns_nameservers   |                                                        |
| enable_dhcp       | False                                                  |
| gateway_ip        | 192.168.200.1                                          |
| host_routes       |                                                        |
| id                | 234dcc9a-2878-4799-b564-bf3a1bd52cad                   |
| ip_version        | 4                                                      |
| ipv6_address_mode |                                                        |
| ipv6_ra_mode      |                                                        |
| name              | ext-subnet                                             |
| network_id        | dc15245e-4391-4514-b489-8976373046a3                   |
| subnetpool_id     |                                                        |
| tenant_id         | 2f7ee2716b3b4140be57b4a5b26401e3                       |
| updated_at        | 2016-07-03T14:50:46                                    |
+-------------------+--------------------------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Create a tenant network and a subnet on it&lt;/p&gt;
&lt;pre&gt;
# neutron net-create demo-net
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:51:39                  |
| description           |                                      |
| id                    | 075ba699-dc4c-4625-8e0d-0a258a9aeb7d |
| name                  | demo-net                             |
| port_security_enabled | True                                 |
| provider:network_type | midonet                              |
| router:external       | False                                |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
| updated_at            | 2016-07-03T14:51:39                  |
+-----------------------+--------------------------------------+
# neutron subnet-create demo-net 10.0.20.0/24 --name demo-subnet 
Created a new subnet:
+-------------------+----------------------------------------------+
| Field             | Value                                        |
+-------------------+----------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;10.0.20.2&quot;, &quot;end&quot;: &quot;10.0.20.254&quot;} |
| cidr              | 10.0.20.0/24                                 |
| created_at        | 2016-07-03T14:52:32                          |
| description       |                                              |
| dns_nameservers   |                                              |
| enable_dhcp       | True                                         |
| gateway_ip        | 10.0.20.1                                    |
| host_routes       |                                              |
| id                | b299d899-33a3-4bfa-aff4-fda071545bdf         |
| ip_version        | 4                                            |
| ipv6_address_mode |                                              |
| ipv6_ra_mode      |                                              |
| name              | demo-subnet                                  |
| network_id        | 075ba699-dc4c-4625-8e0d-0a258a9aeb7d         |
| subnetpool_id     |                                              |
| tenant_id         | 2f7ee2716b3b4140be57b4a5b26401e3             |
| updated_at        | 2016-07-03T14:52:32                          |
+-------------------+----------------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Create a tenant router&lt;/p&gt;
&lt;pre&gt;
# neutron router-create router1
Created a new router:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| description           |                                      |
| external_gateway_info |                                      |
| id                    | 258942d8-9d82-4ebd-b829-c7bdfcc973f5 |
| name                  | router1                              |
| routes                |                                      |
| status                | ACTIVE                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
+-----------------------+--------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Attach the tenant subnet interface we created before to the router&lt;/p&gt;
&lt;pre&gt;
# neutron router-interface-add router1 demo-subnet
Added interface 06c85a56-368c-4d79-bbf0-4bb077f163e5 to router router1.
&lt;/pre&gt;
&lt;p&gt;Set the external network as router gateway&lt;/p&gt;
&lt;pre&gt;
# neutron router-gateway-set router1 ext-net
Set gateway for router router1
&lt;/pre&gt;
&lt;p&gt;Now, you can create an instance at tenant network&lt;/p&gt;
&lt;pre&gt;
# nova boot --flavor m1.tiny --image 80871834-29dd-4100-b038-f5f83f126204 --nic net-id=075ba699-dc4c-4625-8e0d-0a258a9aeb7d test1
+--------------------------------------+-----------------------------------------------------+
| Property                             | Value                                               |
+--------------------------------------+-----------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                              |
| OS-EXT-AZ:availability_zone          |                                                     |
| OS-EXT-SRV-ATTR:host                 | -                                                   |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                                   |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000000a                                   |
| OS-EXT-STS:power_state               | 0                                                   |
| OS-EXT-STS:task_state                | scheduling                                          |
| OS-EXT-STS:vm_state                  | building                                            |
| OS-SRV-USG:launched_at               | -                                                   |
| OS-SRV-USG:terminated_at             | -                                                   |
| accessIPv4                           |                                                     |
| accessIPv6                           |                                                     |
| adminPass                            | q2Cq4kxePSLL                                        |
| config_drive                         |                                                     |
| created                              | 2016-07-03T15:46:19Z                                |
| flavor                               | m1.tiny (1)                                         |
| hostId                               |                                                     |
| id                                   | b8aa46f9-186c-4594-8428-f8dbb16a5e16                |
| image                                | cirros image (80871834-29dd-4100-b038-f5f83f126204) |
| key_name                             | -                                                   |
| metadata                             | {}                                                  |
| name                                 | test1                                               |
| os-extended-volumes:volumes_attached | []                                                  |
| progress                             | 0                                                   |
| security_groups                      | default                                             |
| status                               | BUILD                                               |
| tenant_id                            | 2f7ee2716b3b4140be57b4a5b26401e3                    |
| updated                              | 2016-07-03T15:46:20Z                                |
| user_id                              | a2482a91a1f14750b372445d28b07c75                    |
+--------------------------------------+-----------------------------------------------------+
# nova list
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| ID                                   | Name  | Status | Task State | Power State | Networks            |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| b8aa46f9-186c-4594-8428-f8dbb16a5e16 | test1 | ACTIVE | -          | Running     | demo-net=10.0.20.11 |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
&lt;/pre&gt;
&lt;p&gt;Ensure the instance gets IP and the metadata service is properly running&lt;/p&gt;
&lt;pre&gt;
# nova console-log test1
...#Snipp from the output
Sending discover...
Sending select for 10.0.20.11...
Lease of 10.0.20.11 obtained, lease time 86400
cirros-ds 'net' up at 7.92
checking http://169.254.169.254/2009-04-04/instance-id
successful after 1/20 tries: up 8.22. iid=i-0000000a
...
&lt;/pre&gt;
&lt;p&gt;If you login to the instance through VNC you should be able to ping another instances&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Edge router configuration&lt;/strong&gt;
Create a new router&lt;/p&gt;
&lt;pre&gt;
# neutron router-create edge-router
Created a new router:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| description           |                                      |
| external_gateway_info |                                      |
| id                    | 5ecadb64-cb0d-4f95-a00e-aa1dd20a2012 |
| name                  | edge-router                          |
| routes                |                                      |
| status                | ACTIVE                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
+-----------------------+--------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Attach the external subnet interface to the router&lt;/p&gt;
&lt;pre&gt;
# neutron router-interface-add edge-router ext-subnet
Added interface e37f1986-c6b1-47f4-8268-02b837ceac17 to router edge-router.
&lt;/pre&gt;
&lt;p&gt;Create an uplink network&lt;/p&gt;
&lt;pre&gt;
# neutron net-create uplink-network --tenant_id admin --provider:network_type uplink
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:57:15                  |
| description           |                                      |
| id                    | 77173ed4-6106-4515-af1c-3683897955f9 |
| name                  | uplink-network                       |
| port_security_enabled | True                                 |
| provider:network_type | uplink                               |
| router:external       | False                                |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | admin                                |
| updated_at            | 2016-07-03T14:57:15                  |
+-----------------------+--------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Create a subnet in the uplink network&lt;/p&gt;
&lt;pre&gt;
# neutron subnet-create --tenant_id admin --disable-dhcp --name uplink-subnet uplink-network 192.168.1.0/24
Created a new subnet:
+-------------------+--------------------------------------------------+
| Field             | Value                                            |
+-------------------+--------------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;192.168.1.2&quot;, &quot;end&quot;: &quot;192.168.1.254&quot;} |
| cidr              | 192.168.1.0/24                                   |
| created_at        | 2016-07-03T15:06:28                              |
| description       |                                                  |
| dns_nameservers   |                                                  |
| enable_dhcp       | False                                            |
| gateway_ip        | 192.168.1.1                                      |
| host_routes       |                                                  |
| id                | 4e98e789-20d3-45fd-a3b5-9bcf02d8a832             |
| ip_version        | 4                                                |
| ipv6_address_mode |                                                  |
| ipv6_ra_mode      |                                                  |
| name              | uplink-subnet                                    |
| network_id        | 77173ed4-6106-4515-af1c-3683897955f9             |
| subnetpool_id     |                                                  |
| tenant_id         | admin                                            |
| updated_at        | 2016-07-03T15:06:28                              |
+-------------------+--------------------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Create a port for each of the gateway nodes, interface should match with the NIC you want to use for binding the gateway nodes and a IP address for the same purposes&lt;/p&gt;
&lt;pre&gt;
# neutron port-create uplink-network --binding:host_id gateway1 --binding:profile type=dict interface_name=eth1 --fixed-ip ip_address=192.168.1.199
Created a new port:
+-----------------------+--------------------------------------------------------------------------------------+
| Field                 | Value                                                                                |
+-----------------------+--------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                 |
| allowed_address_pairs |                                                                                      |
| binding:host_id       | compute1                                                                             |
| binding:profile       | {&quot;interface_name&quot;: &quot;eth1&quot;}                                                           |
| binding:vif_details   | {&quot;port_filter&quot;: true}                                                                |
| binding:vif_type      | midonet                                                                              |
| binding:vnic_type     | normal                                                                               |
| created_at            | 2016-07-03T15:10:06                                                                  |
| description           |                                                                                      |
| device_id             |                                                                                      |
| device_owner          |                                                                                      |
| extra_dhcp_opts       |                                                                                      |
| fixed_ips             | {&quot;subnet_id&quot;: &quot;4e98e789-20d3-45fd-a3b5-9bcf02d8a832&quot;, &quot;ip_address&quot;: &quot;192.168.1.199&quot;} |
| id                    | 7b4f54dd-2b41-42ba-9c5c-cda4640dc550                                                 |
| mac_address           | fa:16:3e:44:a8:c9                                                                    |
| name                  |                                                                                      |
| network_id            | 77173ed4-6106-4515-af1c-3683897955f9                                                 |
| port_security_enabled | True                                                                                 |
| security_groups       | 0cf3e33e-dbd6-4b42-a0bd-6679b5eed4e1                                                 |
| status                | ACTIVE                                                                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3                                                     |
| updated_at            | 2016-07-03T15:10:06                                                                  |
+-----------------------+--------------------------------------------------------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Attach each of the ports to the edge router&lt;/p&gt;
&lt;pre&gt;
# neutron router-interface-add edge-router port=7b4f54dd-2b41-42ba-9c5c-cda4640dc550
Added interface 7b4f54dd-2b41-42ba-9c5c-cda4640dc550 to router edge-router.
&lt;/pre&gt;
&lt;p&gt;At this point you have to decide if use border routers with BGP enabled or static routes.
Use one of the following links to configure your use case:
&lt;a href=&quot;https://docs.midonet.org/docs/latest/operations-guide/content/bgp_uplink_configuration.html&quot; target=&quot;_blank&quot;&gt;https://docs.midonet.org/docs/latest/operations-guide/content/bgp_uplink_configuration.html&lt;/a&gt;
&lt;a href=&quot;https://docs.midonet.org/docs/latest/operations-guide/content/static_setup.html&quot; target=&quot;_blank&quot;&gt;https://docs.midonet.org/docs/latest/operations-guide/content/static_setup.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Issues I faced during configuration of Midonet&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Midolman agent don’t start:
It was caused because midolman-env.sh file has more RAM configured as the one of my server.
Edit the file to match your server resources&lt;/p&gt;
&lt;pre&gt;
# egrep ^MAX_HEAP_SIZE /etc/midolman/midolman-env.sh
MAX_HEAP_SIZE=&quot;2048M&quot;
&lt;/pre&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;
Instances doesn’t boot with the following error:&lt;/p&gt;
&lt;pre&gt;
could not open /dev/net/tun: Permission denied
&lt;/pre&gt;
&lt;p&gt;I had to remove br-tun bridges at ovs, if not, ovs locks the device and midolman cannot create the tunnel beetwen compute nodes and gateway nodes.&lt;/p&gt;
&lt;pre&gt;
ovs-vsctl del-br br-tun
&lt;/pre&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;
This post is my experience integrating Midonet into OpenStack, maybe some things are not correct, if you find any issue, please advise me to fix it.
Regards, Eduardo Gonzalez&lt;/p&gt;</content><author><name>Editor</name></author><category term="cloud" /><category term="config" /><category term="install" /><category term="integration" /><category term="manual" /><category term="midokura" /><category term="midonet" /><category term="mitaka" /><category term="network" /><category term="openstack" /><category term="packages" /><category term="packstack" /><category term="rdo" /><category term="sdn" /><summary type="html">MidoNet is an Open Source network virtualization software for IaaS infrastructure. It decouples your IaaS cloud from your network hardware, creating an intelligent software abstraction layer between your end hosts and your physical network. This network abstraction layer allows the cloud operator to move what has traditionally been hardware-based network appliances into a software-based multi-tenant virtual domain. This definition from MidoNet documentation explains what MidoNet is and what MidoNet does. At this I will post cover my experiences integrating MidoNet with OpenStack. I used the following configurations: All servers have CentOS 7.2 installed OpenStack has been previously installed from RDO packages with multinode Packstack x3 NSDB nodes (Casandra and Zookeeper services) x2 Gateway Nodes (Midolman Agent) x1 OpenStack Controller (MidoNet Cluster) x1 OpenStack compute node (Midolman Agent) &amp;lt;/br&amp;gt; NSDB NODES Disable SElinux setenforce 0 sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux Install OpenStack Mitaka release repository sudo yum install -y centos-release-openstack-mitaka Add Cassandra repository cat &amp;lt;&amp;lt;EOF&amp;gt;/etc/yum.repos.d/datastax.repo [datastax] name = DataStax Repo for Apache Cassandra baseurl = http://rpm.datastax.com/community enabled = 1 gpgcheck = 1 gpgkey = https://rpm.datastax.com/rpm/repo_key EOF Add Midonet repository cat &amp;lt;&amp;lt;EOF&amp;gt;/etc/yum.repos.d/midonet.repo [midonet] name=MidoNet baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key [midonet-openstack-integration] name=MidoNet OpenStack Integration baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key [midonet-misc] name=MidoNet 3rd Party Tools and Libraries baseurl=http://builds.midonet.org/misc/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key EOF Clean repo cache and update packages yum clean all yum update Zookeeper Configuration Install Zookeeper, java and dependencies yum install -y java-1.7.0-openjdk-headless zookeeper zkdump nmap-ncat Edit zookeeper configuration file vi /etc/zookeeper/zoo.cfg Add all NSDB nodes at the configuration file server.1=nsdb1:2888:3888 server.2=nsdb2:2888:3888 server.3=nsdb3:2888:3888 autopurge.snapRetainCount=10 autopurge.purgeInterval =12 Create zookeeper folder on which zookeeper will store data, change the owner to zookeeper user mkdir /var/lib/zookeeper/data chown zookeeper:zookeeper /var/lib/zookeeper/data Create myid file at zookeeper data folder, the ID should match with the NSDB node number, insert that number as follows: #NSDB1 echo 1 &amp;gt; /var/lib/zookeeper/data/myid #NSDB2 echo 2 &amp;gt; /var/lib/zookeeper/data/myid #NSDB3 echo 3 &amp;gt; /var/lib/zookeeper/data/myid Create java folder and create a softlink to it mkdir -p /usr/java/default/bin/ ln -s /usr/lib/jvm/jre-1.7.0-openjdk/bin/java /usr/java/default/bin/java Start and enable Zookeeper service systemctl enable zookeeper.service systemctl start zookeeper.service Test if zookeeper is working locally echo ruok | nc 127.0.0.1 2181 imok Test if zookeeper is working at NSDB remote nodes echo stat | nc nsdb3 2181 Zookeeper version: 3.4.5--1, built on 02/08/2013 12:25 GMT Clients: /192.168.100.172:35306[0](queued=0,recved=1,sent=0) Latency min/avg/max: 0/0/0 Received: 1 Sent: 0 Connections: 1 Outstanding: 0 Zxid: 0x100000000 Mode: follower Node count: 4 Cassandra configuration Install Java and Cassandra dependencies yum install -y java-1.8.0-openjdk-headless dsc22 Edit cassandra yaml file vi /etc/cassandra/conf/cassandra.yaml Change cluster_name to midonet Configure seed_provider seeds to match all NSDB nodes Configure listen_address and rpc_address to match the hostname of the self node cluster_name: 'midonet' .... seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: &quot;nsdb1,nsdb2,nsdb3&quot; listen_address: nsdb1 rpc_address: nsdb1 Edit cassandra’s init script in order to fix a bug in the init script vi /etc/init.d/cassandra Add the next two lines after #Casandra startup case &quot;$1&quot; in start) # Cassandra startup echo -n &quot;Starting Cassandra: &quot; mkdir -p /var/run/cassandra #Add this line chown cassandra:cassandra /var/run/cassandra #Add this line su $CASSANDRA_OWNR -c &quot;$CASSANDRA_PROG -p $pid_file&quot; &amp;gt; $log_file 2&amp;gt;&amp;amp;1 retval=$? [ $retval -eq 0 ] &amp;amp;&amp;amp; touch $lock_file echo &quot;OK&quot; ;; Start and enable Cassandra service systemctl enable cassandra.service systemctl start cassandra.service Check if all NSDB nodes join the cluster nodetool --host 127.0.0.1 status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN 192.168.100.172 89.1 KB 256 70.8% 3f1ecedd-8caf-4938-84ad-8614d2134557 rack1 UN 192.168.100.224 67.64 KB 256 60.7% cb36c999-a6e1-4d98-a4dd-d4230b41df08 rack1 UN 192.168.100.195 25.78 KB 256 68.6% 4758bae8-9300-4e57-9a61-5b1107082964 rack1 Configure OpenStack Controller Nodes (On which Neutron Server is running) Disable SElinux setenforce 0 sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux Install OpenStack Mitaka release repository sudo yum install -y centos-release-openstack-mitaka Add Midonet Repository cat &amp;lt;&amp;lt;EOF&amp;gt;/etc/yum.repos.d/midonet.repo [midonet] name=MidoNet baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key [midonet-openstack-integration] name=MidoNet OpenStack Integration baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key [midonet-misc] name=MidoNet 3rd Party Tools and Libraries baseurl=http://builds.midonet.org/misc/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key EOF Clean repos cache and update the system yum clean all yum update Create an OpenStack user for MidoNet, change the password to match your own # openstack user create --password temporal midonet +----------+----------------------------------+ | Field | Value | +----------+----------------------------------+ | email | None | | enabled | True | | id | ac25c5a77e7c4e4598ccadea89e09969 | | name | midonet | | username | midonet | +----------+----------------------------------+ Add admin role at tenant services to Midonet user # openstack role add --project services --user midonet admin +-----------+----------------------------------+ | Field | Value | +-----------+----------------------------------+ | domain_id | None | | id | bca2c6e1f3da42b0ba82aee401398a8a | | name | admin | +-----------+----------------------------------+ Create MidoNet service at Keystone # openstack service create --name midonet --description &quot;MidoNet API Service&quot; midonet +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | MidoNet API Service | | enabled | True | | id | 499059c4a3a040cfb632411408a2be4c | | name | midonet | | type | midonet | +-------------+----------------------------------+ Clean up neutron server Stop neutron services openstack-service stop neutron Remove neutron database and recreate it again mysql -u root -p DROP DATABASE neutron; Query OK, 157 rows affected (11.50 sec) MariaDB [(none)]&amp;gt; CREATE DATABASE neutron; Query OK, 1 row affected (0.00 sec) MariaDB [(none)]&amp;gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'ab4f81b1040a495e'; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]&amp;gt; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'ab4f81b1040a495e'; Query OK, 0 rows affected (0.00 sec) MariaDB [(none)]&amp;gt; exit Bye Remove plugin.ini symbolic link to ml2_conf.ini #rm /etc/neutron/plugin.ini rm: remove symbolic link ‘/etc/neutron/plugin.ini’? y Remove br-tun tunnel used by neutron in all the nodes ovs-vsctl del-br br-tun Install MidoNet packages and remove ml2 package yum install -y openstack-neutron python-networking-midonet python-neutronclient yum remove openstack-neutron-ml2 Make a backup of neutron configuration file cp /etc/neutron/neutron.conf neutron.conf.bak Edit neutron configuration file vi /etc/neutron/neutron.conf Most of the options are already configured by our older neutron configuration, change the ones who apply to match this configuration [DEFAULT] core_plugin = midonet.neutron.plugin_v2.MidonetPluginV2 service_plugins = midonet.neutron.services.l3.l3_midonet.MidonetL3ServicePlugin dhcp_agent_notification = False allow_overlapping_ips = True rpc_backend = rabbit auth_strategy = keystone notify_nova_on_port_status_changes = true notify_nova_on_port_data_changes = true nova_url = http://controller:8774/v2 [database] connection = mysql+pymysql://neutron:ab4f81b1040a495e@controller/neutron [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = guest rabbit_password = guest [keystone_authtoken] auth_uri = http://controller:5000/v2.0 admin_user=neutron admin_tenant_name=services identity_uri=http://controller:35357 admin_password=d88f0bd060d64c33 [nova] region_name = RegionOne auth_url = http://controller:35357 auth_type = password password = 9ca36d15e4824d93 project_domain_id = default project_name = services tenant_name = services user_domain_id = default username = nova [oslo_concurrency] lock_path = /var/lib/neutron/tmp At my deployment these are the options I had to change to configure midonet diff /etc/neutron/neutron.conf neutron.conf.bak 33c33 &amp;lt; core_plugin = midonet.neutron.plugin_v2.MidonetPluginV2 --- &amp;gt; core_plugin = neutron.plugins.ml2.plugin.Ml2Plugin 37c37 &amp;lt; service_plugins = midonet.neutron.services.l3.l3_midonet.MidonetL3ServicePlugin --- &amp;gt; service_plugins =router 120c120 &amp;lt; dhcp_agent_notification = False --- &amp;gt; #dhcp_agent_notification = true 1087c1087,1088 &amp;lt; lock_path = /var/lib/neutron/tmp --- &amp;gt; lock_path = $state_path/lock &amp;gt; Create midonet plugins folder mkdir /etc/neutron/plugins/midonet Create a file called midonet.ini vi /etc/neutron/plugins/midonet/midonet.ini Configure midonet.ini file to match your own configuration options [MIDONET] # MidoNet API URL midonet_uri = http://controller:8181/midonet-api # MidoNet administrative user in Keystone username = midonet password = temporal # MidoNet administrative user's tenant project_id = services Create a symbolic link from midonet.ini to plugin.ini ln -s /etc/neutron/plugins/midonet/midonet.ini /etc/neutron/plugin.ini Sync and populate database tables with Midonet plugin su -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/midonet/midonet.ini upgrade head&quot; neutron su -s /bin/sh -c &quot;neutron-db-manage --subproject networking-midonet upgrade head&quot; neutron Restart nova api and neutron server services systemctl restart openstack-nova-api.service systemctl restart neutron-server Install midonet cluster package yum install -y midonet-cluster Configure midonet.conf file vi /etc/midonet/midonet.conf Add all NSDB nodes at zookeeper_hosts [zookeeper] zookeeper_hosts = nsdb1:2181,nsdb2:2181,nsdb3:2181 Configure midonet to make use of NSDB nodes as Zookeeper and cassandra hosts cat &amp;lt;&amp;lt; EOF | mn-conf set -t default zookeeper { zookeeper_hosts = &quot;nsdb1:2181,nsdb2:2181,nsdb3:2181&quot; } cassandra { servers = &quot;nsdb1,nsdb2,nsdb3&quot; } EOF Set cassandra replication factor to 3 echo &quot;cassandra.replication_factor : 3&quot; | mn-conf set -t default Grab your admin token #egrep ^admin_token /etc/keystone/keystone.conf admin_token = 7b84d89b32c34b71a697eb1a270807ab Configure Midonet to auth with keystone cat &amp;lt;&amp;lt; EOF | mn-conf set -t default cluster.auth { provider_class = &quot;org.midonet.cluster.auth.keystone.KeystoneService&quot; admin_role = &quot;admin&quot; keystone.tenant_name = &quot;admin&quot; keystone.admin_token = &quot;7b84d89b32c34b71a697eb1a270807ab&quot; keystone.host = controller keystone.port = 35357 } EOF Start and enable midonet cluster service systemctl enable midonet-cluster.service systemctl start midonet-cluster.service Install midonet CLI yum install -y python-midonetclient Create a file at you home directory with midonet auth info #vi ~/.midonetrc [cli] api_url = http://controller:8181/midonet-api username = admin password = temporal project_id = admin Configure Compute nodes Disable SElinux setenforce 0 sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux Install OpenStack Mitaka release repository sudo yum install -y centos-release-openstack-mitaka Add Midonet repository cat &amp;lt;&amp;lt;EOF&amp;gt;/etc/yum.repos.d/midonet.repo [midonet] name=MidoNet baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key [midonet-openstack-integration] name=MidoNet OpenStack Integration baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key [midonet-misc] name=MidoNet 3rd Party Tools and Libraries baseurl=http://builds.midonet.org/misc/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key EOF Clean repos cache and update the system yum clean all yum update Edit qemu.conf vi /etc/libvirt/qemu.conf Configure with the following options, by default all these options are commented, you can paste it all wherever you want user = &quot;root&quot; group = &quot;root&quot; cgroup_device_acl = [ &quot;/dev/null&quot;, &quot;/dev/full&quot;, &quot;/dev/zero&quot;, &quot;/dev/random&quot;, &quot;/dev/urandom&quot;, &quot;/dev/ptmx&quot;, &quot;/dev/kvm&quot;, &quot;/dev/kqemu&quot;, &quot;/dev/rtc&quot;,&quot;/dev/hpet&quot;, &quot;/dev/vfio/vfio&quot;, &quot;/dev/net/tun&quot; ] Restart libvirtd service systemctl restart libvirtd.service Install nova-network package yum install -y openstack-nova-network Disable Nova Network service and restart Nova compute service systemctl disable openstack-nova-network.service systemctl restart openstack-nova-compute.service Install Midolman agent and java packages yum install -y java-1.8.0-openjdk-headless midolman Configure midolman.conf vi /etc/midolman/midolman.conf Add all nsdb nodes as zookeeper hosts [zookeeper] zookeeper_hosts = nsdb1:2181,nsdb2:2181,nsdb3:2181 Configure each compute node with an appropiate flavor located at /etc/midolman/ folder, the have different hardware resources configured, use the one that better match your compute host capabilities mn-conf template-set -h local -t agent-compute-medium cp /etc/midolman/midolman-env.sh.compute.medium /etc/midolman/midolman-env.sh Configure metadata, issue the following commands only once, it will automatically populate the configuration to all midonet agents echo &quot;agent.openstack.metadata.nova_metadata_url : \&quot;http://controller:8775\&quot;&quot; | mn-conf set -t default echo &quot;agent.openstack.metadata.shared_secret : 2bfeb930a90d435d&quot; | mn-conf set -t default echo &quot;agent.openstack.metadata.enabled : true&quot; | mn-conf set -t default Allow metadata trafic at iptables iptables -I INPUT 1 -i metadata -j ACCEPT Remove br-tun bridge ovs-vsctl del-br br-tun Start and enable midolman agent service systemctl enable midolman.service systemctl start midolman.service Gateway nodes configuration Disable SElinux setenforce 0 sed -i 's/SELINUX=enforcing/SELINUX=permissive/g' /etc/sysconfig/selinux Install OpenStack Mitaka release repository sudo yum install -y centos-release-openstack-mitaka Add Midonet repository cat &amp;lt;&amp;lt;EOF&amp;gt;/etc/yum.repos.d/midonet.repo [midonet] name=MidoNet baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key [midonet-openstack-integration] name=MidoNet OpenStack Integration baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key [midonet-misc] name=MidoNet 3rd Party Tools and Libraries baseurl=http://builds.midonet.org/misc/stable/el7/ enabled=1 gpgcheck=1 gpgkey=https://builds.midonet.org/midorepo.key EOF Clean repos cache and update the system yum clean all yum update Install Midolman agent and java packages yum install -y java-1.8.0-openjdk-headless midolman Configure midolman.conf vi /etc/midolman/midolman.conf Add all nsdb nodes as zookeeper hosts [zookeeper] zookeeper_hosts = nsdb1:2181,nsdb2:2181,nsdb3:2181 Configure each gateway node with an appropiate flavor located at /etc/midolman/ folder, the have different hardware resources configured, use the one that better match your gateway host capabilities mn-conf template-set -h local -t agent-gateway-medium cp /etc/midolman/midolman-env.sh.gateway.medium /etc/midolman/midolman-env.sh Grab the metadata shared secret located at nova.conf at any of your nova nodes # egrep ^metadata_proxy_shared_secret /etc/nova/nova.conf metadata_proxy_shared_secret =2bfeb930a90d435d Allow metadata trafic at iptables iptables -I INPUT 1 -i metadata -j ACCEPT Start and enable midolman agent service systemctl enable midolman.service systemctl start midolman.service Configure encapsulation and register nodes Enter to midonet CLI from a controller node midonet-cli Create the tunnel zone with VXLAN encapsulation midonet&amp;gt; tunnel-zone create name tz type vxlan tzone0 midonet&amp;gt; list tunnel-zone tzone tzone0 name tz type vxlan List hosts discovered by midonet, should be all the nodes where you configured midonet agents(midolman) midonet&amp;gt; list host host host0 name gateway2 alive true addresses fe80:0:0:0:0:11ff:fe00:1102,169.254.123.1,fe80:0:0:0:0:11ff:fe00:1101,127.0.0.1,0:0:0:0:0:0:0:1,192.168.200.176,fe80:0:0:0:5054:ff:fef9:b2a0,169.254.169.254,fe80:0:0:0:7874:d6ff:fe5b:dea8,192.168.100.227,fe80:0:0:0:5054:ff:fed9:9cc0,fe80:0:0:0:5054:ff:fe4a:e39b,192.168.1.86 flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false host host1 name gateway1 alive true addresses 169.254.169.254,fe80:0:0:0:3cd1:23ff:feac:a3c2,192.168.1.87,fe80:0:0:0:5054:ff:fea8:da91,127.0.0.1,0:0:0:0:0:0:0:1,fe80:0:0:0:5054:ff:feec:92c1,192.168.200.232,fe80:0:0:0:0:11ff:fe00:1102,169.254.123.1,fe80:0:0:0:0:11ff:fe00:1101,192.168.100.141,fe80:0:0:0:5054:ff:fe20:30fb flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false host host2 name compute1 alive true addresses fe80:0:0:0:0:11ff:fe00:1101,169.254.123.1,127.0.0.1,0:0:0:0:0:0:0:1,fe80:0:0:0:0:11ff:fe00:1102,192.168.100.173,fe80:0:0:0:5054:ff:fe06:161,fe80:0:0:0:5054:ff:fee3:eb48,192.168.200.251,fe80:0:0:0:5054:ff:fe8d:d22,192.168.1.93,169.254.169.254,fe80:0:0:0:48cb:adff:fe69:f07b flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false Register each of the nodes at the VXLAN zone we created before midonet&amp;gt; tunnel-zone tzone0 add member host host0 address 192.168.100.227 zone tzone0 host host0 address 192.168.100.227 midonet&amp;gt; tunnel-zone tzone0 add member host host1 address 192.168.100.141 zone tzone0 host host1 address 192.168.100.141 midonet&amp;gt; tunnel-zone tzone0 add member host host2 address 192.168.100.173 zone tzone0 host host2 address 192.168.100.173 Create Networks at Neutron Create an external network # neutron net-create ext-net --router:external Created a new network: +-----------------------+--------------------------------------+ | Field | Value | +-----------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2016-07-03T14:47:30 | | description | | | id | dc15245e-4391-4514-b489-8976373046a3 | | is_default | False | | name | ext-net | | port_security_enabled | True | | provider:network_type | midonet | | router:external | True | | shared | False | | status | ACTIVE | | subnets | | | tags | | | tenant_id | 2f7ee2716b3b4140be57b4a5b26401e3 | | updated_at | 2016-07-03T14:47:30 | +-----------------------+--------------------------------------+ Create an external subnet in the network we created before, use you own IP ranges to match your environment # neutron subnet-create ext-net 192.168.200.0/24 --name ext-subnet \ --allocation-pool start=192.168.200.225,end=192.168.200.240 \ --disable-dhcp --gateway 192.168.200.1 Created a new subnet: +-------------------+--------------------------------------------------------+ | Field | Value | +-------------------+--------------------------------------------------------+ | allocation_pools | {&quot;start&quot;: &quot;192.168.200.225&quot;, &quot;end&quot;: &quot;192.168.200.240&quot;} | | cidr | 192.168.200.0/24 | | created_at | 2016-07-03T14:50:46 | | description | | | dns_nameservers | | | enable_dhcp | False | | gateway_ip | 192.168.200.1 | | host_routes | | | id | 234dcc9a-2878-4799-b564-bf3a1bd52cad | | ip_version | 4 | | ipv6_address_mode | | | ipv6_ra_mode | | | name | ext-subnet | | network_id | dc15245e-4391-4514-b489-8976373046a3 | | subnetpool_id | | | tenant_id | 2f7ee2716b3b4140be57b4a5b26401e3 | | updated_at | 2016-07-03T14:50:46 | +-------------------+--------------------------------------------------------+ Create a tenant network and a subnet on it # neutron net-create demo-net Created a new network: +-----------------------+--------------------------------------+ | Field | Value | +-----------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2016-07-03T14:51:39 | | description | | | id | 075ba699-dc4c-4625-8e0d-0a258a9aeb7d | | name | demo-net | | port_security_enabled | True | | provider:network_type | midonet | | router:external | False | | shared | False | | status | ACTIVE | | subnets | | | tags | | | tenant_id | 2f7ee2716b3b4140be57b4a5b26401e3 | | updated_at | 2016-07-03T14:51:39 | +-----------------------+--------------------------------------+ # neutron subnet-create demo-net 10.0.20.0/24 --name demo-subnet Created a new subnet: +-------------------+----------------------------------------------+ | Field | Value | +-------------------+----------------------------------------------+ | allocation_pools | {&quot;start&quot;: &quot;10.0.20.2&quot;, &quot;end&quot;: &quot;10.0.20.254&quot;} | | cidr | 10.0.20.0/24 | | created_at | 2016-07-03T14:52:32 | | description | | | dns_nameservers | | | enable_dhcp | True | | gateway_ip | 10.0.20.1 | | host_routes | | | id | b299d899-33a3-4bfa-aff4-fda071545bdf | | ip_version | 4 | | ipv6_address_mode | | | ipv6_ra_mode | | | name | demo-subnet | | network_id | 075ba699-dc4c-4625-8e0d-0a258a9aeb7d | | subnetpool_id | | | tenant_id | 2f7ee2716b3b4140be57b4a5b26401e3 | | updated_at | 2016-07-03T14:52:32 | +-------------------+----------------------------------------------+ Create a tenant router # neutron router-create router1 Created a new router: +-----------------------+--------------------------------------+ | Field | Value | +-----------------------+--------------------------------------+ | admin_state_up | True | | description | | | external_gateway_info | | | id | 258942d8-9d82-4ebd-b829-c7bdfcc973f5 | | name | router1 | | routes | | | status | ACTIVE | | tenant_id | 2f7ee2716b3b4140be57b4a5b26401e3 | +-----------------------+--------------------------------------+ Attach the tenant subnet interface we created before to the router # neutron router-interface-add router1 demo-subnet Added interface 06c85a56-368c-4d79-bbf0-4bb077f163e5 to router router1. Set the external network as router gateway # neutron router-gateway-set router1 ext-net Set gateway for router router1 Now, you can create an instance at tenant network # nova boot --flavor m1.tiny --image 80871834-29dd-4100-b038-f5f83f126204 --nic net-id=075ba699-dc4c-4625-8e0d-0a258a9aeb7d test1 +--------------------------------------+-----------------------------------------------------+ | Property | Value | +--------------------------------------+-----------------------------------------------------+ | OS-DCF:diskConfig | MANUAL | | OS-EXT-AZ:availability_zone | | | OS-EXT-SRV-ATTR:host | - | | OS-EXT-SRV-ATTR:hypervisor_hostname | - | | OS-EXT-SRV-ATTR:instance_name | instance-0000000a | | OS-EXT-STS:power_state | 0 | | OS-EXT-STS:task_state | scheduling | | OS-EXT-STS:vm_state | building | | OS-SRV-USG:launched_at | - | | OS-SRV-USG:terminated_at | - | | accessIPv4 | | | accessIPv6 | | | adminPass | q2Cq4kxePSLL | | config_drive | | | created | 2016-07-03T15:46:19Z | | flavor | m1.tiny (1) | | hostId | | | id | b8aa46f9-186c-4594-8428-f8dbb16a5e16 | | image | cirros image (80871834-29dd-4100-b038-f5f83f126204) | | key_name | - | | metadata | {} | | name | test1 | | os-extended-volumes:volumes_attached | [] | | progress | 0 | | security_groups | default | | status | BUILD | | tenant_id | 2f7ee2716b3b4140be57b4a5b26401e3 | | updated | 2016-07-03T15:46:20Z | | user_id | a2482a91a1f14750b372445d28b07c75 | +--------------------------------------+-----------------------------------------------------+ # nova list +--------------------------------------+-------+--------+------------+-------------+---------------------+ | ID | Name | Status | Task State | Power State | Networks | +--------------------------------------+-------+--------+------------+-------------+---------------------+ | b8aa46f9-186c-4594-8428-f8dbb16a5e16 | test1 | ACTIVE | - | Running | demo-net=10.0.20.11 | +--------------------------------------+-------+--------+------------+-------------+---------------------+ Ensure the instance gets IP and the metadata service is properly running # nova console-log test1 ...#Snipp from the output Sending discover... Sending select for 10.0.20.11... Lease of 10.0.20.11 obtained, lease time 86400 cirros-ds 'net' up at 7.92 checking http://169.254.169.254/2009-04-04/instance-id successful after 1/20 tries: up 8.22. iid=i-0000000a ... If you login to the instance through VNC you should be able to ping another instances Edge router configuration Create a new router # neutron router-create edge-router Created a new router: +-----------------------+--------------------------------------+ | Field | Value | +-----------------------+--------------------------------------+ | admin_state_up | True | | description | | | external_gateway_info | | | id | 5ecadb64-cb0d-4f95-a00e-aa1dd20a2012 | | name | edge-router | | routes | | | status | ACTIVE | | tenant_id | 2f7ee2716b3b4140be57b4a5b26401e3 | +-----------------------+--------------------------------------+ Attach the external subnet interface to the router # neutron router-interface-add edge-router ext-subnet Added interface e37f1986-c6b1-47f4-8268-02b837ceac17 to router edge-router. Create an uplink network # neutron net-create uplink-network --tenant_id admin --provider:network_type uplink Created a new network: +-----------------------+--------------------------------------+ | Field | Value | +-----------------------+--------------------------------------+ | admin_state_up | True | | created_at | 2016-07-03T14:57:15 | | description | | | id | 77173ed4-6106-4515-af1c-3683897955f9 | | name | uplink-network | | port_security_enabled | True | | provider:network_type | uplink | | router:external | False | | shared | False | | status | ACTIVE | | subnets | | | tags | | | tenant_id | admin | | updated_at | 2016-07-03T14:57:15 | +-----------------------+--------------------------------------+ Create a subnet in the uplink network # neutron subnet-create --tenant_id admin --disable-dhcp --name uplink-subnet uplink-network 192.168.1.0/24 Created a new subnet: +-------------------+--------------------------------------------------+ | Field | Value | +-------------------+--------------------------------------------------+ | allocation_pools | {&quot;start&quot;: &quot;192.168.1.2&quot;, &quot;end&quot;: &quot;192.168.1.254&quot;} | | cidr | 192.168.1.0/24 | | created_at | 2016-07-03T15:06:28 | | description | | | dns_nameservers | | | enable_dhcp | False | | gateway_ip | 192.168.1.1 | | host_routes | | | id | 4e98e789-20d3-45fd-a3b5-9bcf02d8a832 | | ip_version | 4 | | ipv6_address_mode | | | ipv6_ra_mode | | | name | uplink-subnet | | network_id | 77173ed4-6106-4515-af1c-3683897955f9 | | subnetpool_id | | | tenant_id | admin | | updated_at | 2016-07-03T15:06:28 | +-------------------+--------------------------------------------------+ Create a port for each of the gateway nodes, interface should match with the NIC you want to use for binding the gateway nodes and a IP address for the same purposes # neutron port-create uplink-network --binding:host_id gateway1 --binding:profile type=dict interface_name=eth1 --fixed-ip ip_address=192.168.1.199 Created a new port: +-----------------------+--------------------------------------------------------------------------------------+ | Field | Value | +-----------------------+--------------------------------------------------------------------------------------+ | admin_state_up | True | | allowed_address_pairs | | | binding:host_id | compute1 | | binding:profile | {&quot;interface_name&quot;: &quot;eth1&quot;} | | binding:vif_details | {&quot;port_filter&quot;: true} | | binding:vif_type | midonet | | binding:vnic_type | normal | | created_at | 2016-07-03T15:10:06 | | description | | | device_id | | | device_owner | | | extra_dhcp_opts | | | fixed_ips | {&quot;subnet_id&quot;: &quot;4e98e789-20d3-45fd-a3b5-9bcf02d8a832&quot;, &quot;ip_address&quot;: &quot;192.168.1.199&quot;} | | id | 7b4f54dd-2b41-42ba-9c5c-cda4640dc550 | | mac_address | fa:16:3e:44:a8:c9 | | name | | | network_id | 77173ed4-6106-4515-af1c-3683897955f9 | | port_security_enabled | True | | security_groups | 0cf3e33e-dbd6-4b42-a0bd-6679b5eed4e1 | | status | ACTIVE | | tenant_id | 2f7ee2716b3b4140be57b4a5b26401e3 | | updated_at | 2016-07-03T15:10:06 | +-----------------------+--------------------------------------------------------------------------------------+ Attach each of the ports to the edge router # neutron router-interface-add edge-router port=7b4f54dd-2b41-42ba-9c5c-cda4640dc550 Added interface 7b4f54dd-2b41-42ba-9c5c-cda4640dc550 to router edge-router. At this point you have to decide if use border routers with BGP enabled or static routes. Use one of the following links to configure your use case: https://docs.midonet.org/docs/latest/operations-guide/content/bgp_uplink_configuration.html https://docs.midonet.org/docs/latest/operations-guide/content/static_setup.html Issues I faced during configuration of Midonet Midolman agent don’t start: It was caused because midolman-env.sh file has more RAM configured as the one of my server. Edit the file to match your server resources # egrep ^MAX_HEAP_SIZE /etc/midolman/midolman-env.sh MAX_HEAP_SIZE=&quot;2048M&quot; &amp;lt;/br&amp;gt; Instances doesn’t boot with the following error: could not open /dev/net/tun: Permission denied I had to remove br-tun bridges at ovs, if not, ovs locks the device and midolman cannot create the tunnel beetwen compute nodes and gateway nodes. ovs-vsctl del-br br-tun &amp;lt;/br&amp;gt; This post is my experience integrating Midonet into OpenStack, maybe some things are not correct, if you find any issue, please advise me to fix it. Regards, Eduardo Gonzalez</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://217.182.136.201:5000/wp-content/uploads/2015/09/learn-about-openstack-badge.png" /></entry><entry><title type="html">Rally OpenStack benchmarking from Docker containers</title><link href="http://217.182.136.201:5000/rally-openstack-benchmarking-from-docker-containers/" rel="alternate" type="text/html" title="Rally OpenStack benchmarking from Docker containers" /><published>2016-06-15T21:13:39+00:00</published><updated>2016-06-15T21:13:39+00:00</updated><id>http://217.182.136.201:5000/rally-openstack-benchmarking-from-docker-containers</id><content type="html" xml:base="http://217.182.136.201:5000/rally-openstack-benchmarking-from-docker-containers/">&lt;p&gt;OpenStack Rally is a project under the Big Tent umbrella with the mission of verify OpenStack environments to ensure SLAs under high loads or fail over scenarios, and cloud services verification. Rally can also be used to continuous integration and delivery tasks.&lt;/p&gt;

&lt;p&gt;Why use Rally inside a Docker container? Rally is a service that is not commonly used in most environments, is a tool that is used when new infrastructure changes are made or when a SLAs review must be done, not make any sense have a service consuming infrastructure resources or block a server only for use under specific situations. Also, if your OpenStack infrastructure is automated, with a container you can have a nice integration with CI/CD tools like Jenkins.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;
Main reasons to use Rally inside Docker containers:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;Quick tests/deployments of Rally tasks&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Automated testing&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Cost savings&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Operators can execute tasks with their own computers, freeing infrastructure resources&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Re-utilization of resources&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;p&gt;Here you got my suggestions about how to use Rally inside Docker:&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;Create a new container(automatized or not by another tool)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Always use an external volume to store rally reports data&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Execute Rally tasks&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Export the reports to the volume shared with the Docker host&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Kill the container&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;p&gt;Let’s start with this quick guide:
&lt;br /&gt;&amp;lt;/br&amp;gt;
Clone the repo i created with the Dockerfile&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost ~]$ git clone https://github.com/egonzalez90/docker-rally.git
&lt;/pre&gt;
&lt;p&gt;Move to docker-rally directory&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost ~]$ cd docker-rally/
&lt;/pre&gt;
&lt;p&gt;Create the Docker image&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost docker-rally]$ docker build -t egonzalez90/rally-mitaka .
Sending build context to Docker daemon  76.8 kB
Step 1 : FROM centos:7
 ---&amp;gt; 904d6c400333
Step 2 : MAINTAINER Eduardo Gonzalez Gutierrez &amp;lt;dabarren@gmail.com&amp;gt;
 ---&amp;gt; Using cache
 ---&amp;gt; ee93bc7747e1
Step 3 : RUN yum install -y https://repos.fedorapeople.org/repos/openstack/openstack-mitaka/rdo-release-mitaka-3.noarch.rpm
 ---&amp;gt; Using cache
 ---&amp;gt; 8492ab9ee261
Step 4 : RUN yum update -y
 ---&amp;gt; Using cache
 ---&amp;gt; 1374340eb39a
Step 5 : RUN yum -y install         openstack-rally         gcc         libffi-devel         python-devel         openssl-devel         gmp-devel         libxml2-devel         libxslt-devel         postgresql-devel         redhat-rpm-config         wget         openstack-selinux         openstack-utils &amp;amp;&amp;amp;         yum clean all
 ---&amp;gt; Using cache
 ---&amp;gt; 9b65e4a281be
Step 6 : RUN rally-manage --config-file /etc/rally/rally.conf db recreate
 ---&amp;gt; Using cache
 ---&amp;gt; dc4f3dbc1505
Successfully built dc4f3dbc1505
&lt;/pre&gt;
&lt;p&gt;Start rally container with a pseudo-tty and a volume to store rally execution data&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost docker-rally]$ docker run -ti -v /opt/rally-data/:/rally-data:Z egonzalez90/rally-mitaka
[root@07766ba700e8 /]# 
&lt;/pre&gt;
&lt;p&gt;Create a file called deploy.json with the admin info of your OpenStack environment&lt;/p&gt;
&lt;pre&gt;[root@07766ba700e8 /]# vi deploy.json

{
    &quot;type&quot;: &quot;ExistingCloud&quot;,
    &quot;auth_url&quot;: &quot;http://controller:5000/v2.0&quot;,
    &quot;region_name&quot;: &quot;RegionOne&quot;,
    &quot;admin&quot;: {
        &quot;username&quot;: &quot;admin&quot;,
        &quot;password&quot;: &quot;my_password&quot;,
        &quot;tenant_name&quot;: &quot;admin&quot;
    }
}

&lt;/pre&gt;
&lt;p&gt;Create a deployment with the json we previously created&lt;/p&gt;
&lt;pre&gt;[root@07766ba700e8 /]# rally deployment create --file=deploy.json --name=existing
2016-06-15 09:42:25.428 25 INFO rally.deployment.engine [-] Deployment a5162111-02a5-458f-bb59-f822cab1aa93 | Starting:  OpenStack cloud deployment.
2016-06-15 09:42:25.478 25 INFO rally.deployment.engine [-] Deployment a5162111-02a5-458f-bb59-f822cab1aa93 | Completed: OpenStack cloud deployment.
+--------------------------------------+----------------------------+----------+------------------+--------+
| uuid                                 | created_at                 | name     | status           | active |
+--------------------------------------+----------------------------+----------+------------------+--------+
| a5162111-02a5-458f-bb59-f822cab1aa93 | 2016-06-15 09:42:25.391691 | existing | deploy-&amp;gt;finished |        |
+--------------------------------------+----------------------------+----------+------------------+--------+
Using deployment: a5162111-02a5-458f-bb59-f822cab1aa93
~/.rally/openrc was updated

HINTS:
* To get your cloud resources, run:
        rally show [flavors|images|keypairs|networks|secgroups]

* To use standard OpenStack clients, set up your env by running:
        source ~/.rally/openrc
  OpenStack clients are now configured, e.g run:
        glance image-list
&lt;/pre&gt;
&lt;p&gt;Source the openrc file rally has created with your user info and test if you can connect with glance&lt;/p&gt;
&lt;pre&gt;[root@07766ba700e8 /]# source ~/.rally/openrc

[root@07766ba700e8 /]# glance  image-list
+--------------------------------------+--------+
| ID                                   | Name   |
+--------------------------------------+--------+
| 1c4fc8a6-3ea7-433c-8ece-a14bbaf861e2 | cirros |
+--------------------------------------+--------+
&lt;/pre&gt;
&lt;p&gt;Check deployment status&lt;/p&gt;
&lt;pre&gt;[root@07766ba700e8 /]# rally deployment check
keystone endpoints are valid and following services are available:
+-------------+----------------+-----------+
| services    | type           | status    |
+-------------+----------------+-----------+
| __unknown__ | volumev2       | Available |
| ceilometer  | metering       | Available |
| cinder      | volume         | Available |
| cloud       | cloudformation | Available |
| glance      | image          | Available |
| heat        | orchestration  | Available |
| keystone    | identity       | Available |
| neutron     | network        | Available |
| nova        | compute        | Available |
+-------------+----------------+-----------+
NOTE: '__unknown__' service name means that Keystone service catalog doesn't return name for this service and Rally can not identify service by its type. BUT you still can use such services with api_versions context, specifying type of service (execute `rally plugin show api_versions` for more details).
&lt;/pre&gt;
&lt;p&gt;Create a test execution file, this test will check if nova can boot and delete some instances&lt;/p&gt;
&lt;pre&gt;[root@07766ba700e8 /]# vi execution.json

{
  &quot;NovaServers.boot_and_delete_server&quot;: [
    {
      &quot;runner&quot;: {
        &quot;type&quot;: &quot;constant&quot;, 
        &quot;concurrency&quot;: 2, 
        &quot;times&quot;: 10
      }, 
      &quot;args&quot;: {
        &quot;force_delete&quot;: false, 
        &quot;flavor&quot;: {
          &quot;name&quot;: &quot;m1.tiny&quot;
        }, 
        &quot;image&quot;: {
          &quot;name&quot;: &quot;cirros&quot;
        }
      }, 
      &quot;context&quot;: {
        &quot;users&quot;: {
          &quot;project_domain&quot;: &quot;default&quot;, 
          &quot;users_per_tenant&quot;: 2, 
          &quot;tenants&quot;: 3, 
          &quot;resource_management_workers&quot;: 30, 
          &quot;user_domain&quot;: &quot;default&quot;
        }
      }
    }
  ]
}
&lt;/pre&gt;
&lt;p&gt;Run the task with the following command&lt;/p&gt;
&lt;pre&gt;[root@07766ba700e8 /]# rally task start execution.json
--------------------------------------------------------------------------------
 Preparing input task
--------------------------------------------------------------------------------

Input task is:
{
    &quot;NovaServers.boot_and_delete_server&quot;: [
        {
            &quot;args&quot;: {
                &quot;flavor&quot;: {
                    &quot;name&quot;: &quot;m1.tiny&quot;
                },
                &quot;image&quot;: {
                    &quot;name&quot;: &quot;cirros&quot;
                },
                &quot;force_delete&quot;: false
            },
            &quot;runner&quot;: {
                &quot;type&quot;: &quot;constant&quot;,
                &quot;times&quot;: 10,
                &quot;concurrency&quot;: 2
            },
            &quot;context&quot;: {
                &quot;users&quot;: {
                    &quot;tenants&quot;: 3,
                    &quot;users_per_tenant&quot;: 2
                }
            }
        }
    ]
}

Task syntax is correct :)
2016-06-15 09:48:11.556 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  Task validation.
2016-06-15 09:48:11.579 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  Task validation of scenarios names.
2016-06-15 09:48:11.581 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Task validation of scenarios names.
2016-06-15 09:48:11.581 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  Task validation of syntax.
2016-06-15 09:48:11.587 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Task validation of syntax.
2016-06-15 09:48:11.588 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  Task validation of semantic.
2016-06-15 09:48:11.588 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  Task validation check cloud.
2016-06-15 09:48:11.694 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Task validation check cloud.
2016-06-15 09:48:11.700 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  Enter context: `users`
2016-06-15 09:48:12.004 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Enter context: `users`
2016-06-15 09:48:12.106 101 WARNING rally.task.types [-] FlavorResourceType is deprecated in Rally v0.3.2; use the equivalent resource plugin name instead
2016-06-15 09:48:12.207 101 WARNING rally.task.types [-] ImageResourceType is deprecated in Rally v0.3.2; use the equivalent resource plugin name instead
2016-06-15 09:48:12.395 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  Exit context: `users`
2016-06-15 09:48:13.546 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Exit context: `users`
2016-06-15 09:48:13.546 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Task validation of semantic.
2016-06-15 09:48:13.547 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Task validation.
Task config is valid :)
--------------------------------------------------------------------------------
 Task  137eb997-d1f8-4d3f-918a-8aec3db7500f: started
--------------------------------------------------------------------------------

Benchmarking... This can take a while...

To track task status use:

        rally task status
        or
        rally task detailed

Using task: 137eb997-d1f8-4d3f-918a-8aec3db7500f
2016-06-15 09:48:13.555 101 INFO rally.api [-] Benchmark Task 137eb997-d1f8-4d3f-918a-8aec3db7500f on Deployment a5162111-02a5-458f-bb59-f822cab1aa93
2016-06-15 09:48:13.558 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  Benchmarking.
2016-06-15 09:48:13.586 101 INFO rally.task.engine [-] Running benchmark with key:
{
  &quot;kw&quot;: {
    &quot;runner&quot;: {
      &quot;type&quot;: &quot;constant&quot;,
      &quot;concurrency&quot;: 2,
      &quot;times&quot;: 10
    },
    &quot;args&quot;: {
      &quot;force_delete&quot;: false,
      &quot;flavor&quot;: {
        &quot;name&quot;: &quot;m1.tiny&quot;
      },
      &quot;image&quot;: {
        &quot;name&quot;: &quot;cirros&quot;
      }
    },
    &quot;context&quot;: {
      &quot;users&quot;: {
        &quot;users_per_tenant&quot;: 2,
        &quot;tenants&quot;: 3
      }
    }
  },
  &quot;name&quot;: &quot;NovaServers.boot_and_delete_server&quot;,
  &quot;pos&quot;: 0
}
2016-06-15 09:48:13.592 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  Enter context: `users`
2016-06-15 09:48:14.994 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Enter context: `users`
2016-06-15 09:48:15.244 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 0 START
2016-06-15 09:48:15.245 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 1 START
2016-06-15 09:48:16.975 292 WARNING rally.common.logging [-] 'wait_for' is deprecated in Rally v0.1.2: Use wait_for_status instead.
2016-06-15 09:48:17.095 293 WARNING rally.common.logging [-] 'wait_for' is deprecated in Rally v0.1.2: Use wait_for_status instead.
2016-06-15 09:49:21.024 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 0 END: OK
2016-06-15 09:49:21.028 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 2 START
2016-06-15 09:49:32.109 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 1 END: OK
2016-06-15 09:49:32.112 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 3 START
2016-06-15 09:49:41.504 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 2 END: OK
2016-06-15 09:49:41.508 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 4 START
2016-06-15 09:49:52.455 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 3 END: OK
2016-06-15 09:49:52.462 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 5 START
2016-06-15 09:50:01.907 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 4 END: OK
2016-06-15 09:50:01.918 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 6 START
2016-06-15 09:50:12.692 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 5 END: OK
2016-06-15 09:50:12.694 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 7 START
2016-06-15 09:50:23.122 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 6 END: OK
2016-06-15 09:50:23.131 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 8 START
2016-06-15 09:50:33.322 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 7 END: OK
2016-06-15 09:50:33.332 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 9 START
2016-06-15 09:50:43.285 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 8 END: OK
2016-06-15 09:50:53.422 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 9 END: OK
2016-06-15 09:50:53.436 101 INFO rally.plugins.openstack.context.cleanup.user [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  user resources cleanup
2016-06-15 09:50:55.244 101 INFO rally.plugins.openstack.context.cleanup.user [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: user resources cleanup
2016-06-15 09:50:55.245 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting:  Exit context: `users`
2016-06-15 09:50:57.438 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Exit context: `users`
2016-06-15 09:50:58.023 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Benchmarking.

--------------------------------------------------------------------------------
Task 137eb997-d1f8-4d3f-918a-8aec3db7500f: finished
--------------------------------------------------------------------------------

test scenario NovaServers.boot_and_delete_server
args position 0
args values:
{
  &quot;runner&quot;: {
    &quot;type&quot;: &quot;constant&quot;,
    &quot;concurrency&quot;: 2,
    &quot;times&quot;: 10
  },
  &quot;args&quot;: {
    &quot;force_delete&quot;: false,
    &quot;flavor&quot;: {
      &quot;name&quot;: &quot;m1.tiny&quot;
    },
    &quot;image&quot;: {
      &quot;name&quot;: &quot;cirros&quot;
    }
  },
  &quot;context&quot;: {
    &quot;users&quot;: {
      &quot;project_domain&quot;: &quot;default&quot;,
      &quot;users_per_tenant&quot;: 2,
      &quot;tenants&quot;: 3,
      &quot;resource_management_workers&quot;: 30,
      &quot;user_domain&quot;: &quot;default&quot;
    }
  }
}

+-----------------------------------------------------------------------------------------------------------------------+
|                                                 Response Times (sec)                                                  |
+--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+
| Action             | Min (sec) | Median (sec) | 90%ile (sec) | 95%ile (sec) | Max (sec) | Avg (sec) | Success | Count |
+--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+
| nova.boot_server   | 17.84     | 18.158       | 64.433       | 69.419       | 74.405    | 28.299    | 100.0%  | 10    |
| nova.delete_server | 2.24      | 2.275        | 2.454        | 2.456        | 2.458     | 2.317     | 100.0%  | 10    |
| total              | 20.09     | 20.437       | 66.888       | 71.875       | 76.863    | 30.616    | 100.0%  | 10    |
+--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+

Load duration: 158.199862003
Full duration: 163.846753836

HINTS:
* To plot HTML graphics with this data, run:
        rally task report 137eb997-d1f8-4d3f-918a-8aec3db7500f --out output.html

* To generate a JUnit report, run:
        rally task report 137eb997-d1f8-4d3f-918a-8aec3db7500f --junit --out output.xml

* To get raw JSON output of task results, run:
        rally task results 137eb997-d1f8-4d3f-918a-8aec3db7500f

&lt;/pre&gt;
&lt;p&gt;After a while, you will receive an output execution resume, you can export to a report file with the following command in a pretty style report.
Use the volume we created with the Docker Host to save report files.&lt;/p&gt;
&lt;pre&gt;
[root@07766ba700e8 /]# rally task report 137eb997-d1f8-4d3f-918a-8aec3db7500f --html-static --out /rally-data/output.html
&lt;/pre&gt;
&lt;p&gt;Open the output file form a Web browser and review the report.
&lt;img src=&quot;http://egonzalez.org/wp-content/uploads/2016/06/Selection_002-1024x692.png&quot; alt=&quot;Selection_002&quot; width=&quot;750&quot; height=&quot;507&quot; class=&quot;alignnone size-large wp-image-1240&quot; /&gt;
Regards&lt;/p&gt;</content><author><name>Editor</name></author><category term="benchmark" /><category term="container" /><category term="docker" /><category term="dockerfile" /><category term="liberty" /><category term="mitaka" /><category term="openstack" /><category term="output" /><category term="rally" /><category term="rdo" /><category term="report" /><category term="sla" /><category term="test" /><summary type="html">OpenStack Rally is a project under the Big Tent umbrella with the mission of verify OpenStack environments to ensure SLAs under high loads or fail over scenarios, and cloud services verification. Rally can also be used to continuous integration and delivery tasks. Why use Rally inside a Docker container? Rally is a service that is not commonly used in most environments, is a tool that is used when new infrastructure changes are made or when a SLAs review must be done, not make any sense have a service consuming infrastructure resources or block a server only for use under specific situations. Also, if your OpenStack infrastructure is automated, with a container you can have a nice integration with CI/CD tools like Jenkins. &amp;lt;/br&amp;gt; Main reasons to use Rally inside Docker containers: Quick tests/deployments of Rally tasks Automated testing Cost savings Operators can execute tasks with their own computers, freeing infrastructure resources Re-utilization of resources &amp;lt;/br&amp;gt; Here you got my suggestions about how to use Rally inside Docker: Create a new container(automatized or not by another tool) Always use an external volume to store rally reports data Execute Rally tasks Export the reports to the volume shared with the Docker host Kill the container &amp;lt;/br&amp;gt; Let’s start with this quick guide: &amp;lt;/br&amp;gt; Clone the repo i created with the Dockerfile [egonzalez@localhost ~]$ git clone https://github.com/egonzalez90/docker-rally.git Move to docker-rally directory [egonzalez@localhost ~]$ cd docker-rally/ Create the Docker image [egonzalez@localhost docker-rally]$ docker build -t egonzalez90/rally-mitaka . Sending build context to Docker daemon 76.8 kB Step 1 : FROM centos:7 ---&amp;gt; 904d6c400333 Step 2 : MAINTAINER Eduardo Gonzalez Gutierrez &amp;lt;dabarren@gmail.com&amp;gt; ---&amp;gt; Using cache ---&amp;gt; ee93bc7747e1 Step 3 : RUN yum install -y https://repos.fedorapeople.org/repos/openstack/openstack-mitaka/rdo-release-mitaka-3.noarch.rpm ---&amp;gt; Using cache ---&amp;gt; 8492ab9ee261 Step 4 : RUN yum update -y ---&amp;gt; Using cache ---&amp;gt; 1374340eb39a Step 5 : RUN yum -y install openstack-rally gcc libffi-devel python-devel openssl-devel gmp-devel libxml2-devel libxslt-devel postgresql-devel redhat-rpm-config wget openstack-selinux openstack-utils &amp;amp;&amp;amp; yum clean all ---&amp;gt; Using cache ---&amp;gt; 9b65e4a281be Step 6 : RUN rally-manage --config-file /etc/rally/rally.conf db recreate ---&amp;gt; Using cache ---&amp;gt; dc4f3dbc1505 Successfully built dc4f3dbc1505 Start rally container with a pseudo-tty and a volume to store rally execution data [egonzalez@localhost docker-rally]$ docker run -ti -v /opt/rally-data/:/rally-data:Z egonzalez90/rally-mitaka [root@07766ba700e8 /]# Create a file called deploy.json with the admin info of your OpenStack environment [root@07766ba700e8 /]# vi deploy.json { &quot;type&quot;: &quot;ExistingCloud&quot;, &quot;auth_url&quot;: &quot;http://controller:5000/v2.0&quot;, &quot;region_name&quot;: &quot;RegionOne&quot;, &quot;admin&quot;: { &quot;username&quot;: &quot;admin&quot;, &quot;password&quot;: &quot;my_password&quot;, &quot;tenant_name&quot;: &quot;admin&quot; } } Create a deployment with the json we previously created [root@07766ba700e8 /]# rally deployment create --file=deploy.json --name=existing 2016-06-15 09:42:25.428 25 INFO rally.deployment.engine [-] Deployment a5162111-02a5-458f-bb59-f822cab1aa93 | Starting: OpenStack cloud deployment. 2016-06-15 09:42:25.478 25 INFO rally.deployment.engine [-] Deployment a5162111-02a5-458f-bb59-f822cab1aa93 | Completed: OpenStack cloud deployment. +--------------------------------------+----------------------------+----------+------------------+--------+ | uuid | created_at | name | status | active | +--------------------------------------+----------------------------+----------+------------------+--------+ | a5162111-02a5-458f-bb59-f822cab1aa93 | 2016-06-15 09:42:25.391691 | existing | deploy-&amp;gt;finished | | +--------------------------------------+----------------------------+----------+------------------+--------+ Using deployment: a5162111-02a5-458f-bb59-f822cab1aa93 ~/.rally/openrc was updated HINTS: * To get your cloud resources, run: rally show [flavors|images|keypairs|networks|secgroups] * To use standard OpenStack clients, set up your env by running: source ~/.rally/openrc OpenStack clients are now configured, e.g run: glance image-list Source the openrc file rally has created with your user info and test if you can connect with glance [root@07766ba700e8 /]# source ~/.rally/openrc [root@07766ba700e8 /]# glance image-list +--------------------------------------+--------+ | ID | Name | +--------------------------------------+--------+ | 1c4fc8a6-3ea7-433c-8ece-a14bbaf861e2 | cirros | +--------------------------------------+--------+ Check deployment status [root@07766ba700e8 /]# rally deployment check keystone endpoints are valid and following services are available: +-------------+----------------+-----------+ | services | type | status | +-------------+----------------+-----------+ | __unknown__ | volumev2 | Available | | ceilometer | metering | Available | | cinder | volume | Available | | cloud | cloudformation | Available | | glance | image | Available | | heat | orchestration | Available | | keystone | identity | Available | | neutron | network | Available | | nova | compute | Available | +-------------+----------------+-----------+ NOTE: '__unknown__' service name means that Keystone service catalog doesn't return name for this service and Rally can not identify service by its type. BUT you still can use such services with api_versions context, specifying type of service (execute `rally plugin show api_versions` for more details). Create a test execution file, this test will check if nova can boot and delete some instances [root@07766ba700e8 /]# vi execution.json { &quot;NovaServers.boot_and_delete_server&quot;: [ { &quot;runner&quot;: { &quot;type&quot;: &quot;constant&quot;, &quot;concurrency&quot;: 2, &quot;times&quot;: 10 }, &quot;args&quot;: { &quot;force_delete&quot;: false, &quot;flavor&quot;: { &quot;name&quot;: &quot;m1.tiny&quot; }, &quot;image&quot;: { &quot;name&quot;: &quot;cirros&quot; } }, &quot;context&quot;: { &quot;users&quot;: { &quot;project_domain&quot;: &quot;default&quot;, &quot;users_per_tenant&quot;: 2, &quot;tenants&quot;: 3, &quot;resource_management_workers&quot;: 30, &quot;user_domain&quot;: &quot;default&quot; } } } ] } Run the task with the following command [root@07766ba700e8 /]# rally task start execution.json -------------------------------------------------------------------------------- Preparing input task -------------------------------------------------------------------------------- Input task is: { &quot;NovaServers.boot_and_delete_server&quot;: [ { &quot;args&quot;: { &quot;flavor&quot;: { &quot;name&quot;: &quot;m1.tiny&quot; }, &quot;image&quot;: { &quot;name&quot;: &quot;cirros&quot; }, &quot;force_delete&quot;: false }, &quot;runner&quot;: { &quot;type&quot;: &quot;constant&quot;, &quot;times&quot;: 10, &quot;concurrency&quot;: 2 }, &quot;context&quot;: { &quot;users&quot;: { &quot;tenants&quot;: 3, &quot;users_per_tenant&quot;: 2 } } } ] } Task syntax is correct :) 2016-06-15 09:48:11.556 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: Task validation. 2016-06-15 09:48:11.579 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: Task validation of scenarios names. 2016-06-15 09:48:11.581 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Task validation of scenarios names. 2016-06-15 09:48:11.581 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: Task validation of syntax. 2016-06-15 09:48:11.587 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Task validation of syntax. 2016-06-15 09:48:11.588 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: Task validation of semantic. 2016-06-15 09:48:11.588 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: Task validation check cloud. 2016-06-15 09:48:11.694 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Task validation check cloud. 2016-06-15 09:48:11.700 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: Enter context: `users` 2016-06-15 09:48:12.004 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Enter context: `users` 2016-06-15 09:48:12.106 101 WARNING rally.task.types [-] FlavorResourceType is deprecated in Rally v0.3.2; use the equivalent resource plugin name instead 2016-06-15 09:48:12.207 101 WARNING rally.task.types [-] ImageResourceType is deprecated in Rally v0.3.2; use the equivalent resource plugin name instead 2016-06-15 09:48:12.395 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: Exit context: `users` 2016-06-15 09:48:13.546 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Exit context: `users` 2016-06-15 09:48:13.546 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Task validation of semantic. 2016-06-15 09:48:13.547 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Task validation. Task config is valid :) -------------------------------------------------------------------------------- Task 137eb997-d1f8-4d3f-918a-8aec3db7500f: started -------------------------------------------------------------------------------- Benchmarking... This can take a while... To track task status use: rally task status or rally task detailed Using task: 137eb997-d1f8-4d3f-918a-8aec3db7500f 2016-06-15 09:48:13.555 101 INFO rally.api [-] Benchmark Task 137eb997-d1f8-4d3f-918a-8aec3db7500f on Deployment a5162111-02a5-458f-bb59-f822cab1aa93 2016-06-15 09:48:13.558 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: Benchmarking. 2016-06-15 09:48:13.586 101 INFO rally.task.engine [-] Running benchmark with key: { &quot;kw&quot;: { &quot;runner&quot;: { &quot;type&quot;: &quot;constant&quot;, &quot;concurrency&quot;: 2, &quot;times&quot;: 10 }, &quot;args&quot;: { &quot;force_delete&quot;: false, &quot;flavor&quot;: { &quot;name&quot;: &quot;m1.tiny&quot; }, &quot;image&quot;: { &quot;name&quot;: &quot;cirros&quot; } }, &quot;context&quot;: { &quot;users&quot;: { &quot;users_per_tenant&quot;: 2, &quot;tenants&quot;: 3 } } }, &quot;name&quot;: &quot;NovaServers.boot_and_delete_server&quot;, &quot;pos&quot;: 0 } 2016-06-15 09:48:13.592 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: Enter context: `users` 2016-06-15 09:48:14.994 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Enter context: `users` 2016-06-15 09:48:15.244 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 0 START 2016-06-15 09:48:15.245 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 1 START 2016-06-15 09:48:16.975 292 WARNING rally.common.logging [-] 'wait_for' is deprecated in Rally v0.1.2: Use wait_for_status instead. 2016-06-15 09:48:17.095 293 WARNING rally.common.logging [-] 'wait_for' is deprecated in Rally v0.1.2: Use wait_for_status instead. 2016-06-15 09:49:21.024 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 0 END: OK 2016-06-15 09:49:21.028 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 2 START 2016-06-15 09:49:32.109 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 1 END: OK 2016-06-15 09:49:32.112 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 3 START 2016-06-15 09:49:41.504 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 2 END: OK 2016-06-15 09:49:41.508 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 4 START 2016-06-15 09:49:52.455 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 3 END: OK 2016-06-15 09:49:52.462 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 5 START 2016-06-15 09:50:01.907 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 4 END: OK 2016-06-15 09:50:01.918 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 6 START 2016-06-15 09:50:12.692 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 5 END: OK 2016-06-15 09:50:12.694 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 7 START 2016-06-15 09:50:23.122 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 6 END: OK 2016-06-15 09:50:23.131 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 8 START 2016-06-15 09:50:33.322 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 7 END: OK 2016-06-15 09:50:33.332 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 9 START 2016-06-15 09:50:43.285 292 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 8 END: OK 2016-06-15 09:50:53.422 293 INFO rally.task.runner [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | ITER: 9 END: OK 2016-06-15 09:50:53.436 101 INFO rally.plugins.openstack.context.cleanup.user [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: user resources cleanup 2016-06-15 09:50:55.244 101 INFO rally.plugins.openstack.context.cleanup.user [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: user resources cleanup 2016-06-15 09:50:55.245 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Starting: Exit context: `users` 2016-06-15 09:50:57.438 101 INFO rally.plugins.openstack.context.keystone.users [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Exit context: `users` 2016-06-15 09:50:58.023 101 INFO rally.task.engine [-] Task 137eb997-d1f8-4d3f-918a-8aec3db7500f | Completed: Benchmarking. -------------------------------------------------------------------------------- Task 137eb997-d1f8-4d3f-918a-8aec3db7500f: finished -------------------------------------------------------------------------------- test scenario NovaServers.boot_and_delete_server args position 0 args values: { &quot;runner&quot;: { &quot;type&quot;: &quot;constant&quot;, &quot;concurrency&quot;: 2, &quot;times&quot;: 10 }, &quot;args&quot;: { &quot;force_delete&quot;: false, &quot;flavor&quot;: { &quot;name&quot;: &quot;m1.tiny&quot; }, &quot;image&quot;: { &quot;name&quot;: &quot;cirros&quot; } }, &quot;context&quot;: { &quot;users&quot;: { &quot;project_domain&quot;: &quot;default&quot;, &quot;users_per_tenant&quot;: 2, &quot;tenants&quot;: 3, &quot;resource_management_workers&quot;: 30, &quot;user_domain&quot;: &quot;default&quot; } } } +-----------------------------------------------------------------------------------------------------------------------+ | Response Times (sec) | +--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+ | Action | Min (sec) | Median (sec) | 90%ile (sec) | 95%ile (sec) | Max (sec) | Avg (sec) | Success | Count | +--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+ | nova.boot_server | 17.84 | 18.158 | 64.433 | 69.419 | 74.405 | 28.299 | 100.0% | 10 | | nova.delete_server | 2.24 | 2.275 | 2.454 | 2.456 | 2.458 | 2.317 | 100.0% | 10 | | total | 20.09 | 20.437 | 66.888 | 71.875 | 76.863 | 30.616 | 100.0% | 10 | +--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+ Load duration: 158.199862003 Full duration: 163.846753836 HINTS: * To plot HTML graphics with this data, run: rally task report 137eb997-d1f8-4d3f-918a-8aec3db7500f --out output.html * To generate a JUnit report, run: rally task report 137eb997-d1f8-4d3f-918a-8aec3db7500f --junit --out output.xml * To get raw JSON output of task results, run: rally task results 137eb997-d1f8-4d3f-918a-8aec3db7500f After a while, you will receive an output execution resume, you can export to a report file with the following command in a pretty style report. Use the volume we created with the Docker Host to save report files. [root@07766ba700e8 /]# rally task report 137eb997-d1f8-4d3f-918a-8aec3db7500f --html-static --out /rally-data/output.html Open the output file form a Web browser and review the report. Regards</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://217.182.136.201:5000/wp-content/uploads/2015/09/learn-about-openstack-badge.png" /></entry><entry><title type="html">OpenDaylight in a Docker container</title><link href="http://217.182.136.201:5000/opendaylight-in-a-docker-container/" rel="alternate" type="text/html" title="OpenDaylight in a Docker container" /><published>2016-06-06T18:26:22+00:00</published><updated>2016-06-06T18:26:22+00:00</updated><id>http://217.182.136.201:5000/opendaylight-in-a-docker-container</id><content type="html" xml:base="http://217.182.136.201:5000/opendaylight-in-a-docker-container/">&lt;p&gt;This is a quick guide to start a Docker container with OpenDaylight running on it.&lt;/p&gt;

&lt;p&gt;Clone OpenDaylight integration repository&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost]$ git clone https://github.com/opendaylight/integration.git
&lt;/pre&gt;
&lt;p&gt;Move to the directory where CentOS Dockerfile is saved.&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost]$ cd integration/packaging/docker/centos/
&lt;/pre&gt;
&lt;p&gt;Build the new image, you can call it as your DockerHub name(in my case egonzalez90), so you can push it there later.
If you don’t want to create a new image, you can use my image. This step will download and start the new container: &lt;code&gt;docker run -d egonzalez90/opendaylight&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost centos]$ docker build -t egonzalez90/opendaylight .

Sending build context to Docker daemon  7.68 kB
Step 1 : FROM centos:7
Trying to pull repository docker.io/library/centos ... 7: Pulling from library/centos
1544084fad81: Pull complete 
df0fc3863fbc: Pull complete 
a3d54b467fad: Pull complete 
a65193109361: Pull complete 
Digest: sha256:a9237ff42b09cc6f610bab60a36df913ef326178a92f3b61631331867178f982
Status: Downloaded newer image for docker.io/centos:7

 ---&amp;gt; a65193109361
Step 2 : MAINTAINER OpenDaylight Project &amp;lt;info@opendaylight.org&amp;gt;
 ---&amp;gt; Running in d3f98f949b11
 ---&amp;gt; 81a1bad2e3a7
Removing intermediate container d3f98f949b11
Step 3 : ADD opendaylight-3-candidate.repo /etc/yum.repos.d/
 ---&amp;gt; 069a9c60878e
Removing intermediate container b9afb18311f3
Step 4 : RUN yum update -y &amp;amp;&amp;amp; yum install -y opendaylight
 ---&amp;gt; Running in 559b3970235d

[[[ PACKAGE INSTALLATION STUFF ]]]                                      

Complete!
 ---&amp;gt; 4003e5874b03
Removing intermediate container 559b3970235d
Step 5 : EXPOSE 162 179 1088 1790 1830 2400 2550 2551 2552 4189 4342 5005 5666 6633 6640 6653 7800 8000 8080 8101 8181 8383 12001
 ---&amp;gt; Running in 7defebe8b7e2
 ---&amp;gt; 9668a559bdac
Removing intermediate container 7defebe8b7e2
Step 6 : WORKDIR /opt/opendaylight
 ---&amp;gt; Running in 9298a116dd14
 ---&amp;gt; 5bf42f56e282
Removing intermediate container 9298a116dd14
Step 7 : CMD ./bin/karaf server
 ---&amp;gt; Running in e0a218941b15
 ---&amp;gt; c1a0db72dbbc
Removing intermediate container e0a218941b15
Successfully built c1a0db72dbbc
&lt;/pre&gt;
&lt;p&gt;Once the image is built or downloaded, ensure you have it locally&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost]$ docker images | grep opendaylight
egonzalez90/opendaylight                              latest              c1a0db72dbbc        About a minute ago   740.6 MB
&lt;/pre&gt;
&lt;p&gt;Start a new container in a detached mode.&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost]$ docker run -d egonzalez90/opendaylight
ae08898ba6adc30df012513dc6eac54943d9de8c8059e73ade185757fe684c6a
Usage of loopback devices is strongly discouraged for production use. Either use `--storage-opt dm.thinpooldev` or use `--storage-opt dm.no_warn_on_loop_devices=true` to suppress this warning.
&lt;/pre&gt;
&lt;p&gt;Check if the container is running with:&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost]$ docker ps | grep opendaylight 
ae08898ba6ad        egonzalez90/opendaylight   &quot;./bin/karaf server&quot;     14 seconds ago      Up 11 seconds       162/tcp, 179/tcp, 1088/tcp, 1790/tcp, 1830/tcp, 2400/tcp, 2550-2552/tcp, 4189/tcp, 4342/tcp, 5005/tcp, 5666/tcp, 6633/tcp, 6640/tcp, 6653/tcp, 7800/tcp, 8000/tcp, 8080/tcp, 8101/tcp, 8181/tcp, 8383/tcp, 12001/tcp   awesome_khorana
&lt;/pre&gt;
&lt;p&gt;Now, check container information with docker inspect, we search for the IP address&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost]$ docker inspect  ae08898ba6ad | grep -i IPAddress
        &quot;SecondaryIPAddresses&quot;: null,
        &quot;IPAddress&quot;: &quot;172.17.0.3&quot;,
                &quot;IPAddress&quot;: &quot;172.17.0.3&quot;,
&lt;/pre&gt;
&lt;p&gt;Now you know the container IP address, to login into karaf, first we need to download and install karaf client tool
Go to the following URL to download the package: &lt;a href=&quot;http://www.apache.org/dyn/closer.lua/karaf/4.0.5/apache-karaf-4.0.5.tar.gz&quot; target=&quot;_blank&quot;&gt;http://www.apache.org/dyn/closer.lua/karaf/4.0.5/apache-karaf-4.0.5.tar.gz&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Extract the files and move to the new directory&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost Downloads]$ tar -xzvf apache-karaf-4.0.5.tar.gz 
[egonzalez@localhost Downloads]$ cd apache-karaf-4.0.5/
&lt;/pre&gt;
&lt;p&gt;Execute the client authenticating with the container IP&lt;/p&gt;
&lt;pre&gt;[egonzalez@localhost apache-karaf-4.0.5]$ ./bin/client -a 8101 -h 172.17.0.3 -u karaf -v
client: JAVA_HOME not set; results may vary
13 [main] INFO org.apache.sshd.common.util.SecurityUtils - BouncyCastle not registered, using the default JCE provider
Logging in as karaf
194 [sshd-SshClient[12bb4df8]-nio2-thread-1] INFO org.apache.sshd.client.session.ClientSessionImpl - Client session created
203 [main] INFO org.apache.sshd.client.session.ClientSessionImpl - Start flagging packets as pending until key exchange is done
204 [sshd-SshClient[12bb4df8]-nio2-thread-1] INFO org.apache.sshd.client.session.ClientSessionImpl - Server version string: SSH-2.0-SSHD-CORE-0.12.0
321 [sshd-SshClient[12bb4df8]-nio2-thread-3] WARN org.apache.sshd.client.keyverifier.AcceptAllServerKeyVerifier - Server at /172.17.0.3:8101 presented unverified DSA key: 09:a0:45:95:7a:dd:94:7c:6b:c3:f9:c0:23:88:1d:b0
324 [sshd-SshClient[12bb4df8]-nio2-thread-3] INFO org.apache.sshd.client.session.ClientSessionImpl - Dequeing pending packets
327 [sshd-SshClient[12bb4df8]-nio2-thread-4] INFO org.apache.sshd.client.session.ClientUserAuthServiceNew - Received SSH_MSG_USERAUTH_FAILURE
338 [sshd-SshClient[12bb4df8]-nio2-thread-5] INFO org.apache.sshd.client.session.ClientUserAuthServiceNew - Received SSH_MSG_USERAUTH_FAILURE
341 [sshd-SshClient[12bb4df8]-nio2-thread-6] INFO org.apache.sshd.client.auth.UserAuthKeyboardInteractive - Received Password authentication  en-US
344 [sshd-SshClient[12bb4df8]-nio2-thread-7] INFO org.apache.sshd.client.session.ClientUserAuthServiceNew - Received SSH_MSG_USERAUTH_SUCCESS
                                                                                           
    ________                       ________                .__  .__       .__     __       
    \_____  \ ______   ____   ____ \______ \ _____  ___.__.|  | |__| ____ |  |___/  |_     
     /   |   \\____ \_/ __ \ /    \ |    |  \\__  \&amp;lt; | || | | |/ ___\| | \ __\ / | \ |_&amp;gt; &amp;gt;  ___/|   |  \|    `   \/ __ \\___  ||  |_|  / /_/  &amp;gt;   Y  \  |      
    \_______  /   __/ \___  &amp;gt;___|  /_______  (____  / ____||____/__\___  /|___|  /__|      
            \/|__|        \/     \/        \/     \/\/            /_____/      \/          
                                                                                           

Hit '' for a list of available commands
and '[cmd] --help' for help on a specific command.
Hit '' or type 'system:shutdown' or 'logout' to shutdown OpenDaylight.
&lt;/pre&gt;
&lt;p&gt;Once karaf login succeed, install a few features like DLUX&lt;/p&gt;
&lt;pre&gt;opendaylight-user@root&amp;gt;feature:install odl-restconf odl-l2switch-switch odl-mdsal-apidocs odl-dlux-core
&lt;/pre&gt;
&lt;p&gt;Now you can login at the container IP with admin as username and password.&lt;/p&gt;
&lt;pre&gt;http://172.17.0.3:8181/index.html
&lt;/pre&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter wp-image-1226 size-full&quot; src=&quot;http://egonzalez.org/wp-content/uploads/2016/06/Selection_001.png&quot; alt=&quot;Selection_001&quot; width=&quot;774&quot; height=&quot;619&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Best regards&lt;/p&gt;</content><author><name>Editor</name></author><category term="--configure" /><category term="container" /><category term="controller" /><category term="docker" /><category term="install" /><category term="OpenDaylight" /><category term="sdn" /><summary type="html">This is a quick guide to start a Docker container with OpenDaylight running on it.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://217.182.136.201:5000/wp-content/uploads/2016/06/Selection_001-e1465233899442.png" /></entry><entry><title type="html">OpenStack Kolla deployment from RDO packages</title><link href="http://217.182.136.201:5000/openstack-kolla-deployment-from-rdo-packages/" rel="alternate" type="text/html" title="OpenStack Kolla deployment from  RDO packages" /><published>2016-04-24T04:12:04+00:00</published><updated>2016-04-24T04:12:04+00:00</updated><id>http://217.182.136.201:5000/openstack-kolla-deployment-from-rdo-packages</id><content type="html" xml:base="http://217.182.136.201:5000/openstack-kolla-deployment-from-rdo-packages/">&lt;p&gt;OpenStack, Ansible, Docker, production ready, HA, etc. Nothing can be so interesting as Kolla.
Kolla includes all you need to create, maintain and operate an OpenStack environment.
All the services will be installed along the nodes you specify inside docker containers with high availability and load balancing between services by default, you don’t need to care about an external tool for these purposes.
In future posts, i will talk in more detail about Kolla and how works, also more tips or deployment types. For now, go to the official documentation.
At this demo, i will use:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;x1 Deployment node: Laptop with 12GB of RAM and a single CPU&lt;/li&gt;
  &lt;li&gt;x3 Target nodes: VMs with 24GB of RAM and 2 vCPU each one.&lt;/li&gt;
  &lt;li&gt;All nodes connected to a shared connection with 300Mbs&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;all-nodes&quot;&gt;ALL NODES&lt;/h2&gt;

&lt;p&gt;Before deploy OpenStack with Kolla, we need to ensure all the nodes got time synchronized.&lt;/p&gt;
&lt;pre&gt;
yum -y install ntp
systemctl enable ntpd.service
systemctl start ntpd.service
&lt;/pre&gt;
&lt;p&gt;Next, stop and disable libvirt service to avoid conflicts with libvirt containers.&lt;/p&gt;
&lt;pre&gt;
systemctl stop libvirtd
systemctl disable libvirtd
&lt;/pre&gt;
&lt;p&gt;Install docker&lt;/p&gt;
&lt;pre&gt;
curl -sSL https://get.docker.io | bash
&lt;/pre&gt;
&lt;p&gt;Add the user you are using to docker group so this user can issue docker commands without sudo. Logoff and login to apply changes.&lt;/p&gt;
&lt;pre&gt;
sudo usermod -aG docker root
&lt;/pre&gt;
&lt;p&gt;Create a file called kolla.conf with the following content.&lt;/p&gt;
&lt;pre&gt;
vi /etc/systemd/system/docker.service.d/kolla.conf
[Service]
MountFlags=shared
&lt;/pre&gt;
&lt;p&gt;Restart and enable docker service&lt;/p&gt;
&lt;pre&gt;
systemctl restart docker
systemctl enable docker
&lt;/pre&gt;
&lt;p&gt;Install some packages who are needed by next steps.&lt;/p&gt;
&lt;pre&gt;
yum install -y python-devel libffi-devel openssl-devel gcc git python-pip python-openstackclient
&lt;/pre&gt;

&lt;h3 id=&quot;deploy-node&quot;&gt;DEPLOY NODE&lt;/h3&gt;

&lt;p&gt;Install EPEL repository&lt;/p&gt;
&lt;pre&gt;
yum install -y epel-release
&lt;/pre&gt;
&lt;p&gt;Install ansible&lt;/p&gt;
&lt;pre&gt;
yum install -y ansible
&lt;/pre&gt;
&lt;p&gt;Clone Kolla mitaka/stable code.&lt;/p&gt;
&lt;pre&gt;
git clone https://git.openstack.org/openstack/kolla -b stable/mitaka
&lt;/pre&gt;
&lt;p&gt;Install kolla and dependencies.&lt;/p&gt;
&lt;pre&gt;
pip install kolla/
&lt;/pre&gt;
&lt;p&gt;Copy kolla configuration files to /etc/&lt;/p&gt;
&lt;pre&gt;
cd kolla
cp -r etc/kolla /etc/
&lt;/pre&gt;
&lt;p&gt;Create kolla build config file&lt;/p&gt;
&lt;pre&gt;
pip install tox
tox -e genconfig
&lt;/pre&gt;
&lt;p&gt;Edit kolla-build file with the following content&lt;/p&gt;
&lt;pre&gt;
vi /etc/kolla/kolla-build.conf 

base = centos
base_tag = mitaka
push = true
install_type = rdo
registry = docker.io
&lt;/pre&gt;
&lt;p&gt;Login with your DockerHub account, sometimes, login doesn’t works as expected. Review auth url at authentication file in ~/.docker/ directory. After Austin Summit i will post exact changes i made in the URL.&lt;/p&gt;
&lt;pre&gt;
docker login
&lt;/pre&gt;
&lt;p&gt;Create and push the images to your DockerHub account.
If images are not automatically pushed to the remote repository, push them manually once image creation finished.
Building images can last various hours, in my experience sometimes were built in 3 hours and another times in 9 hours. And much more if you are going to push them to your DockerHub instead of a private registry.&lt;/p&gt;
&lt;pre&gt;
kolla-build -n egonzalez90 --push
&lt;/pre&gt;
&lt;p&gt;Review all docker images kolla has created.&lt;/p&gt;
&lt;pre&gt;
[egonzalez@localhost kolla]$ docker images | grep mitaka
egonzalez90/centos-binary-cinder-api                  mitaka              ba2cca4b09fa        16 hours ago        814.5 MB
egonzalez90/centos-binary-cinder-volume               mitaka              1d31a049f327        16 hours ago        802.4 MB
egonzalez90/centos-binary-cinder-rpcbind              mitaka              5f7bc909f41b        16 hours ago        804.2 MB
egonzalez90/centos-binary-mesos-slave                 mitaka              57a0e00d1901        16 hours ago        651.6 MB
egonzalez90/centos-binary-swift-rsyncd                mitaka              36f5b9c9d4c5        16 hours ago        565.3 MB
egonzalez90/centos-binary-cinder-backup               mitaka              a7a8161398fe        16 hours ago        775.3 MB
egonzalez90/centos-binary-cinder-scheduler            mitaka              a5c5b79a25f6        16 hours ago        775.3 MB
egonzalez90/centos-binary-marathon                    mitaka              704ce8261a7f        16 hours ago        770.4 MB
egonzalez90/centos-binary-chronos                     mitaka              974525562cea        16 hours ago        732.8 MB
egonzalez90/centos-binary-swift-object                mitaka              e09b529bad32        16 hours ago        582.9 MB
egonzalez90/centos-binary-swift-account               mitaka              573b8e5bd3c7        16 hours ago        582.9 MB
egonzalez90/centos-binary-swift-container             mitaka              c63d9a5be014        16 hours ago        583.2 MB
egonzalez90/centos-binary-mesos-master                mitaka              2610881df9c0        16 hours ago        536.8 MB
egonzalez90/centos-binary-swift-proxy-server          mitaka              3632ee65ace9        16 hours ago        584.7 MB
egonzalez90/centos-binary-ceilometer-api              mitaka              808cd12e9287        16 hours ago        598.6 MB
egonzalez90/centos-binary-ceilometer-compute          mitaka              59e7a5e3bd79        16 hours ago        612.6 MB
egonzalez90/centos-binary-ceilometer-central          mitaka              de094dabf9fd        16 hours ago        612.6 MB
egonzalez90/centos-binary-magnum-api                  mitaka              6ce41a1856f8        16 hours ago        690 MB
egonzalez90/centos-binary-glance-api                  mitaka              2a1c8702341a        16 hours ago        688.5 MB
egonzalez90/centos-binary-ceilometer-notification     mitaka              7ccb484383ae        16 hours ago        594 MB
egonzalez90/centos-binary-ceilometer-collector        mitaka              c2e043f6e2b1        16 hours ago        595.4 MB
egonzalez90/centos-binary-magnum-conductor            mitaka              19674f37dc9b        16 hours ago        790.8 MB
egonzalez90/centos-binary-aodh-api                    mitaka              c35c48dee3c4        16 hours ago        593.2 MB
egonzalez90/centos-binary-glance-registry             mitaka              a72949aaaf45        16 hours ago        688.5 MB
egonzalez90/centos-binary-aodh-expirer                mitaka              ffa9bc296a02        16 hours ago        593.2 MB
egonzalez90/centos-binary-aodh-evaluator              mitaka              c214eac9bbd9        16 hours ago        593.2 MB
egonzalez90/centos-binary-neutron-metadata-agent      mitaka              0cea7ba50b8e        16 hours ago        817.9 MB
egonzalez90/centos-binary-aodh-listener               mitaka              c5d255b20d4e        16 hours ago        593.2 MB
egonzalez90/centos-binary-aodh-notifier               mitaka              dbd4c8d5515d        16 hours ago        593.2 MB
egonzalez90/centos-binary-neutron-server              mitaka              688d6800684b        16 hours ago        817.9 MB
egonzalez90/centos-binary-gnocchi-api                 mitaka              5f8daeb7a511        17 hours ago        840.8 MB
egonzalez90/centos-binary-neutron-openvswitch-agent   mitaka              3c2f03d388fa        17 hours ago        843.4 MB
egonzalez90/centos-binary-nova-compute                mitaka              aef19eb18b41        17 hours ago        1.076 GB
egonzalez90/centos-binary-neutron-linuxbridge-agent   mitaka              672550e296af        17 hours ago        843.1 MB
egonzalez90/centos-binary-nova-libvirt                mitaka              46cd6d68a29d        17 hours ago        1.127 GB
egonzalez90/centos-binary-gnocchi-statsd              mitaka              8369b97d0fb7        17 hours ago        840.7 MB
egonzalez90/centos-binary-neutron-dhcp-agent          mitaka              b6a6de5c4d3f        17 hours ago        817.9 MB
egonzalez90/centos-binary-neutron-l3-agent            mitaka              6d4956cd63e6        17 hours ago        817.9 MB
egonzalez90/centos-binary-nova-spicehtml5proxy        mitaka              6db500ef18b0        17 hours ago        629.5 MB
egonzalez90/centos-binary-nova-compute-ironic         mitaka              89f4f8ba32b9        17 hours ago        1.04 GB
egonzalez90/centos-binary-nova-conductor              mitaka              71e00696b65a        17 hours ago        629.4 MB
egonzalez90/centos-binary-nova-novncproxy             mitaka              4153ed5cdfa5        17 hours ago        630 MB
egonzalez90/centos-binary-nova-api                    mitaka              7bf702527a50        17 hours ago        629.4 MB
egonzalez90/centos-binary-nova-ssh                    mitaka              0c71e10ba8bb        17 hours ago        630.4 MB
egonzalez90/centos-binary-nova-network                mitaka              ff2ed3dc65ab        17 hours ago        630.4 MB
egonzalez90/centos-binary-heat-api                    mitaka              3f3bac2b91b4        17 hours ago        592.2 MB
egonzalez90/centos-binary-nova-consoleauth            mitaka              f7f558ed3061        17 hours ago        629.5 MB
egonzalez90/centos-binary-nova-scheduler              mitaka              f9b8750d4812        17 hours ago        629.4 MB
egonzalez90/centos-binary-heat-engine                 mitaka              69b416b2481c        17 hours ago        592.2 MB
egonzalez90/centos-binary-heat-api-cfn                mitaka              220acaf5f692        18 hours ago        592.2 MB
egonzalez90/centos-binary-manila-api                  mitaka              3e21270b4e91        18 hours ago        588.4 MB
egonzalez90/centos-binary-trove-api                   mitaka              68868b718307        18 hours ago        585.8 MB
egonzalez90/centos-binary-manila-share                mitaka              45e069ec5233        18 hours ago        637.8 MB
egonzalez90/centos-binary-trove-guestagent            mitaka              484a9b5b5631        18 hours ago        586.1 MB
egonzalez90/centos-binary-trove-conductor             mitaka              2817941fed43        18 hours ago        585.8 MB
egonzalez90/centos-binary-trove-taskmanager           mitaka              16fc85e299a1        18 hours ago        585.8 MB
egonzalez90/centos-binary-manila-scheduler            mitaka              075beb4c058e        18 hours ago        588.4 MB
egonzalez90/centos-binary-designate-api               mitaka              0dfb2e4b971d        18 hours ago        589.8 MB
egonzalez90/centos-binary-designate-central           mitaka              d4ab5d846989        18 hours ago        589.8 MB
egonzalez90/centos-binary-designate-poolmanager       mitaka              17570055aa01        18 hours ago        594.3 MB
egonzalez90/centos-binary-designate-sink              mitaka              16e1113010dd        18 hours ago        589.8 MB
egonzalez90/centos-binary-designate-backend-bind9     mitaka              a83d15642a07        18 hours ago        594.3 MB
egonzalez90/centos-binary-cinder-base                 mitaka              ebc196468197        18 hours ago        775.3 MB
egonzalez90/centos-binary-ironic-pxe                  mitaka              3b825ca5e758        18 hours ago        595.2 MB
egonzalez90/centos-binary-ironic-api                  mitaka              53b3a144266a        18 hours ago        591.6 MB
egonzalez90/centos-binary-zookeeper                   mitaka              91270c923346        18 hours ago        544.8 MB
egonzalez90/centos-binary-designate-mdns              mitaka              2de6dfb55068        18 hours ago        589.8 MB
egonzalez90/centos-binary-ironic-inspector            mitaka              631d5c362116        18 hours ago        597.4 MB
egonzalez90/centos-binary-ironic-conductor            mitaka              aceccff4bef0        18 hours ago        620.3 MB
egonzalez90/centos-binary-horizon                     mitaka              b8a5f7db8daf        18 hours ago        690.6 MB
egonzalez90/centos-binary-swift-base                  mitaka              c98164063b84        18 hours ago        563.7 MB
egonzalez90/centos-binary-mesos-base                  mitaka              a50e0e1e8edc        18 hours ago        536.5 MB
egonzalez90/centos-binary-ceilometer-base             mitaka              07164b2054b8        18 hours ago        574.2 MB
egonzalez90/centos-binary-glance-base                 mitaka              b40e34f047d7        18 hours ago        688.5 MB
egonzalez90/centos-binary-magnum-base                 mitaka              bad9157e57ba        18 hours ago        668.3 MB
egonzalez90/centos-binary-aodh-base                   mitaka              9a919ceb1213        19 hours ago        573.5 MB
egonzalez90/centos-binary-neutron-base                mitaka              7669e9646a22        19 hours ago        817.9 MB
egonzalez90/centos-binary-gnocchi-base                mitaka              509a5c7395fb        19 hours ago        817.5 MB
egonzalez90/centos-binary-keystone                    mitaka              231990ed7b4d        19 hours ago        606.4 MB
egonzalez90/centos-binary-nova-base                   mitaka              a4523a00e9b2        19 hours ago        608.8 MB
egonzalez90/centos-binary-zaqar                       mitaka              43b8675a9bda        19 hours ago        607.4 MB
egonzalez90/centos-binary-heat-base                   mitaka              10662065592f        19 hours ago        572.6 MB
egonzalez90/centos-binary-manila-base                 mitaka              215fc8275580        19 hours ago        588.4 MB
egonzalez90/centos-binary-trove-base                  mitaka              0eda6621a5c3        19 hours ago        566.5 MB
egonzalez90/centos-binary-designate-base              mitaka              dc53110d609c        19 hours ago        570.2 MB
egonzalez90/centos-binary-dind                        mitaka              f2e7bbe028b4        19 hours ago        539.3 MB
egonzalez90/centos-binary-tempest                     mitaka              28cceef2319d        19 hours ago        628 MB
egonzalez90/centos-binary-ironic-base                 mitaka              7b52957bf3a0        19 hours ago        572 MB
egonzalez90/centos-binary-openvswitch-db-server       mitaka              a624dd2d260d        19 hours ago        379 MB
egonzalez90/centos-binary-openvswitch-vswitchd        mitaka              4c36af8e0e44        20 hours ago        379 MB
egonzalez90/centos-binary-ceph-mon                    mitaka              81486c6a7605        20 hours ago        553.3 MB
egonzalez90/centos-binary-kolla-toolbox               mitaka              3fc4535c3d5e        20 hours ago        675.4 MB
egonzalez90/centos-binary-elasticsearch               mitaka              0a81ba71ec7f        20 hours ago        576.4 MB
egonzalez90/centos-binary-keepalived                  mitaka              3559905c7d86        20 hours ago        409.3 MB
egonzalez90/centos-binary-ceph-osd                    mitaka              26dc5c40e160        20 hours ago        553.3 MB
egonzalez90/centos-binary-heka                        mitaka              919dd5a93ca3        20 hours ago        420.6 MB
egonzalez90/centos-binary-rabbitmq                    mitaka              4ab020955a66        20 hours ago        552.7 MB
egonzalez90/centos-binary-mesosphere-base             mitaka              a9f2a4c7cf1c        20 hours ago        381.9 MB
egonzalez90/centos-binary-openstack-base              mitaka              46a527edf49a        20 hours ago        539.3 MB
egonzalez90/centos-binary-ceph-rgw                    mitaka              f57ab1371bd3        20 hours ago        553.3 MB
egonzalez90/centos-binary-openvswitch-base            mitaka              f91c5a909b2c        20 hours ago        379 MB
egonzalez90/centos-binary-mariadb                     mitaka              8fe89c13a637        20 hours ago        678.6 MB
egonzalez90/centos-binary-cron                        mitaka              a239ea240c2e        20 hours ago        366.7 MB
egonzalez90/centos-binary-mongodb                     mitaka              48946c962d7e        20 hours ago        539.2 MB
egonzalez90/centos-binary-ceph-base                   mitaka              02be30a43c6e        20 hours ago        553.3 MB
egonzalez90/centos-binary-haproxy                     mitaka              b8d8ac3e371d        20 hours ago        367.4 MB
egonzalez90/centos-binary-memcached                   mitaka              175026eb6466        20 hours ago        404.1 MB
egonzalez90/centos-binary-kibana                      mitaka              885aeb0b2b97        20 hours ago        490.9 MB
egonzalez90/centos-binary-mesos-dns                   mitaka              95e29f8429e7        21 hours ago        361 MB
egonzalez90/centos-binary-base                        mitaka              b104d01004c6        21 hours ago        349.2 MB
&lt;/pre&gt;

&lt;h2 id=&quot;target-hosts&quot;&gt;TARGET HOSTS&lt;/h2&gt;

&lt;p&gt;In target nodes, a newer version of pip and docker-py is needed, install it.&lt;/p&gt;
&lt;pre&gt;
sudo pip install -U pip
pip install -U docker-py
&lt;/pre&gt;

&lt;h2 id=&quot;deploy-kolla&quot;&gt;DEPLOY KOLLA&lt;/h2&gt;

&lt;p&gt;Kolla ships a tool to create random passwords, issue this command to run this tool. Also, you can modify passwords file at /etc/kolla/ directory.&lt;/p&gt;
&lt;pre&gt;
kolla-genpwd
&lt;/pre&gt;
&lt;p&gt;Edit globals.yml file with the following content, use your own info if necessary.
Change docker_namespace with your docker account name.&lt;/p&gt;
&lt;pre&gt;
vi /etc/kolla/globals.yml

kolla_base_distro: &quot;centos&quot;
kolla_install_type: &quot;binary&quot;
openstack_release: &quot;mitaka&quot; ## Tag at docker hub
kolla_internal_vip_address: &quot;192.168.1.90&quot;
docker_registry: &quot;docker.io&quot;
docker_namespace: &quot;egonzalez90&quot;
network_interface: &quot;eth2&quot;
neutron_external_interface: &quot;ens9&quot;
&lt;/pre&gt;
&lt;p&gt;Edit the inventory file with your server’s IPs or hostnames.&lt;/p&gt;
&lt;pre&gt;
vi ansible/inventory/multinode

[control]
# These hostname must be resolvable from your deployment host
192.168.1.77
192.168.1.74
192.168.1.78

# The network nodes are where your l3-agent and loadbalancers will run
# This can be the same as a host in the control group
[network]
192.168.1.77
192.168.1.74
192.168.1.78

[compute]
192.168.1.77
192.168.1.74
192.168.1.78

# When compute nodes and control nodes use different interfaces,
# you can specify &quot;api_interface&quot; and another interfaces like below:
#compute01 neutron_external_interface=eth0 api_interface=em1 storage_interface=em1 tunnel_interface=em1

[storage]
192.168.1.77
192.168.1.74
192.168.1.78
&lt;/pre&gt;
&lt;p&gt;Create an SSH key to login into target servers.&lt;/p&gt;
&lt;pre&gt;
[root@kolla-deployment-node kolla]# ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
bd:3e:ce:7c:2a:6b:a7:99:ed:04:cf:c2:60:5f:2f:12 root@kolla-deployment-node
The key's randomart image is:
+--[ RSA 2048]----+
|                 |
|                 |
|                 |
|         .       |
|      o E o      |
|     . + * o     |
|        = * .    |
|        o@o..    |
|       .=BO+     |
+-----------------+
&lt;/pre&gt;
&lt;p&gt;Copy the SSH key you have previously created to all your target nodes.&lt;/p&gt;
&lt;pre&gt;
[root@kolla-deployment-node kolla]# ssh-copy-id root@192.168.1.77
[root@kolla-deployment-node kolla]# ssh-copy-id root@192.168.1.74
[root@kolla-deployment-node kolla]# ssh-copy-id root@192.168.1.78
&lt;/pre&gt;
&lt;p&gt;Ensure all hostnames can be resolved between all the nodes, this is a necessary step, if not, rabbitmq will fail.
If using a DNS server you can skip this task.
Configure hosts file.&lt;/p&gt;
&lt;pre&gt;
vi /etc/hosts

192.168.1.77 node1
192.168.1.74 node2
192.168.1.78 node3
&lt;/pre&gt;
&lt;p&gt;Copy hosts file to the other nodes.&lt;/p&gt;
&lt;pre&gt;
scp /etc/hosts root@node2:/etc/hosts
scp /etc/hosts root@node3:/etc/hosts
&lt;/pre&gt;
&lt;p&gt;Execute the prechecks tool to ensure all requisites are ok.&lt;/p&gt;
&lt;pre&gt;
[root@kolla-deployment-node kolla]# kolla-ansible prechecks -i ansible/inventory/multinode 
Pre-deployment checking : ansible-playbook -i ansible/inventory/multinode -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla  /usr/share/kolla/ansible/prechecks.yml 

PLAY [all] ******************************************************************** 

GATHERING FACTS *************************************************************** 
ok: [192.168.1.77]
ok: [192.168.1.74]
ok: [192.168.1.78]
.......................
PLAY RECAP ******************************************************************** 
192.168.1.74               : ok=63   changed=0    unreachable=0    failed=0   
192.168.1.77               : ok=63   changed=0    unreachable=0    failed=0   
192.168.1.78               : ok=63   changed=0    unreachable=0    failed=0   
&lt;/pre&gt;
&lt;p&gt;Once all requistes are passed, start the installation of OpenStack by Kolla.
The first time usually take a long time, because docker images need to be pulled into target hosts, and more if pull comes from DockerHub registry instead of a local one.&lt;/p&gt;
&lt;pre&gt;
[root@kolla-deployment-node kolla]# kolla-ansible deploy -i ansible/inventory/multinode
Deploying Playbooks : ansible-playbook -i ansible/inventory/multinode -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla  -e action=deploy /usr/share/kolla/ansible/site.yml 

PLAY [ceph-mon;ceph-osd;ceph-rgw] ********************************************* 

GATHERING FACTS *************************************************************** 
ok: [192.168.1.77]
ok: [192.168.1.74]
ok: [192.168.1.78]

TASK: [common | Ensuring config directories exist] **************************** 
skipping: [192.168.1.77] =&amp;gt; (item=heka)
skipping: [192.168.1.74] =&amp;gt; (item=heka)
skipping: [192.168.1.77] =&amp;gt; (item=cron)
skipping: [192.168.1.78] =&amp;gt; (item=heka)
skipping: [192.168.1.74] =&amp;gt; (item=cron)
skipping: [192.168.1.77] =&amp;gt; (item=cron/logrotate)
skipping: [192.168.1.74] =&amp;gt; (item=cron/logrotate)
skipping: [192.168.1.78] =&amp;gt; (item=cron)
skipping: [192.168.1.78] =&amp;gt; (item=cron/logrotate)

.......................

PLAY RECAP ******************************************************************** 
192.168.1.74               : ok=301  changed=93   unreachable=0    failed=0   
192.168.1.77               : ok=301  changed=93   unreachable=0    failed=0   
192.168.1.78               : ok=301  changed=93   unreachable=0    failed=0   
&lt;/pre&gt;
&lt;p&gt;Execute this tool to create a credential file.&lt;/p&gt;
&lt;pre&gt;
[root@kolla-deployment-node kolla]# kolla-ansible post-deploy

Post-Deploying Playbooks : ansible-playbook -i /usr/share/kolla/ansible/inventory/all-in-one -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla  /usr/share/kolla/ansible/post-deploy.yml 

PLAY [Creating admin openrc file on the deploy node] ************************** 

GATHERING FACTS *************************************************************** 
ok: [localhost]

TASK: [template ] ************************************************************* 
changed: [localhost]

PLAY RECAP ******************************************************************** 
localhost                  : ok=2    changed=1    unreachable=0    failed=0   
&lt;/pre&gt;
&lt;p&gt;Source credential file.&lt;/p&gt;
&lt;pre&gt;
[root@kolla-deployment-node kolla]# source /etc/kolla/admin-openrc.sh
&lt;/pre&gt;
&lt;p&gt;Kolla ships a tool to create a base Openstack configuration layout, this will create networks, routers, images, etc.
Execute it in the newly OpenStack environment.&lt;/p&gt;
&lt;pre&gt;
[root@kolla-deployment-node kolla]# tools/init-runonce
Downloading glance image.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 12.6M  100 12.6M    0     0   873k      0  0:00:14  0:00:14 --:--:-- 1823k
Creating glance image.
[=============================&amp;gt;] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | ee1eca47dc88f4879d8a229cc70a07c6     |
| container_format | bare                                 |
| created_at       | 2016-04-15T19:41:20.000000           |
| deleted          | False                                |
| deleted_at       | None                                 |
| disk_format      | qcow2                                |
| id               | 0b5ec320-ace9-4b34-93cb-54fa6f2c70f5 |
| is_public        | False                                |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | cirros                               |
| owner            | a9c2e6c6a55b40619d4f12f05aea03f1     |
| protected        | False                                |
| size             | 13287936                             |
| status           | active                               |
| updated_at       | 2016-04-15T19:42:35.000000           |
| virtual_size     | None                                 |
+------------------+--------------------------------------+
Configuring neutron.
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2016-04-15T19:43:07                  |
| description               |                                      |
| id                        | 12c74cdb-9218-4d8b-ab24-d5bc7f17d8c5 |
| ipv4_address_scope        |                                      |
| ipv6_address_scope        |                                      |
| is_default                | False                                |
| mtu                       | 1500                                 |
| name                      | public1                              |
| provider:network_type     | flat                                 |
| provider:physical_network | physnet1                             |
| provider:segmentation_id  |                                      |
| router:external           | True                                 |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| tenant_id                 | a9c2e6c6a55b40619d4f12f05aea03f1     |
| updated_at                | 2016-04-15T19:43:07                  |
+---------------------------+--------------------------------------+
Created a new subnet:
+-------------------+----------------------------------------------+
| Field             | Value                                        |
+-------------------+----------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;10.0.2.150&quot;, &quot;end&quot;: &quot;10.0.2.199&quot;} |
| cidr              | 10.0.2.0/24                                  |
| created_at        | 2016-04-15T19:43:47                          |
| description       |                                              |
| dns_nameservers   |                                              |
| enable_dhcp       | False                                        |
| gateway_ip        | 10.0.2.1                                     |
| host_routes       |                                              |
| id                | 274bee58-68bb-4a96-bae5-41c03022a363         |
| ip_version        | 4                                            |
| ipv6_address_mode |                                              |
| ipv6_ra_mode      |                                              |
| name              | 1-subnet                                     |
| network_id        | 12c74cdb-9218-4d8b-ab24-d5bc7f17d8c5         |
| subnetpool_id     |                                              |
| tenant_id         | a9c2e6c6a55b40619d4f12f05aea03f1             |
| updated_at        | 2016-04-15T19:43:47                          |
+-------------------+----------------------------------------------+
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| availability_zone_hints   |                                      |
| availability_zones        |                                      |
| created_at                | 2016-04-15T19:44:42                  |
| description               |                                      |
| id                        | 9bb7cca0-e7ea-4601-8770-7296473bdfff |
| ipv4_address_scope        |                                      |
| ipv6_address_scope        |                                      |
| mtu                       | 1450                                 |
| name                      | demo-net                             |
| provider:network_type     | vxlan                                |
| provider:physical_network |                                      |
| provider:segmentation_id  | 94                                   |
| router:external           | False                                |
| shared                    | False                                |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tags                      |                                      |
| tenant_id                 | a9c2e6c6a55b40619d4f12f05aea03f1     |
| updated_at                | 2016-04-15T19:44:43                  |
+---------------------------+--------------------------------------+
Created a new subnet:
+-------------------+--------------------------------------------+
| Field             | Value                                      |
+-------------------+--------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;10.0.0.2&quot;, &quot;end&quot;: &quot;10.0.0.254&quot;} |
| cidr              | 10.0.0.0/24                                |
| created_at        | 2016-04-15T19:45:25                        |
| description       |                                            |
| dns_nameservers   | 8.8.8.8                                    |
| enable_dhcp       | True                                       |
| gateway_ip        | 10.0.0.1                                   |
| host_routes       |                                            |
| id                | 28ef0e39-33a4-43ea-b1a6-8ea01d7c3379       |
| ip_version        | 4                                          |
| ipv6_address_mode |                                            |
| ipv6_ra_mode      |                                            |
| name              | demo-subnet                                |
| network_id        | 9bb7cca0-e7ea-4601-8770-7296473bdfff       |
| subnetpool_id     |                                            |
| tenant_id         | a9c2e6c6a55b40619d4f12f05aea03f1           |
| updated_at        | 2016-04-15T19:45:25                        |
+-------------------+--------------------------------------------+
Created a new router:
+-------------------------+--------------------------------------+
| Field                   | Value                                |
+-------------------------+--------------------------------------+
| admin_state_up          | True                                 |
| availability_zone_hints |                                      |
| availability_zones      |                                      |
| description             |                                      |
| distributed             | False                                |
| external_gateway_info   |                                      |
| ha                      | False                                |
| id                      | 53a09f8a-576a-4f83-82b0-995a26f83deb |
| name                    | demo-router                          |
| routes                  |                                      |
| status                  | ACTIVE                               |
| tenant_id               | a9c2e6c6a55b40619d4f12f05aea03f1     |
+-------------------------+--------------------------------------+
Added interface ed81ba4c-0e51-4cd9-9810-0a9b883102c2 to router demo-router.
Set gateway for router demo-router
Created a new security_group_rule:
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| description       |                                      |
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | 4f836611-830d-48e7-a81c-7aa65a2573a4 |
| port_range_max    |                                      |
| port_range_min    |                                      |
| protocol          | icmp                                 |
| remote_group_id   |                                      |
| remote_ip_prefix  | 0.0.0.0/0                            |
| security_group_id | c9e76d1f-d58c-4621-b402-1295d9e5168d |
| tenant_id         | a9c2e6c6a55b40619d4f12f05aea03f1     |
+-------------------+--------------------------------------+
Created a new security_group_rule:
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| description       |                                      |
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | 8cb6c081-0388-4d94-98f8-58190c574133 |
| port_range_max    | 22                                   |
| port_range_min    | 22                                   |
| protocol          | tcp                                  |
| remote_group_id   |                                      |
| remote_ip_prefix  | 0.0.0.0/0                            |
| security_group_id | c9e76d1f-d58c-4621-b402-1295d9e5168d |
| tenant_id         | a9c2e6c6a55b40619d4f12f05aea03f1     |
+-------------------+--------------------------------------+
Created a new security_group_rule:
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| description       |                                      |
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | 76142824-3cb2-43a5-bbd7-635aedd05666 |
| port_range_max    | 8000                                 |
| port_range_min    | 8000                                 |
| protocol          | tcp                                  |
| remote_group_id   |                                      |
| remote_ip_prefix  | 0.0.0.0/0                            |
| security_group_id | c9e76d1f-d58c-4621-b402-1295d9e5168d |
| tenant_id         | a9c2e6c6a55b40619d4f12f05aea03f1     |
+-------------------+--------------------------------------+
Created a new security_group_rule:
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| description       |                                      |
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | ce77b36f-a9ed-4c10-ba1f-2697ad1c8138 |
| port_range_max    | 8080                                 |
| port_range_min    | 8080                                 |
| protocol          | tcp                                  |
| remote_group_id   |                                      |
| remote_ip_prefix  | 0.0.0.0/0                            |
| security_group_id | c9e76d1f-d58c-4621-b402-1295d9e5168d |
| tenant_id         | a9c2e6c6a55b40619d4f12f05aea03f1     |
+-------------------+--------------------------------------+
Configuring nova public key and quotas.
&lt;/pre&gt;
&lt;p&gt;Check nova services status&lt;/p&gt;
&lt;pre&gt;
[egonzalez@localhost kolla]$ nova service-list
+----+------------------+-------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host  | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+-------+----------+---------+-------+----------------------------+-----------------+
| 40 | nova-consoleauth | node3 | internal | enabled | up    | 2016-04-15T20:15:44.000000 | -               |
| 43 | nova-consoleauth | node1 | internal | enabled | up    | 2016-04-15T20:15:46.000000 | -               |
| 46 | nova-consoleauth | node2 | internal | enabled | up    | 2016-04-15T20:15:48.000000 | -               |
| 49 | nova-scheduler   | node3 | internal | enabled | up    | 2016-04-15T20:15:50.000000 | -               |
| 52 | nova-scheduler   | node2 | internal | enabled | up    | 2016-04-15T20:15:42.000000 | -               |
| 55 | nova-scheduler   | node1 | internal | enabled | up    | 2016-04-15T20:15:43.000000 | -               |
| 58 | nova-conductor   | node1 | internal | enabled | up    | 2016-04-15T20:15:36.000000 | -               |
| 64 | nova-conductor   | node2 | internal | enabled | up    | 2016-04-15T20:15:37.000000 | -               |
| 70 | nova-conductor   | node3 | internal | enabled | up    | 2016-04-15T20:15:35.000000 | -               |
| 79 | nova-compute     | node3 | nova     | enabled | up    | 2016-04-15T20:15:43.000000 | -               |
| 85 | nova-compute     | node2 | nova     | enabled | up    | 2016-04-15T20:15:50.000000 | -               |
| 88 | nova-compute     | node1 | nova     | enabled | up    | 2016-04-15T20:15:51.000000 | -               |
+----+------------------+-------+----------+---------+-------+----------------------------+-----------------+
&lt;/pre&gt;
&lt;p&gt;Check Neutron agents status.&lt;/p&gt;
&lt;pre&gt;
[egonzalez@localhost kolla]$ neutron agent-list
+--------------------------------------+--------------------+-------+-------+----------------+---------------------------+
| id                                   | agent_type         | host  | alive | admin_state_up | binary                    |
+--------------------------------------+--------------------+-------+-------+----------------+---------------------------+
| 08d12ccd-74cd-4e8e-9cda-3d3d2e191191 | Metadata agent     | node3 | :-)   | True           | neutron-metadata-agent    |
| 0916aa0e-6d07-4398-99a5-e0e9123cef37 | DHCP agent         | node1 | :-)   | True           | neutron-dhcp-agent        |
| 14707eaf-2d37-4eaf-964a-82b63d1bdc96 | Open vSwitch agent | node3 | :-)   | True           | neutron-openvswitch-agent |
| 265a0acc-e31a-4098-842a-b139e8095056 | L3 agent           | node2 | :-)   | True           | neutron-l3-agent          |
| 50869311-b3bb-4fb3-9676-d1f56d77deb0 | Metadata agent     | node2 | :-)   | True           | neutron-metadata-agent    |
| 5c48b20a-1b57-4e3b-865a-f0f298ea0af8 | DHCP agent         | node2 | :-)   | True           | neutron-dhcp-agent        |
| 89470cc7-6430-45a2-8ee2-852e0ba85cff | Open vSwitch agent | node2 | :-)   | True           | neutron-openvswitch-agent |
| ba689300-c49a-46a7-8c85-e7a6daa5f2cb | DHCP agent         | node3 | :-)   | True           | neutron-dhcp-agent        |
| baadfe87-db69-491b-b7ad-7f16c1468632 | Metadata agent     | node1 | :-)   | True           | neutron-metadata-agent    |
| bc823fff-11a3-4f81-90d5-8f9e4a7a617a | L3 agent           | node3 | :-)   | True           | neutron-l3-agent          |
| d26c860d-e5e3-4da0-b0af-f8ad3a69e9f6 | L3 agent           | node1 | :-)   | True           | neutron-l3-agent          |
| e90277e7-3e46-42d0-a2fd-dce412f503dd | Open vSwitch agent | node1 | :-)   | True           | neutron-openvswitch-agent |
+--------------------------------------+--------------------+-------+-------+----------------+---------------------------+
&lt;/pre&gt;
&lt;p&gt;Create a new instance and see what happens.&lt;/p&gt;
&lt;pre&gt;
[egonzalez@localhost kolla]$ openstack server create --image cirros --flavor m1.tiny --nic net-id=demo-net demo-instance
&lt;/pre&gt;
&lt;p&gt;Check how the instance is going.&lt;/p&gt;
&lt;pre&gt;
[egonzalez@localhost kolla]$ openstack server list
+--------------------------------------+---------------+--------+-------------------+
| ID                                   | Name          | Status | Networks          |
+--------------------------------------+---------------+--------+-------------------+
| b234e514-2975-47fd-a618-8ef6aa9ff2bc | demo-instance | ACTIVE | demo-net=10.0.0.3 |
+--------------------------------------+---------------+--------+-------------------+
&lt;/pre&gt;

&lt;p&gt;Thats all for now, in future posts we will see in more detail how Kolla works.&lt;/p&gt;

&lt;p&gt;Cheers, Eduardo Gonzalez&lt;/p&gt;</content><author><name>Editor</name></author><category term="ansible" /><category term="deploy" /><category term="docker" /><category term="how to" /><category term="kolla" /><category term="openstack" /><category term="rdo" /><summary type="html">OpenStack, Ansible, Docker, production ready, HA, etc. Nothing can be so interesting as Kolla. Kolla includes all you need to create, maintain and operate an OpenStack environment. All the services will be installed along the nodes you specify inside docker containers with high availability and load balancing between services by default, you don’t need to care about an external tool for these purposes. In future posts, i will talk in more detail about Kolla and how works, also more tips or deployment types. For now, go to the official documentation. At this demo, i will use: x1 Deployment node: Laptop with 12GB of RAM and a single CPU x3 Target nodes: VMs with 24GB of RAM and 2 vCPU each one. All nodes connected to a shared connection with 300Mbs ALL NODES Before deploy OpenStack with Kolla, we need to ensure all the nodes got time synchronized. yum -y install ntp systemctl enable ntpd.service systemctl start ntpd.service Next, stop and disable libvirt service to avoid conflicts with libvirt containers. systemctl stop libvirtd systemctl disable libvirtd Install docker curl -sSL https://get.docker.io | bash Add the user you are using to docker group so this user can issue docker commands without sudo. Logoff and login to apply changes. sudo usermod -aG docker root Create a file called kolla.conf with the following content. vi /etc/systemd/system/docker.service.d/kolla.conf [Service] MountFlags=shared Restart and enable docker service systemctl restart docker systemctl enable docker Install some packages who are needed by next steps. yum install -y python-devel libffi-devel openssl-devel gcc git python-pip python-openstackclient DEPLOY NODE Install EPEL repository yum install -y epel-release Install ansible yum install -y ansible Clone Kolla mitaka/stable code. git clone https://git.openstack.org/openstack/kolla -b stable/mitaka Install kolla and dependencies. pip install kolla/ Copy kolla configuration files to /etc/ cd kolla cp -r etc/kolla /etc/ Create kolla build config file pip install tox tox -e genconfig Edit kolla-build file with the following content vi /etc/kolla/kolla-build.conf base = centos base_tag = mitaka push = true install_type = rdo registry = docker.io Login with your DockerHub account, sometimes, login doesn’t works as expected. Review auth url at authentication file in ~/.docker/ directory. After Austin Summit i will post exact changes i made in the URL. docker login Create and push the images to your DockerHub account. If images are not automatically pushed to the remote repository, push them manually once image creation finished. Building images can last various hours, in my experience sometimes were built in 3 hours and another times in 9 hours. And much more if you are going to push them to your DockerHub instead of a private registry. kolla-build -n egonzalez90 --push Review all docker images kolla has created. [egonzalez@localhost kolla]$ docker images | grep mitaka egonzalez90/centos-binary-cinder-api mitaka ba2cca4b09fa 16 hours ago 814.5 MB egonzalez90/centos-binary-cinder-volume mitaka 1d31a049f327 16 hours ago 802.4 MB egonzalez90/centos-binary-cinder-rpcbind mitaka 5f7bc909f41b 16 hours ago 804.2 MB egonzalez90/centos-binary-mesos-slave mitaka 57a0e00d1901 16 hours ago 651.6 MB egonzalez90/centos-binary-swift-rsyncd mitaka 36f5b9c9d4c5 16 hours ago 565.3 MB egonzalez90/centos-binary-cinder-backup mitaka a7a8161398fe 16 hours ago 775.3 MB egonzalez90/centos-binary-cinder-scheduler mitaka a5c5b79a25f6 16 hours ago 775.3 MB egonzalez90/centos-binary-marathon mitaka 704ce8261a7f 16 hours ago 770.4 MB egonzalez90/centos-binary-chronos mitaka 974525562cea 16 hours ago 732.8 MB egonzalez90/centos-binary-swift-object mitaka e09b529bad32 16 hours ago 582.9 MB egonzalez90/centos-binary-swift-account mitaka 573b8e5bd3c7 16 hours ago 582.9 MB egonzalez90/centos-binary-swift-container mitaka c63d9a5be014 16 hours ago 583.2 MB egonzalez90/centos-binary-mesos-master mitaka 2610881df9c0 16 hours ago 536.8 MB egonzalez90/centos-binary-swift-proxy-server mitaka 3632ee65ace9 16 hours ago 584.7 MB egonzalez90/centos-binary-ceilometer-api mitaka 808cd12e9287 16 hours ago 598.6 MB egonzalez90/centos-binary-ceilometer-compute mitaka 59e7a5e3bd79 16 hours ago 612.6 MB egonzalez90/centos-binary-ceilometer-central mitaka de094dabf9fd 16 hours ago 612.6 MB egonzalez90/centos-binary-magnum-api mitaka 6ce41a1856f8 16 hours ago 690 MB egonzalez90/centos-binary-glance-api mitaka 2a1c8702341a 16 hours ago 688.5 MB egonzalez90/centos-binary-ceilometer-notification mitaka 7ccb484383ae 16 hours ago 594 MB egonzalez90/centos-binary-ceilometer-collector mitaka c2e043f6e2b1 16 hours ago 595.4 MB egonzalez90/centos-binary-magnum-conductor mitaka 19674f37dc9b 16 hours ago 790.8 MB egonzalez90/centos-binary-aodh-api mitaka c35c48dee3c4 16 hours ago 593.2 MB egonzalez90/centos-binary-glance-registry mitaka a72949aaaf45 16 hours ago 688.5 MB egonzalez90/centos-binary-aodh-expirer mitaka ffa9bc296a02 16 hours ago 593.2 MB egonzalez90/centos-binary-aodh-evaluator mitaka c214eac9bbd9 16 hours ago 593.2 MB egonzalez90/centos-binary-neutron-metadata-agent mitaka 0cea7ba50b8e 16 hours ago 817.9 MB egonzalez90/centos-binary-aodh-listener mitaka c5d255b20d4e 16 hours ago 593.2 MB egonzalez90/centos-binary-aodh-notifier mitaka dbd4c8d5515d 16 hours ago 593.2 MB egonzalez90/centos-binary-neutron-server mitaka 688d6800684b 16 hours ago 817.9 MB egonzalez90/centos-binary-gnocchi-api mitaka 5f8daeb7a511 17 hours ago 840.8 MB egonzalez90/centos-binary-neutron-openvswitch-agent mitaka 3c2f03d388fa 17 hours ago 843.4 MB egonzalez90/centos-binary-nova-compute mitaka aef19eb18b41 17 hours ago 1.076 GB egonzalez90/centos-binary-neutron-linuxbridge-agent mitaka 672550e296af 17 hours ago 843.1 MB egonzalez90/centos-binary-nova-libvirt mitaka 46cd6d68a29d 17 hours ago 1.127 GB egonzalez90/centos-binary-gnocchi-statsd mitaka 8369b97d0fb7 17 hours ago 840.7 MB egonzalez90/centos-binary-neutron-dhcp-agent mitaka b6a6de5c4d3f 17 hours ago 817.9 MB egonzalez90/centos-binary-neutron-l3-agent mitaka 6d4956cd63e6 17 hours ago 817.9 MB egonzalez90/centos-binary-nova-spicehtml5proxy mitaka 6db500ef18b0 17 hours ago 629.5 MB egonzalez90/centos-binary-nova-compute-ironic mitaka 89f4f8ba32b9 17 hours ago 1.04 GB egonzalez90/centos-binary-nova-conductor mitaka 71e00696b65a 17 hours ago 629.4 MB egonzalez90/centos-binary-nova-novncproxy mitaka 4153ed5cdfa5 17 hours ago 630 MB egonzalez90/centos-binary-nova-api mitaka 7bf702527a50 17 hours ago 629.4 MB egonzalez90/centos-binary-nova-ssh mitaka 0c71e10ba8bb 17 hours ago 630.4 MB egonzalez90/centos-binary-nova-network mitaka ff2ed3dc65ab 17 hours ago 630.4 MB egonzalez90/centos-binary-heat-api mitaka 3f3bac2b91b4 17 hours ago 592.2 MB egonzalez90/centos-binary-nova-consoleauth mitaka f7f558ed3061 17 hours ago 629.5 MB egonzalez90/centos-binary-nova-scheduler mitaka f9b8750d4812 17 hours ago 629.4 MB egonzalez90/centos-binary-heat-engine mitaka 69b416b2481c 17 hours ago 592.2 MB egonzalez90/centos-binary-heat-api-cfn mitaka 220acaf5f692 18 hours ago 592.2 MB egonzalez90/centos-binary-manila-api mitaka 3e21270b4e91 18 hours ago 588.4 MB egonzalez90/centos-binary-trove-api mitaka 68868b718307 18 hours ago 585.8 MB egonzalez90/centos-binary-manila-share mitaka 45e069ec5233 18 hours ago 637.8 MB egonzalez90/centos-binary-trove-guestagent mitaka 484a9b5b5631 18 hours ago 586.1 MB egonzalez90/centos-binary-trove-conductor mitaka 2817941fed43 18 hours ago 585.8 MB egonzalez90/centos-binary-trove-taskmanager mitaka 16fc85e299a1 18 hours ago 585.8 MB egonzalez90/centos-binary-manila-scheduler mitaka 075beb4c058e 18 hours ago 588.4 MB egonzalez90/centos-binary-designate-api mitaka 0dfb2e4b971d 18 hours ago 589.8 MB egonzalez90/centos-binary-designate-central mitaka d4ab5d846989 18 hours ago 589.8 MB egonzalez90/centos-binary-designate-poolmanager mitaka 17570055aa01 18 hours ago 594.3 MB egonzalez90/centos-binary-designate-sink mitaka 16e1113010dd 18 hours ago 589.8 MB egonzalez90/centos-binary-designate-backend-bind9 mitaka a83d15642a07 18 hours ago 594.3 MB egonzalez90/centos-binary-cinder-base mitaka ebc196468197 18 hours ago 775.3 MB egonzalez90/centos-binary-ironic-pxe mitaka 3b825ca5e758 18 hours ago 595.2 MB egonzalez90/centos-binary-ironic-api mitaka 53b3a144266a 18 hours ago 591.6 MB egonzalez90/centos-binary-zookeeper mitaka 91270c923346 18 hours ago 544.8 MB egonzalez90/centos-binary-designate-mdns mitaka 2de6dfb55068 18 hours ago 589.8 MB egonzalez90/centos-binary-ironic-inspector mitaka 631d5c362116 18 hours ago 597.4 MB egonzalez90/centos-binary-ironic-conductor mitaka aceccff4bef0 18 hours ago 620.3 MB egonzalez90/centos-binary-horizon mitaka b8a5f7db8daf 18 hours ago 690.6 MB egonzalez90/centos-binary-swift-base mitaka c98164063b84 18 hours ago 563.7 MB egonzalez90/centos-binary-mesos-base mitaka a50e0e1e8edc 18 hours ago 536.5 MB egonzalez90/centos-binary-ceilometer-base mitaka 07164b2054b8 18 hours ago 574.2 MB egonzalez90/centos-binary-glance-base mitaka b40e34f047d7 18 hours ago 688.5 MB egonzalez90/centos-binary-magnum-base mitaka bad9157e57ba 18 hours ago 668.3 MB egonzalez90/centos-binary-aodh-base mitaka 9a919ceb1213 19 hours ago 573.5 MB egonzalez90/centos-binary-neutron-base mitaka 7669e9646a22 19 hours ago 817.9 MB egonzalez90/centos-binary-gnocchi-base mitaka 509a5c7395fb 19 hours ago 817.5 MB egonzalez90/centos-binary-keystone mitaka 231990ed7b4d 19 hours ago 606.4 MB egonzalez90/centos-binary-nova-base mitaka a4523a00e9b2 19 hours ago 608.8 MB egonzalez90/centos-binary-zaqar mitaka 43b8675a9bda 19 hours ago 607.4 MB egonzalez90/centos-binary-heat-base mitaka 10662065592f 19 hours ago 572.6 MB egonzalez90/centos-binary-manila-base mitaka 215fc8275580 19 hours ago 588.4 MB egonzalez90/centos-binary-trove-base mitaka 0eda6621a5c3 19 hours ago 566.5 MB egonzalez90/centos-binary-designate-base mitaka dc53110d609c 19 hours ago 570.2 MB egonzalez90/centos-binary-dind mitaka f2e7bbe028b4 19 hours ago 539.3 MB egonzalez90/centos-binary-tempest mitaka 28cceef2319d 19 hours ago 628 MB egonzalez90/centos-binary-ironic-base mitaka 7b52957bf3a0 19 hours ago 572 MB egonzalez90/centos-binary-openvswitch-db-server mitaka a624dd2d260d 19 hours ago 379 MB egonzalez90/centos-binary-openvswitch-vswitchd mitaka 4c36af8e0e44 20 hours ago 379 MB egonzalez90/centos-binary-ceph-mon mitaka 81486c6a7605 20 hours ago 553.3 MB egonzalez90/centos-binary-kolla-toolbox mitaka 3fc4535c3d5e 20 hours ago 675.4 MB egonzalez90/centos-binary-elasticsearch mitaka 0a81ba71ec7f 20 hours ago 576.4 MB egonzalez90/centos-binary-keepalived mitaka 3559905c7d86 20 hours ago 409.3 MB egonzalez90/centos-binary-ceph-osd mitaka 26dc5c40e160 20 hours ago 553.3 MB egonzalez90/centos-binary-heka mitaka 919dd5a93ca3 20 hours ago 420.6 MB egonzalez90/centos-binary-rabbitmq mitaka 4ab020955a66 20 hours ago 552.7 MB egonzalez90/centos-binary-mesosphere-base mitaka a9f2a4c7cf1c 20 hours ago 381.9 MB egonzalez90/centos-binary-openstack-base mitaka 46a527edf49a 20 hours ago 539.3 MB egonzalez90/centos-binary-ceph-rgw mitaka f57ab1371bd3 20 hours ago 553.3 MB egonzalez90/centos-binary-openvswitch-base mitaka f91c5a909b2c 20 hours ago 379 MB egonzalez90/centos-binary-mariadb mitaka 8fe89c13a637 20 hours ago 678.6 MB egonzalez90/centos-binary-cron mitaka a239ea240c2e 20 hours ago 366.7 MB egonzalez90/centos-binary-mongodb mitaka 48946c962d7e 20 hours ago 539.2 MB egonzalez90/centos-binary-ceph-base mitaka 02be30a43c6e 20 hours ago 553.3 MB egonzalez90/centos-binary-haproxy mitaka b8d8ac3e371d 20 hours ago 367.4 MB egonzalez90/centos-binary-memcached mitaka 175026eb6466 20 hours ago 404.1 MB egonzalez90/centos-binary-kibana mitaka 885aeb0b2b97 20 hours ago 490.9 MB egonzalez90/centos-binary-mesos-dns mitaka 95e29f8429e7 21 hours ago 361 MB egonzalez90/centos-binary-base mitaka b104d01004c6 21 hours ago 349.2 MB TARGET HOSTS In target nodes, a newer version of pip and docker-py is needed, install it. sudo pip install -U pip pip install -U docker-py DEPLOY KOLLA Kolla ships a tool to create random passwords, issue this command to run this tool. Also, you can modify passwords file at /etc/kolla/ directory. kolla-genpwd Edit globals.yml file with the following content, use your own info if necessary. Change docker_namespace with your docker account name. vi /etc/kolla/globals.yml kolla_base_distro: &quot;centos&quot; kolla_install_type: &quot;binary&quot; openstack_release: &quot;mitaka&quot; ## Tag at docker hub kolla_internal_vip_address: &quot;192.168.1.90&quot; docker_registry: &quot;docker.io&quot; docker_namespace: &quot;egonzalez90&quot; network_interface: &quot;eth2&quot; neutron_external_interface: &quot;ens9&quot; Edit the inventory file with your server’s IPs or hostnames. vi ansible/inventory/multinode [control] # These hostname must be resolvable from your deployment host 192.168.1.77 192.168.1.74 192.168.1.78 # The network nodes are where your l3-agent and loadbalancers will run # This can be the same as a host in the control group [network] 192.168.1.77 192.168.1.74 192.168.1.78 [compute] 192.168.1.77 192.168.1.74 192.168.1.78 # When compute nodes and control nodes use different interfaces, # you can specify &quot;api_interface&quot; and another interfaces like below: #compute01 neutron_external_interface=eth0 api_interface=em1 storage_interface=em1 tunnel_interface=em1 [storage] 192.168.1.77 192.168.1.74 192.168.1.78 Create an SSH key to login into target servers. [root@kolla-deployment-node kolla]# ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: bd:3e:ce:7c:2a:6b:a7:99:ed:04:cf:c2:60:5f:2f:12 root@kolla-deployment-node The key's randomart image is: +--[ RSA 2048]----+ | | | | | | | . | | o E o | | . + * o | | = * . | | o@o.. | | .=BO+ | +-----------------+ Copy the SSH key you have previously created to all your target nodes. [root@kolla-deployment-node kolla]# ssh-copy-id root@192.168.1.77 [root@kolla-deployment-node kolla]# ssh-copy-id root@192.168.1.74 [root@kolla-deployment-node kolla]# ssh-copy-id root@192.168.1.78 Ensure all hostnames can be resolved between all the nodes, this is a necessary step, if not, rabbitmq will fail. If using a DNS server you can skip this task. Configure hosts file. vi /etc/hosts 192.168.1.77 node1 192.168.1.74 node2 192.168.1.78 node3 Copy hosts file to the other nodes. scp /etc/hosts root@node2:/etc/hosts scp /etc/hosts root@node3:/etc/hosts Execute the prechecks tool to ensure all requisites are ok. [root@kolla-deployment-node kolla]# kolla-ansible prechecks -i ansible/inventory/multinode Pre-deployment checking : ansible-playbook -i ansible/inventory/multinode -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla /usr/share/kolla/ansible/prechecks.yml PLAY [all] ******************************************************************** GATHERING FACTS *************************************************************** ok: [192.168.1.77] ok: [192.168.1.74] ok: [192.168.1.78] ....................... PLAY RECAP ******************************************************************** 192.168.1.74 : ok=63 changed=0 unreachable=0 failed=0 192.168.1.77 : ok=63 changed=0 unreachable=0 failed=0 192.168.1.78 : ok=63 changed=0 unreachable=0 failed=0 Once all requistes are passed, start the installation of OpenStack by Kolla. The first time usually take a long time, because docker images need to be pulled into target hosts, and more if pull comes from DockerHub registry instead of a local one. [root@kolla-deployment-node kolla]# kolla-ansible deploy -i ansible/inventory/multinode Deploying Playbooks : ansible-playbook -i ansible/inventory/multinode -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla -e action=deploy /usr/share/kolla/ansible/site.yml PLAY [ceph-mon;ceph-osd;ceph-rgw] ********************************************* GATHERING FACTS *************************************************************** ok: [192.168.1.77] ok: [192.168.1.74] ok: [192.168.1.78] TASK: [common | Ensuring config directories exist] **************************** skipping: [192.168.1.77] =&amp;gt; (item=heka) skipping: [192.168.1.74] =&amp;gt; (item=heka) skipping: [192.168.1.77] =&amp;gt; (item=cron) skipping: [192.168.1.78] =&amp;gt; (item=heka) skipping: [192.168.1.74] =&amp;gt; (item=cron) skipping: [192.168.1.77] =&amp;gt; (item=cron/logrotate) skipping: [192.168.1.74] =&amp;gt; (item=cron/logrotate) skipping: [192.168.1.78] =&amp;gt; (item=cron) skipping: [192.168.1.78] =&amp;gt; (item=cron/logrotate) ....................... PLAY RECAP ******************************************************************** 192.168.1.74 : ok=301 changed=93 unreachable=0 failed=0 192.168.1.77 : ok=301 changed=93 unreachable=0 failed=0 192.168.1.78 : ok=301 changed=93 unreachable=0 failed=0 Execute this tool to create a credential file. [root@kolla-deployment-node kolla]# kolla-ansible post-deploy Post-Deploying Playbooks : ansible-playbook -i /usr/share/kolla/ansible/inventory/all-in-one -e @/etc/kolla/globals.yml -e @/etc/kolla/passwords.yml -e CONFIG_DIR=/etc/kolla /usr/share/kolla/ansible/post-deploy.yml PLAY [Creating admin openrc file on the deploy node] ************************** GATHERING FACTS *************************************************************** ok: [localhost] TASK: [template ] ************************************************************* changed: [localhost] PLAY RECAP ******************************************************************** localhost : ok=2 changed=1 unreachable=0 failed=0 Source credential file. [root@kolla-deployment-node kolla]# source /etc/kolla/admin-openrc.sh Kolla ships a tool to create a base Openstack configuration layout, this will create networks, routers, images, etc. Execute it in the newly OpenStack environment. [root@kolla-deployment-node kolla]# tools/init-runonce Downloading glance image. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 12.6M 100 12.6M 0 0 873k 0 0:00:14 0:00:14 --:--:-- 1823k Creating glance image. [=============================&amp;gt;] 100% +------------------+--------------------------------------+ | Property | Value | +------------------+--------------------------------------+ | checksum | ee1eca47dc88f4879d8a229cc70a07c6 | | container_format | bare | | created_at | 2016-04-15T19:41:20.000000 | | deleted | False | | deleted_at | None | | disk_format | qcow2 | | id | 0b5ec320-ace9-4b34-93cb-54fa6f2c70f5 | | is_public | False | | min_disk | 0 | | min_ram | 0 | | name | cirros | | owner | a9c2e6c6a55b40619d4f12f05aea03f1 | | protected | False | | size | 13287936 | | status | active | | updated_at | 2016-04-15T19:42:35.000000 | | virtual_size | None | +------------------+--------------------------------------+ Configuring neutron. Created a new network: +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | True | | availability_zone_hints | | | availability_zones | | | created_at | 2016-04-15T19:43:07 | | description | | | id | 12c74cdb-9218-4d8b-ab24-d5bc7f17d8c5 | | ipv4_address_scope | | | ipv6_address_scope | | | is_default | False | | mtu | 1500 | | name | public1 | | provider:network_type | flat | | provider:physical_network | physnet1 | | provider:segmentation_id | | | router:external | True | | shared | False | | status | ACTIVE | | subnets | | | tags | | | tenant_id | a9c2e6c6a55b40619d4f12f05aea03f1 | | updated_at | 2016-04-15T19:43:07 | +---------------------------+--------------------------------------+ Created a new subnet: +-------------------+----------------------------------------------+ | Field | Value | +-------------------+----------------------------------------------+ | allocation_pools | {&quot;start&quot;: &quot;10.0.2.150&quot;, &quot;end&quot;: &quot;10.0.2.199&quot;} | | cidr | 10.0.2.0/24 | | created_at | 2016-04-15T19:43:47 | | description | | | dns_nameservers | | | enable_dhcp | False | | gateway_ip | 10.0.2.1 | | host_routes | | | id | 274bee58-68bb-4a96-bae5-41c03022a363 | | ip_version | 4 | | ipv6_address_mode | | | ipv6_ra_mode | | | name | 1-subnet | | network_id | 12c74cdb-9218-4d8b-ab24-d5bc7f17d8c5 | | subnetpool_id | | | tenant_id | a9c2e6c6a55b40619d4f12f05aea03f1 | | updated_at | 2016-04-15T19:43:47 | +-------------------+----------------------------------------------+ Created a new network: +---------------------------+--------------------------------------+ | Field | Value | +---------------------------+--------------------------------------+ | admin_state_up | True | | availability_zone_hints | | | availability_zones | | | created_at | 2016-04-15T19:44:42 | | description | | | id | 9bb7cca0-e7ea-4601-8770-7296473bdfff | | ipv4_address_scope | | | ipv6_address_scope | | | mtu | 1450 | | name | demo-net | | provider:network_type | vxlan | | provider:physical_network | | | provider:segmentation_id | 94 | | router:external | False | | shared | False | | status | ACTIVE | | subnets | | | tags | | | tenant_id | a9c2e6c6a55b40619d4f12f05aea03f1 | | updated_at | 2016-04-15T19:44:43 | +---------------------------+--------------------------------------+ Created a new subnet: +-------------------+--------------------------------------------+ | Field | Value | +-------------------+--------------------------------------------+ | allocation_pools | {&quot;start&quot;: &quot;10.0.0.2&quot;, &quot;end&quot;: &quot;10.0.0.254&quot;} | | cidr | 10.0.0.0/24 | | created_at | 2016-04-15T19:45:25 | | description | | | dns_nameservers | 8.8.8.8 | | enable_dhcp | True | | gateway_ip | 10.0.0.1 | | host_routes | | | id | 28ef0e39-33a4-43ea-b1a6-8ea01d7c3379 | | ip_version | 4 | | ipv6_address_mode | | | ipv6_ra_mode | | | name | demo-subnet | | network_id | 9bb7cca0-e7ea-4601-8770-7296473bdfff | | subnetpool_id | | | tenant_id | a9c2e6c6a55b40619d4f12f05aea03f1 | | updated_at | 2016-04-15T19:45:25 | +-------------------+--------------------------------------------+ Created a new router: +-------------------------+--------------------------------------+ | Field | Value | +-------------------------+--------------------------------------+ | admin_state_up | True | | availability_zone_hints | | | availability_zones | | | description | | | distributed | False | | external_gateway_info | | | ha | False | | id | 53a09f8a-576a-4f83-82b0-995a26f83deb | | name | demo-router | | routes | | | status | ACTIVE | | tenant_id | a9c2e6c6a55b40619d4f12f05aea03f1 | +-------------------------+--------------------------------------+ Added interface ed81ba4c-0e51-4cd9-9810-0a9b883102c2 to router demo-router. Set gateway for router demo-router Created a new security_group_rule: +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | description | | | direction | ingress | | ethertype | IPv4 | | id | 4f836611-830d-48e7-a81c-7aa65a2573a4 | | port_range_max | | | port_range_min | | | protocol | icmp | | remote_group_id | | | remote_ip_prefix | 0.0.0.0/0 | | security_group_id | c9e76d1f-d58c-4621-b402-1295d9e5168d | | tenant_id | a9c2e6c6a55b40619d4f12f05aea03f1 | +-------------------+--------------------------------------+ Created a new security_group_rule: +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | description | | | direction | ingress | | ethertype | IPv4 | | id | 8cb6c081-0388-4d94-98f8-58190c574133 | | port_range_max | 22 | | port_range_min | 22 | | protocol | tcp | | remote_group_id | | | remote_ip_prefix | 0.0.0.0/0 | | security_group_id | c9e76d1f-d58c-4621-b402-1295d9e5168d | | tenant_id | a9c2e6c6a55b40619d4f12f05aea03f1 | +-------------------+--------------------------------------+ Created a new security_group_rule: +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | description | | | direction | ingress | | ethertype | IPv4 | | id | 76142824-3cb2-43a5-bbd7-635aedd05666 | | port_range_max | 8000 | | port_range_min | 8000 | | protocol | tcp | | remote_group_id | | | remote_ip_prefix | 0.0.0.0/0 | | security_group_id | c9e76d1f-d58c-4621-b402-1295d9e5168d | | tenant_id | a9c2e6c6a55b40619d4f12f05aea03f1 | +-------------------+--------------------------------------+ Created a new security_group_rule: +-------------------+--------------------------------------+ | Field | Value | +-------------------+--------------------------------------+ | description | | | direction | ingress | | ethertype | IPv4 | | id | ce77b36f-a9ed-4c10-ba1f-2697ad1c8138 | | port_range_max | 8080 | | port_range_min | 8080 | | protocol | tcp | | remote_group_id | | | remote_ip_prefix | 0.0.0.0/0 | | security_group_id | c9e76d1f-d58c-4621-b402-1295d9e5168d | | tenant_id | a9c2e6c6a55b40619d4f12f05aea03f1 | +-------------------+--------------------------------------+ Configuring nova public key and quotas. Check nova services status [egonzalez@localhost kolla]$ nova service-list +----+------------------+-------+----------+---------+-------+----------------------------+-----------------+ | Id | Binary | Host | Zone | Status | State | Updated_at | Disabled Reason | +----+------------------+-------+----------+---------+-------+----------------------------+-----------------+ | 40 | nova-consoleauth | node3 | internal | enabled | up | 2016-04-15T20:15:44.000000 | - | | 43 | nova-consoleauth | node1 | internal | enabled | up | 2016-04-15T20:15:46.000000 | - | | 46 | nova-consoleauth | node2 | internal | enabled | up | 2016-04-15T20:15:48.000000 | - | | 49 | nova-scheduler | node3 | internal | enabled | up | 2016-04-15T20:15:50.000000 | - | | 52 | nova-scheduler | node2 | internal | enabled | up | 2016-04-15T20:15:42.000000 | - | | 55 | nova-scheduler | node1 | internal | enabled | up | 2016-04-15T20:15:43.000000 | - | | 58 | nova-conductor | node1 | internal | enabled | up | 2016-04-15T20:15:36.000000 | - | | 64 | nova-conductor | node2 | internal | enabled | up | 2016-04-15T20:15:37.000000 | - | | 70 | nova-conductor | node3 | internal | enabled | up | 2016-04-15T20:15:35.000000 | - | | 79 | nova-compute | node3 | nova | enabled | up | 2016-04-15T20:15:43.000000 | - | | 85 | nova-compute | node2 | nova | enabled | up | 2016-04-15T20:15:50.000000 | - | | 88 | nova-compute | node1 | nova | enabled | up | 2016-04-15T20:15:51.000000 | - | +----+------------------+-------+----------+---------+-------+----------------------------+-----------------+ Check Neutron agents status. [egonzalez@localhost kolla]$ neutron agent-list +--------------------------------------+--------------------+-------+-------+----------------+---------------------------+ | id | agent_type | host | alive | admin_state_up | binary | +--------------------------------------+--------------------+-------+-------+----------------+---------------------------+ | 08d12ccd-74cd-4e8e-9cda-3d3d2e191191 | Metadata agent | node3 | :-) | True | neutron-metadata-agent | | 0916aa0e-6d07-4398-99a5-e0e9123cef37 | DHCP agent | node1 | :-) | True | neutron-dhcp-agent | | 14707eaf-2d37-4eaf-964a-82b63d1bdc96 | Open vSwitch agent | node3 | :-) | True | neutron-openvswitch-agent | | 265a0acc-e31a-4098-842a-b139e8095056 | L3 agent | node2 | :-) | True | neutron-l3-agent | | 50869311-b3bb-4fb3-9676-d1f56d77deb0 | Metadata agent | node2 | :-) | True | neutron-metadata-agent | | 5c48b20a-1b57-4e3b-865a-f0f298ea0af8 | DHCP agent | node2 | :-) | True | neutron-dhcp-agent | | 89470cc7-6430-45a2-8ee2-852e0ba85cff | Open vSwitch agent | node2 | :-) | True | neutron-openvswitch-agent | | ba689300-c49a-46a7-8c85-e7a6daa5f2cb | DHCP agent | node3 | :-) | True | neutron-dhcp-agent | | baadfe87-db69-491b-b7ad-7f16c1468632 | Metadata agent | node1 | :-) | True | neutron-metadata-agent | | bc823fff-11a3-4f81-90d5-8f9e4a7a617a | L3 agent | node3 | :-) | True | neutron-l3-agent | | d26c860d-e5e3-4da0-b0af-f8ad3a69e9f6 | L3 agent | node1 | :-) | True | neutron-l3-agent | | e90277e7-3e46-42d0-a2fd-dce412f503dd | Open vSwitch agent | node1 | :-) | True | neutron-openvswitch-agent | +--------------------------------------+--------------------+-------+-------+----------------+---------------------------+ Create a new instance and see what happens. [egonzalez@localhost kolla]$ openstack server create --image cirros --flavor m1.tiny --nic net-id=demo-net demo-instance Check how the instance is going. [egonzalez@localhost kolla]$ openstack server list +--------------------------------------+---------------+--------+-------------------+ | ID | Name | Status | Networks | +--------------------------------------+---------------+--------+-------------------+ | b234e514-2975-47fd-a618-8ef6aa9ff2bc | demo-instance | ACTIVE | demo-net=10.0.0.3 | +--------------------------------------+---------------+--------+-------------------+ Thats all for now, in future posts we will see in more detail how Kolla works. Cheers, Eduardo Gonzalez</summary></entry><entry><title type="html">Magnum in RDO OpenStack – Liberty Manual Installation from source code</title><link href="http://217.182.136.201:5000/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code/" rel="alternate" type="text/html" title="Magnum in RDO OpenStack &amp;#8211; Liberty Manual Installation from source code" /><published>2016-03-24T19:22:51+00:00</published><updated>2016-03-24T19:22:51+00:00</updated><id>http://217.182.136.201:5000/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code</id><content type="html" xml:base="http://217.182.136.201:5000/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code/">&lt;p&gt;Want to install Magnum (Containers as a Service) in an OpenStack environment based on packages from RDO project?
Here are the steps to do it:&lt;/p&gt;

&lt;p&gt;Primary steps are the same as official Magnum guide, major differences come from DevStack or manual installations vs packages from RDO project.
Also, some of the steps are explained to show how Magnum should work, as well this guide can help you understand Magnum integration with your current environment.
I’m not going to use Barbican service for certs management, you will see how to use Magnum without Barbican too.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For now, there is not RDO packages for magnum, so we are going to install it from source code.&lt;/li&gt;
  &lt;li&gt;As i know, currently magnum packages are under development and will be added in future OpenStack versions to RDO project packages. (Probably Mitaka or Newton)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Passwords used at this demo are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;temporal (Databases and OpenStack users)&lt;/li&gt;
  &lt;li&gt;guest (RabbitMQ)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;IPs used are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;192.168.200.208 (Service APIs)&lt;/li&gt;
  &lt;li&gt;192.168.100.0/24 (External network range)&lt;/li&gt;
  &lt;li&gt;10.0.0.0/24 (Tenant network range)&lt;/li&gt;
  &lt;li&gt;8.8.8.8 (Google DNS server)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First we need to install some dependencies and packages needed for next steps.&lt;/p&gt;
&lt;pre&gt;
sudo yum install -y gcc python-setuptools python-devel git libffi-devel openssl-devel wget
&lt;/pre&gt;
&lt;p&gt;Install pip&lt;/p&gt;
&lt;pre&gt;
easy_install pip
&lt;/pre&gt;
&lt;p&gt;Clone Magnum source code from OpenStack git repository, ensure you use Liberty branch, if not, Magnum dependencies will break all OpenStack services dependencies and lost your current environment (Trust me, i’m talking from my own experience)&lt;/p&gt;
&lt;pre&gt;
git clone https://git.openstack.org/openstack/magnum -b stable/liberty
&lt;/pre&gt;
&lt;p&gt;Move to your newly created folder and install Magnum (dependency requirements and Magnum)&lt;/p&gt;
&lt;pre&gt;
cd magnum
sudo pip install -e .
&lt;/pre&gt;
&lt;p&gt;Once Magnum is installed, create Magnum database and Magnum user&lt;/p&gt;
&lt;pre&gt;
mysql -uroot -p
CREATE DATABASE IF NOT EXISTS magnum DEFAULT CHARACTER SET utf8;
GRANT ALL PRIVILEGES ON magnum.* TO'magnum'@'localhost' IDENTIFIED BY 'temporal';
GRANT ALL PRIVILEGES ON magnum.* TO'magnum'@'%' IDENTIFIED BY 'temporal';
&lt;/pre&gt;
&lt;p&gt;Create Magnum folder and copy sample configuration files.&lt;/p&gt;
&lt;pre&gt;
mkdir /etc/magnum
sudo cp etc/magnum/magnum.conf.sample /etc/magnum/magnum.conf
sudo cp etc/magnum/policy.json /etc/magnum/policy.json
&lt;/pre&gt;
&lt;p&gt;Edit Magnum main configuration file&lt;/p&gt;
&lt;pre&gt;
vi /etc/magnum/magnum.conf
&lt;/pre&gt;
&lt;p&gt;Configure messaging backend to RabbitMQ&lt;/p&gt;
&lt;pre&gt;
[DEFAULT]

rpc_backend = rabbit
notification_driver = messaging
&lt;/pre&gt;
&lt;p&gt;Bind Magnum API port to listen on all the interfaces, you can also especify on which IP Magnum API will be listening if you are concerned about security risks.&lt;/p&gt;
&lt;pre&gt;
[api]

host = 0.0.0.0
&lt;/pre&gt;
&lt;p&gt;Configure RabbitMQ backend&lt;/p&gt;
&lt;pre&gt;
[oslo_messaging_rabbit]

rabbit_host = 192.168.200.208
rabbit_userid = guest
rabbit_password = guest
rabbit_virtual_host = /
&lt;/pre&gt;
&lt;p&gt;Set database connection&lt;/p&gt;
&lt;pre&gt;
[database]

connection=mysql://magnum:temporal@192.168.200.208/magnum
&lt;/pre&gt;
&lt;p&gt;Set cert_manager_type to local, this option will disable Barbican service, you will need to create a folder (We will do it in next steps)&lt;/p&gt;
&lt;pre&gt;
[certificates]

cert_manager_type = local
&lt;/pre&gt;

&lt;p&gt;As all OpenStack services, Keystone authentication is required.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Check what your service tenant name it is (RDO default name is “services” other installations usually use “service” name.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;
[keystone_authtoken]

auth_uri=http://192.168.200.208:5000/v2.0
identity_uri=http://192.168.200.208:35357
auth_strategy=keystone
admin_user=magnum
admin_password=temporal
admin_tenant_name=services
&lt;/pre&gt;
&lt;p&gt;As we saw before, create local certificates folder to avoid using Barbican service. This is the step we previously commented&lt;/p&gt;
&lt;pre&gt;
mkdir -p /var/lib/magnum/certificates/
&lt;/pre&gt;
&lt;p&gt;Clone python-magnumclient and install it, this package will provide us commands to use Magnum&lt;/p&gt;
&lt;pre&gt;
git clone https://git.openstack.org/openstack/python-magnumclient -b stable/liberty
cd python-magnumclient
sudo pip install -e .
&lt;/pre&gt;
&lt;p&gt;Create Magnum user at keystone&lt;/p&gt;
&lt;pre&gt;
openstack user create --password temporal magnum
&lt;/pre&gt;
&lt;p&gt;Add admin role to Magnum user at tenant services&lt;/p&gt;
&lt;pre&gt;
openstack role add --project services --user magnum admin
&lt;/pre&gt;
&lt;p&gt;Create container service&lt;/p&gt;
&lt;pre&gt;
openstack service create --name magnum --description &quot;Magnum Container Service&quot; container
&lt;/pre&gt;
&lt;p&gt;Finally create Magnum endpoints&lt;/p&gt;
&lt;pre&gt;
openstack endpoint create --region RegionOne --publicurl 'http://192.168.200.208:9511/v1' --adminurl 'http://192.168.200.208:9511/v1' --internalurl 'http://192.168.200.208:9511/v1' magnum
&lt;/pre&gt;
&lt;p&gt;Sync Magnum database, this step will create Magnum tables at the database&lt;/p&gt;
&lt;pre&gt;
magnum-db-manage --config-file /etc/magnum/magnum.conf upgrade
&lt;/pre&gt;
&lt;p&gt;Open two terminal session and execute one command on each terminal to start both services. If you encounter any issue, logs can be found at these terminal&lt;/p&gt;
&lt;pre&gt;
magnum-api --config-file /etc/magnum/magnum.conf
magnum-conductor --config-file /etc/magnum/magnum.conf
&lt;/pre&gt;
&lt;p&gt;Check if Magnum service is fine&lt;/p&gt;
&lt;pre&gt;
magnum service-list
+----+------------+------------------+-------+
| id | host       | binary           | state |
+----+------------+------------------+-------+
| 1  | controller | magnum-conductor | up    |
+----+------------+------------------+-------+
&lt;/pre&gt;
&lt;p&gt;Download fedora atomic image&lt;/p&gt;
&lt;pre&gt;
wget https://fedorapeople.org/groups/magnum/fedora-21-atomic-5.qcow2
&lt;/pre&gt;
&lt;p&gt;Create a Glance image with Atomic.qcow2 file&lt;/p&gt;
&lt;pre&gt;
glance image-create --name fedora-21-atomic-5 \
                    --visibility public \
                    --disk-format qcow2 \
                    --os-distro fedora-atomic \
                    --container-format bare &amp;lt; fedora-21-atomic-5.qcow2
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | cebefc0c21fb8567e662bf9f2d5b78b0     |
| container_format | bare                                 |
| created_at       | 2016-03-19T15:55:21Z                 |
| disk_format      | qcow2                                |
| id               | 7293891d-cfba-48a9-a4db-72c29c65f681 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | fedora-21-atomic-5                   |
| os_distro        | fedora-atomic                        |
| owner            | e3cca42ed57745148e0c342a000d99e9     |
| protected        | False                                |
| size             | 891355136                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2016-03-19T15:55:28Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Create a ssh key if not exists, this command won’t create a new ssh key if already exists&lt;/p&gt;
&lt;pre&gt;
test -f ~/.ssh/id_rsa.pub || ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa
&lt;/pre&gt;
&lt;p&gt;Add the key to nova, mine is called egonzalez&lt;/p&gt;
&lt;pre&gt;
nova keypair-add --pub-key ~/.ssh/id_rsa.pub egonzalez
&lt;/pre&gt;

&lt;p&gt;Now we are going to test our new Magnum service, you have various methods to do it.
I will use Docker Swarm method because is the simplest one for this demo purposes. Go through Magnum documentation to check other container methods as Kubernetes is.&lt;/p&gt;

&lt;p&gt;Create a baymodel with atomic image and swarm, select a flavor with at least 10GB of disk&lt;/p&gt;
&lt;pre&gt;
magnum baymodel-create --name demoswarmbaymodel \
                       --image-id fedora-21-atomic-5 \
                       --keypair-id egonzalez \
                       --external-network-id public \
                       --dns-nameserver 8.8.8.8 \
                       --flavor-id testflavor \
                       --docker-volume-size 1 \
                       --coe swarm
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| http_proxy          | None                                 |
| updated_at          | None                                 |
| master_flavor_id    | None                                 |
| fixed_network       | None                                 |
| uuid                | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| no_proxy            | None                                 |
| https_proxy         | None                                 |
| tls_disabled        | False                                |
| keypair_id          | egonzalez                            |
| public              | False                                |
| labels              | {}                                   |
| docker_volume_size  | 1                                    |
| external_network_id | public                               |
| cluster_distro      | fedora-atomic                        |
| image_id            | fedora-21-atomic-5                   |
| registry_enabled    | False                                |
| apiserver_port      | None                                 |
| name                | demoswarmbaymodel                    |
| created_at          | 2016-03-19T17:22:43+00:00            |
| network_driver      | None                                 |
| ssh_authorized_key  | None                                 |
| coe                 | swarm                                |
| flavor_id           | testflavor                           |
| dns_nameserver      | 8.8.8.8                              |
+---------------------+--------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Create a bay with the previous bay model, we are going to create one master node and one worker, specify all that apply to your environment&lt;/p&gt;
&lt;pre&gt;
magnum bay-create --name demoswarmbay --baymodel demoswarmbaymodel --master-count 1 --node-count 1
+--------------------+--------------------------------------+
| Property           | Value                                |
+--------------------+--------------------------------------+
| status             | None                                 |
| uuid               | a2388916-db30-41bf-84eb-df0b65979eaf |
| status_reason      | None                                 |
| created_at         | 2016-03-19T17:23:00+00:00            |
| updated_at         | None                                 |
| bay_create_timeout | 0                                    |
| api_address        | None                                 |
| baymodel_id        | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| node_count         | 1                                    |
| node_addresses     | None                                 |
| master_count       | 1                                    |
| discovery_url      | None                                 |
| name               | demoswarmbay                         |
+--------------------+--------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Check bay status, for now it should be in CREATE_IN_PROGRESS state&lt;/p&gt;
&lt;pre&gt;
magnum bay-show demoswarmbay
+--------------------+--------------------------------------+
| Property           | Value                                |
+--------------------+--------------------------------------+
| status             | CREATE_IN_PROGRESS                   |
| uuid               | a2388916-db30-41bf-84eb-df0b65979eaf |
| status_reason      |                                      |
| created_at         | 2016-03-19T17:23:00+00:00            |
| updated_at         | 2016-03-19T17:23:01+00:00            |
| bay_create_timeout | 0                                    |
| api_address        | None                                 |
| baymodel_id        | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| node_count         | 1                                    |
| node_addresses     | []                                   |
| master_count       | 1                                    |
| discovery_url      | None                                 |
| name               | demoswarmbay                         |
+--------------------+--------------------------------------+
&lt;/pre&gt;
&lt;p&gt;If all is going fine, nova should have two new instances(in ACTIVE state), one for the master node and second for the worker.&lt;/p&gt;
&lt;pre&gt;
nova list
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
| ID                                   | Name                                                  | Status | Task State | Power State | Networks                                                                      |
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
| e38eb88c-bb6b-427d-a2c5-cdfe868796f0 | de-44kx2l4q4wc-0-d6j5svvjxmne-swarm_node-xafkm2jskf5j | ACTIVE | -          | Running     | demoswarmbay-agf6y3qnjoyw-fixed_network-g37bcmc52akv=10.0.0.4, 192.168.100.16 |
| 5acc579d-152a-4656-9eb8-e800b7ab3bcf | demoswarmbay-agf6y3qnjoyw-swarm_master-fllwhrpuabbq   | ACTIVE | -          | Running     | demoswarmbay-agf6y3qnjoyw-fixed_network-g37bcmc52akv=10.0.0.3, 192.168.100.15 |
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
&lt;/pre&gt;
&lt;p&gt;You can see how heat stack is going&lt;/p&gt;
&lt;pre&gt;
heat stack-list
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
| id                                   | stack_name                | stack_status       | creation_time       | updated_time |
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
| 3a64fa60-4df8-498f-aceb-a0cb8cfc0b18 | demoswarmbay-agf6y3qnjoyw | CREATE_IN_PROGRESS | 2016-03-19T17:22:59 | None         |
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
&lt;/pre&gt;
&lt;p&gt;We can see what tasks are executing during stack creation&lt;/p&gt;
&lt;pre&gt;
heat event-list demoswarmbay-agf6y3qnjoyw
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
| resource_name                       | id                                   | resource_status_reason | resource_status    | event_time          |
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
| demoswarmbay-agf6y3qnjoyw           | 004c9388-b8ab-4541-ada8-99b65203e41d | Stack CREATE started   | CREATE_IN_PROGRESS | 2016-03-19T17:23:01 |
| master_wait_handle                  | d6f0798a-bfde-4bad-9c73-e108bd101009 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:01 |
| secgroup_manager                    | e2e0eb08-aeeb-4290-9ad5-bd20fe243f07 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:02 |
| disable_selinux                     | d7290592-ab81-4d7a-b2fa-902975904a25 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| agent_wait_handle                   | 65ec5553-56a4-4416-9748-bfa0ae35737a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| add_proxy                           | 46bdcff8-4606-406f-8c99-7f48adc4de57 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| write_docker_socket                 | ab5402ea-44af-4433-84aa-a63256817a9a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| make_cert                           | 3b9817a5-606f-41ab-8799-b411c017f05d | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| cfn_signal                          | 0add665a-3fdf-4408-ab15-76332aa326fe | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| remove_docker_key                   | 94f4106e-f139-4d9f-9974-8821d04be103 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| configure_swarm                     | f7e0ebd5-1893-43d1-bd29-81a7e39de0c0 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| extrouter                           | a94a8f68-c237-4dbc-9513-cdbe3de1465e | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| enable_services                     | c250f532-99bd-43d7-9d15-b2d3ae16567a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| write_docker_service                | 2c9d8954-4446-4578-a871-0910e8996571 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| cloud_init_wait_handle              | 6cc51d2d-56e9-458b-a21b-bc553e0c8291 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| fixed_network                       | 3125395f-c689-4481-bf01-94bb2f701993 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:07 |
| agent_wait_handle                   | 2db801e8-c2b5-47b0-ac16-122dba3a22d6 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| remove_docker_key                   | 75e2c7a6-a2ce-4026-aeeb-739c4a522f48 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| secgroup_manager                    | ac51a029-26c1-495a-bc13-232cfb8c1060 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| write_docker_socket                 | 58e08b52-a12a-43e9-b41d-071750294024 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| master_wait_handle                  | 3e741b76-6470-47d4-b13e-3f8f446be53c | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| cfn_signal                          | 96c26b4f-1e99-478e-a8e5-9dcc4486e1b3 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| enable_services                     | beedc358-ee72-4b34-a6b9-1b47ffc15306 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| add_proxy                           | caae3a07-d5f1-4eb0-8a82-02ea634f77ae | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| make_cert                           | 79363643-e5e4-4d1b-ad8a-5a56e1f6a8e7 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| cloud_init_wait_handle              | 0457b008-6da8-44fd-abef-cb99bd4d0518 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| configure_swarm                     | baf1e089-c627-4b24-a571-63b3c9c14e28 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| extrouter                           | 184614d9-2280-4cb4-9253-f538463dbdf4 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| write_docker_service                | 80e66b4e-d40a-4243-bb27-0d2a6b68651f | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| disable_selinux                     | d8a64822-2571-4dcf-9da5-b3ec73e771eb | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| fixed_network                       | 528b0ced-23f6-4c22-8cbc-357ba0ee5bc5 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| write_swarm_manager_failure_service | 9fa100a3-b4a9-465c-8b33-dd000cb4866a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| write_swarm_agent_failure_service   | a7c09833-929e-4711-a3e9-39923d23b2f2 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| fixed_subnet                        | 23d8b0a6-a7a3-4f71-9c18-ba6255cf071a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| write_swarm_master_service          | d24a6099-3cad-41ce-8d4b-a7ad0661aaea | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:11 |
| fixed_subnet                        | 1a2b7397-1d09-4544-bb9f-985c2f64cb09 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_manager_failure_service | 615a2a7a-5266-487b-bbe1-fcaa82f43243 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_agent_failure_service   | 3f8c54b4-6644-49a0-ad98-9bc6b4332a07 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_master_service          | 2f58b3c8-d1cc-4590-a328-0e775e495bcf | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| extrouter_inside                    | f3da7f2f-643e-4f29-a00f-d2595d7faeaf | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:14 |
| swarm_master_eth0                   | 1d6a510d-520c-4796-8990-aa8f7dd59757 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:16 |
| swarm_master_eth0                   | 3fd85913-7399-49be-bb46-5085ff953611 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:19 |
| extrouter_inside                    | 33749e30-cbea-4093-b36a-94967e299002 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:19 |
| write_heat_params                   | 054e0af5-e3e0-4bc0-92b5-b40aeedc39ab | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:19 |
| swarm_nodes                         | df7af58c-8148-4b51-bd65-b0734d9051b5 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:20 |
| write_swarm_agent_service           | ab1e8b1e-2837-4693-b791-e1311f85fa63 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:21 |
| swarm_master_floating               | d99ffe66-cb02-4279-99dc-a1f3e2ca817c | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:22 |
| write_heat_params                   | 33d9999f-6c93-453d-8565-ac99db021f8f | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| write_swarm_agent_service           | 02a1b7f6-2660-4345-ad08-42b66ffaaad5 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| swarm_master_floating               | 8ce6ecd8-c421-4e4a-ab81-cba4b5ccedf4 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| swarm_master_init                   | 3787dcc8-e644-412b-859b-63a434b9ee6c | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:26 |
| swarm_master_init                   | a1dd67bb-49c7-4507-8af0-7758b76b57e1 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:28 |
| swarm_master                        | d12b915e-3087-4e17-9954-8233926b504b | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:29 |
| swarm_master                        | a34ad52a-def7-460b-b5b7-410000207b3e | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:48 |
| master_wait_condition               | 0c9331a4-8ad0-46e0-bf2a-35943021a1a3 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
| cloud_init_wait_condition           | de3707a0-f46a-44a9-b4b8-ff50e12cc77f | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
| agent_wait_condition                | a1a810a4-9c19-4983-aaa8-e03f308c1e39 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
&lt;/pre&gt;
&lt;p&gt;Once all tasks are completed, we can create containers in the bay we created in previous steps.&lt;/p&gt;
&lt;pre&gt;
magnum container-create --name demo-container \
                        --image docker.io/cirros:latest \
                        --bay demoswarmbay \
                        --command &quot;ping -c 4 192.168.100.2&quot;
+------------+----------------------------------------+
| Property   | Value                                  |
+------------+----------------------------------------+
| uuid       | 36595858-8657-d465-3e5a-dfcddad8a238   |
| links      | ...                                    |
| bay_uuid   | a2388916-db30-41bf-84eb-df0b65979eaf   |
| updated_at | None                                   |
| image      | cirros                                 |
| command    | ping -c 4 192.168.100.2                |
| created_at | 2016-03-19T17:30:00+00:00              |
| name       | demo-container                         |
+------------+----------------------------------------+
&lt;/pre&gt;
&lt;p&gt;Container is created, but not started.
Start the container&lt;/p&gt;
&lt;pre&gt;
magnum container-start demo-container
&lt;/pre&gt;
&lt;p&gt;Check container logs, you should see 4 pings succeed to our external router gateway.&lt;/p&gt;
&lt;pre&gt;
magnum container-logs demo-container

PING 192.168.100.2 (192.168.100.2) 56(84) bytes of data.
64 bytes from 192.168.100.2: icmp_seq=1 ttl=64 time=0.083 ms
64 bytes from 192.168.100.2: icmp_seq=2 ttl=64 time=0.068 ms
64 bytes from 192.168.100.2: icmp_seq=3 ttl=64 time=0.043 ms
64 bytes from 192.168.100.2: icmp_seq=4 ttl=64 time=0.099 ms
&lt;/pre&gt;
&lt;p&gt;You can delete the container&lt;/p&gt;
&lt;pre&gt;
magnum container-delete demo-container
&lt;/pre&gt;

&lt;p&gt;While doing this demo, i missed adding branch name while cloning Magnum source code, when i installed Magnum all package dependencies where installed from master, who was Mitaka instead of Liberty, which broke my environment.&lt;/p&gt;

&lt;p&gt;I suffered the following issues:&lt;/p&gt;

&lt;p&gt;Issues with packages&lt;/p&gt;
&lt;pre&gt;
ImportError: No module named MySQLdb
&lt;/pre&gt;
&lt;p&gt;Was solved installing MySQL-python from pip instead of yum&lt;/p&gt;
&lt;pre&gt;
pip install MySQL-python
&lt;/pre&gt;
&lt;p&gt;Issues with policies, admin privileges weren’t recognized by Magnum api.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PolicyNotAuthorized: magnum-service:get_all &amp;lt; bunch of stuff &amp;gt; disallowed by policy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Was solved removing admin_api rule at Magnum policy.json file&lt;/p&gt;
&lt;pre&gt;
vi /etc/magnum/policy.json

#    &quot;admin_api&quot;: &quot;rule:context_is_admin&quot;,
&lt;/pre&gt;

&lt;p&gt;Unfortunately, nova was completely broken and it was not working at all, so i installed a new environment and added branch while cloning source code.
Next issue i found was Barbican, who was not installed, i used the steps mentioned at this post to solve this issue.&lt;/p&gt;

&lt;p&gt;Hope this guide helps you integrating Magnum Container as a Service in OpenStack.&lt;/p&gt;

&lt;p&gt;Regards, Eduardo Gonzalez&lt;/p&gt;</content><author><name>Editor</name></author><category term="container" /><category term="docker" /><category term="guide" /><category term="install" /><category term="liberty" /><category term="magnum" /><category term="manual" /><category term="openstack" /><category term="rdo" /><summary type="html">Want to install Magnum (Containers as a Service) in an OpenStack environment based on packages from RDO project? Here are the steps to do it: Primary steps are the same as official Magnum guide, major differences come from DevStack or manual installations vs packages from RDO project. Also, some of the steps are explained to show how Magnum should work, as well this guide can help you understand Magnum integration with your current environment. I’m not going to use Barbican service for certs management, you will see how to use Magnum without Barbican too. For now, there is not RDO packages for magnum, so we are going to install it from source code. As i know, currently magnum packages are under development and will be added in future OpenStack versions to RDO project packages. (Probably Mitaka or Newton) Passwords used at this demo are: temporal (Databases and OpenStack users) guest (RabbitMQ) IPs used are: 192.168.200.208 (Service APIs) 192.168.100.0/24 (External network range) 10.0.0.0/24 (Tenant network range) 8.8.8.8 (Google DNS server) First we need to install some dependencies and packages needed for next steps. sudo yum install -y gcc python-setuptools python-devel git libffi-devel openssl-devel wget Install pip easy_install pip Clone Magnum source code from OpenStack git repository, ensure you use Liberty branch, if not, Magnum dependencies will break all OpenStack services dependencies and lost your current environment (Trust me, i’m talking from my own experience) git clone https://git.openstack.org/openstack/magnum -b stable/liberty Move to your newly created folder and install Magnum (dependency requirements and Magnum) cd magnum sudo pip install -e . Once Magnum is installed, create Magnum database and Magnum user mysql -uroot -p CREATE DATABASE IF NOT EXISTS magnum DEFAULT CHARACTER SET utf8; GRANT ALL PRIVILEGES ON magnum.* TO'magnum'@'localhost' IDENTIFIED BY 'temporal'; GRANT ALL PRIVILEGES ON magnum.* TO'magnum'@'%' IDENTIFIED BY 'temporal'; Create Magnum folder and copy sample configuration files. mkdir /etc/magnum sudo cp etc/magnum/magnum.conf.sample /etc/magnum/magnum.conf sudo cp etc/magnum/policy.json /etc/magnum/policy.json Edit Magnum main configuration file vi /etc/magnum/magnum.conf Configure messaging backend to RabbitMQ [DEFAULT] rpc_backend = rabbit notification_driver = messaging Bind Magnum API port to listen on all the interfaces, you can also especify on which IP Magnum API will be listening if you are concerned about security risks. [api] host = 0.0.0.0 Configure RabbitMQ backend [oslo_messaging_rabbit] rabbit_host = 192.168.200.208 rabbit_userid = guest rabbit_password = guest rabbit_virtual_host = / Set database connection [database] connection=mysql://magnum:temporal@192.168.200.208/magnum Set cert_manager_type to local, this option will disable Barbican service, you will need to create a folder (We will do it in next steps) [certificates] cert_manager_type = local As all OpenStack services, Keystone authentication is required. Check what your service tenant name it is (RDO default name is “services” other installations usually use “service” name. [keystone_authtoken] auth_uri=http://192.168.200.208:5000/v2.0 identity_uri=http://192.168.200.208:35357 auth_strategy=keystone admin_user=magnum admin_password=temporal admin_tenant_name=services As we saw before, create local certificates folder to avoid using Barbican service. This is the step we previously commented mkdir -p /var/lib/magnum/certificates/ Clone python-magnumclient and install it, this package will provide us commands to use Magnum git clone https://git.openstack.org/openstack/python-magnumclient -b stable/liberty cd python-magnumclient sudo pip install -e . Create Magnum user at keystone openstack user create --password temporal magnum Add admin role to Magnum user at tenant services openstack role add --project services --user magnum admin Create container service openstack service create --name magnum --description &quot;Magnum Container Service&quot; container Finally create Magnum endpoints openstack endpoint create --region RegionOne --publicurl 'http://192.168.200.208:9511/v1' --adminurl 'http://192.168.200.208:9511/v1' --internalurl 'http://192.168.200.208:9511/v1' magnum Sync Magnum database, this step will create Magnum tables at the database magnum-db-manage --config-file /etc/magnum/magnum.conf upgrade Open two terminal session and execute one command on each terminal to start both services. If you encounter any issue, logs can be found at these terminal magnum-api --config-file /etc/magnum/magnum.conf magnum-conductor --config-file /etc/magnum/magnum.conf Check if Magnum service is fine magnum service-list +----+------------+------------------+-------+ | id | host | binary | state | +----+------------+------------------+-------+ | 1 | controller | magnum-conductor | up | +----+------------+------------------+-------+ Download fedora atomic image wget https://fedorapeople.org/groups/magnum/fedora-21-atomic-5.qcow2 Create a Glance image with Atomic.qcow2 file glance image-create --name fedora-21-atomic-5 \ --visibility public \ --disk-format qcow2 \ --os-distro fedora-atomic \ --container-format bare &amp;lt; fedora-21-atomic-5.qcow2 +------------------+--------------------------------------+ | Property | Value | +------------------+--------------------------------------+ | checksum | cebefc0c21fb8567e662bf9f2d5b78b0 | | container_format | bare | | created_at | 2016-03-19T15:55:21Z | | disk_format | qcow2 | | id | 7293891d-cfba-48a9-a4db-72c29c65f681 | | min_disk | 0 | | min_ram | 0 | | name | fedora-21-atomic-5 | | os_distro | fedora-atomic | | owner | e3cca42ed57745148e0c342a000d99e9 | | protected | False | | size | 891355136 | | status | active | | tags | [] | | updated_at | 2016-03-19T15:55:28Z | | virtual_size | None | | visibility | public | +------------------+--------------------------------------+ Create a ssh key if not exists, this command won’t create a new ssh key if already exists test -f ~/.ssh/id_rsa.pub || ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa Add the key to nova, mine is called egonzalez nova keypair-add --pub-key ~/.ssh/id_rsa.pub egonzalez Now we are going to test our new Magnum service, you have various methods to do it. I will use Docker Swarm method because is the simplest one for this demo purposes. Go through Magnum documentation to check other container methods as Kubernetes is. Create a baymodel with atomic image and swarm, select a flavor with at least 10GB of disk magnum baymodel-create --name demoswarmbaymodel \ --image-id fedora-21-atomic-5 \ --keypair-id egonzalez \ --external-network-id public \ --dns-nameserver 8.8.8.8 \ --flavor-id testflavor \ --docker-volume-size 1 \ --coe swarm +---------------------+--------------------------------------+ | Property | Value | +---------------------+--------------------------------------+ | http_proxy | None | | updated_at | None | | master_flavor_id | None | | fixed_network | None | | uuid | 887edbc7-0805-4796-be78-dfcddad8eb03 | | no_proxy | None | | https_proxy | None | | tls_disabled | False | | keypair_id | egonzalez | | public | False | | labels | {} | | docker_volume_size | 1 | | external_network_id | public | | cluster_distro | fedora-atomic | | image_id | fedora-21-atomic-5 | | registry_enabled | False | | apiserver_port | None | | name | demoswarmbaymodel | | created_at | 2016-03-19T17:22:43+00:00 | | network_driver | None | | ssh_authorized_key | None | | coe | swarm | | flavor_id | testflavor | | dns_nameserver | 8.8.8.8 | +---------------------+--------------------------------------+ Create a bay with the previous bay model, we are going to create one master node and one worker, specify all that apply to your environment magnum bay-create --name demoswarmbay --baymodel demoswarmbaymodel --master-count 1 --node-count 1 +--------------------+--------------------------------------+ | Property | Value | +--------------------+--------------------------------------+ | status | None | | uuid | a2388916-db30-41bf-84eb-df0b65979eaf | | status_reason | None | | created_at | 2016-03-19T17:23:00+00:00 | | updated_at | None | | bay_create_timeout | 0 | | api_address | None | | baymodel_id | 887edbc7-0805-4796-be78-dfcddad8eb03 | | node_count | 1 | | node_addresses | None | | master_count | 1 | | discovery_url | None | | name | demoswarmbay | +--------------------+--------------------------------------+ Check bay status, for now it should be in CREATE_IN_PROGRESS state magnum bay-show demoswarmbay +--------------------+--------------------------------------+ | Property | Value | +--------------------+--------------------------------------+ | status | CREATE_IN_PROGRESS | | uuid | a2388916-db30-41bf-84eb-df0b65979eaf | | status_reason | | | created_at | 2016-03-19T17:23:00+00:00 | | updated_at | 2016-03-19T17:23:01+00:00 | | bay_create_timeout | 0 | | api_address | None | | baymodel_id | 887edbc7-0805-4796-be78-dfcddad8eb03 | | node_count | 1 | | node_addresses | [] | | master_count | 1 | | discovery_url | None | | name | demoswarmbay | +--------------------+--------------------------------------+ If all is going fine, nova should have two new instances(in ACTIVE state), one for the master node and second for the worker. nova list +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+ | ID | Name | Status | Task State | Power State | Networks | +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+ | e38eb88c-bb6b-427d-a2c5-cdfe868796f0 | de-44kx2l4q4wc-0-d6j5svvjxmne-swarm_node-xafkm2jskf5j | ACTIVE | - | Running | demoswarmbay-agf6y3qnjoyw-fixed_network-g37bcmc52akv=10.0.0.4, 192.168.100.16 | | 5acc579d-152a-4656-9eb8-e800b7ab3bcf | demoswarmbay-agf6y3qnjoyw-swarm_master-fllwhrpuabbq | ACTIVE | - | Running | demoswarmbay-agf6y3qnjoyw-fixed_network-g37bcmc52akv=10.0.0.3, 192.168.100.15 | +--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+ You can see how heat stack is going heat stack-list +--------------------------------------+---------------------------+--------------------+---------------------+--------------+ | id | stack_name | stack_status | creation_time | updated_time | +--------------------------------------+---------------------------+--------------------+---------------------+--------------+ | 3a64fa60-4df8-498f-aceb-a0cb8cfc0b18 | demoswarmbay-agf6y3qnjoyw | CREATE_IN_PROGRESS | 2016-03-19T17:22:59 | None | +--------------------------------------+---------------------------+--------------------+---------------------+--------------+ We can see what tasks are executing during stack creation heat event-list demoswarmbay-agf6y3qnjoyw +-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+ | resource_name | id | resource_status_reason | resource_status | event_time | +-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+ | demoswarmbay-agf6y3qnjoyw | 004c9388-b8ab-4541-ada8-99b65203e41d | Stack CREATE started | CREATE_IN_PROGRESS | 2016-03-19T17:23:01 | | master_wait_handle | d6f0798a-bfde-4bad-9c73-e108bd101009 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:01 | | secgroup_manager | e2e0eb08-aeeb-4290-9ad5-bd20fe243f07 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:02 | | disable_selinux | d7290592-ab81-4d7a-b2fa-902975904a25 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 | | agent_wait_handle | 65ec5553-56a4-4416-9748-bfa0ae35737a | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 | | add_proxy | 46bdcff8-4606-406f-8c99-7f48adc4de57 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 | | write_docker_socket | ab5402ea-44af-4433-84aa-a63256817a9a | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 | | make_cert | 3b9817a5-606f-41ab-8799-b411c017f05d | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 | | cfn_signal | 0add665a-3fdf-4408-ab15-76332aa326fe | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 | | remove_docker_key | 94f4106e-f139-4d9f-9974-8821d04be103 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 | | configure_swarm | f7e0ebd5-1893-43d1-bd29-81a7e39de0c0 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 | | extrouter | a94a8f68-c237-4dbc-9513-cdbe3de1465e | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 | | enable_services | c250f532-99bd-43d7-9d15-b2d3ae16567a | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 | | write_docker_service | 2c9d8954-4446-4578-a871-0910e8996571 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 | | cloud_init_wait_handle | 6cc51d2d-56e9-458b-a21b-bc553e0c8291 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 | | fixed_network | 3125395f-c689-4481-bf01-94bb2f701993 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:07 | | agent_wait_handle | 2db801e8-c2b5-47b0-ac16-122dba3a22d6 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:08 | | remove_docker_key | 75e2c7a6-a2ce-4026-aeeb-739c4a522f48 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:08 | | secgroup_manager | ac51a029-26c1-495a-bc13-232cfb8c1060 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:08 | | write_docker_socket | 58e08b52-a12a-43e9-b41d-071750294024 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:08 | | master_wait_handle | 3e741b76-6470-47d4-b13e-3f8f446be53c | state changed | CREATE_COMPLETE | 2016-03-19T17:23:08 | | cfn_signal | 96c26b4f-1e99-478e-a8e5-9dcc4486e1b3 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:08 | | enable_services | beedc358-ee72-4b34-a6b9-1b47ffc15306 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:08 | | add_proxy | caae3a07-d5f1-4eb0-8a82-02ea634f77ae | state changed | CREATE_COMPLETE | 2016-03-19T17:23:08 | | make_cert | 79363643-e5e4-4d1b-ad8a-5a56e1f6a8e7 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:08 | | cloud_init_wait_handle | 0457b008-6da8-44fd-abef-cb99bd4d0518 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:09 | | configure_swarm | baf1e089-c627-4b24-a571-63b3c9c14e28 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:09 | | extrouter | 184614d9-2280-4cb4-9253-f538463dbdf4 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:09 | | write_docker_service | 80e66b4e-d40a-4243-bb27-0d2a6b68651f | state changed | CREATE_COMPLETE | 2016-03-19T17:23:09 | | disable_selinux | d8a64822-2571-4dcf-9da5-b3ec73e771eb | state changed | CREATE_COMPLETE | 2016-03-19T17:23:09 | | fixed_network | 528b0ced-23f6-4c22-8cbc-357ba0ee5bc5 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:09 | | write_swarm_manager_failure_service | 9fa100a3-b4a9-465c-8b33-dd000cb4866a | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 | | write_swarm_agent_failure_service | a7c09833-929e-4711-a3e9-39923d23b2f2 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 | | fixed_subnet | 23d8b0a6-a7a3-4f71-9c18-ba6255cf071a | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 | | write_swarm_master_service | d24a6099-3cad-41ce-8d4b-a7ad0661aaea | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:11 | | fixed_subnet | 1a2b7397-1d09-4544-bb9f-985c2f64cb09 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:13 | | write_swarm_manager_failure_service | 615a2a7a-5266-487b-bbe1-fcaa82f43243 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:13 | | write_swarm_agent_failure_service | 3f8c54b4-6644-49a0-ad98-9bc6b4332a07 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:13 | | write_swarm_master_service | 2f58b3c8-d1cc-4590-a328-0e775e495bcf | state changed | CREATE_COMPLETE | 2016-03-19T17:23:13 | | extrouter_inside | f3da7f2f-643e-4f29-a00f-d2595d7faeaf | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:14 | | swarm_master_eth0 | 1d6a510d-520c-4796-8990-aa8f7dd59757 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:16 | | swarm_master_eth0 | 3fd85913-7399-49be-bb46-5085ff953611 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:19 | | extrouter_inside | 33749e30-cbea-4093-b36a-94967e299002 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:19 | | write_heat_params | 054e0af5-e3e0-4bc0-92b5-b40aeedc39ab | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:19 | | swarm_nodes | df7af58c-8148-4b51-bd65-b0734d9051b5 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:20 | | write_swarm_agent_service | ab1e8b1e-2837-4693-b791-e1311f85fa63 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:21 | | swarm_master_floating | d99ffe66-cb02-4279-99dc-a1f3e2ca817c | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:22 | | write_heat_params | 33d9999f-6c93-453d-8565-ac99db021f8f | state changed | CREATE_COMPLETE | 2016-03-19T17:23:25 | | write_swarm_agent_service | 02a1b7f6-2660-4345-ad08-42b66ffaaad5 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:25 | | swarm_master_floating | 8ce6ecd8-c421-4e4a-ab81-cba4b5ccedf4 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:25 | | swarm_master_init | 3787dcc8-e644-412b-859b-63a434b9ee6c | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:26 | | swarm_master_init | a1dd67bb-49c7-4507-8af0-7758b76b57e1 | state changed | CREATE_COMPLETE | 2016-03-19T17:23:28 | | swarm_master | d12b915e-3087-4e17-9954-8233926b504b | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:29 | | swarm_master | a34ad52a-def7-460b-b5b7-410000207b3e | state changed | CREATE_COMPLETE | 2016-03-19T17:23:48 | | master_wait_condition | 0c9331a4-8ad0-46e0-bf2a-35943021a1a3 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 | | cloud_init_wait_condition | de3707a0-f46a-44a9-b4b8-ff50e12cc77f | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 | | agent_wait_condition | a1a810a4-9c19-4983-aaa8-e03f308c1e39 | state changed | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 | +-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+ Once all tasks are completed, we can create containers in the bay we created in previous steps. magnum container-create --name demo-container \ --image docker.io/cirros:latest \ --bay demoswarmbay \ --command &quot;ping -c 4 192.168.100.2&quot; +------------+----------------------------------------+ | Property | Value | +------------+----------------------------------------+ | uuid | 36595858-8657-d465-3e5a-dfcddad8a238 | | links | ... | | bay_uuid | a2388916-db30-41bf-84eb-df0b65979eaf | | updated_at | None | | image | cirros | | command | ping -c 4 192.168.100.2 | | created_at | 2016-03-19T17:30:00+00:00 | | name | demo-container | +------------+----------------------------------------+ Container is created, but not started. Start the container magnum container-start demo-container Check container logs, you should see 4 pings succeed to our external router gateway. magnum container-logs demo-container PING 192.168.100.2 (192.168.100.2) 56(84) bytes of data. 64 bytes from 192.168.100.2: icmp_seq=1 ttl=64 time=0.083 ms 64 bytes from 192.168.100.2: icmp_seq=2 ttl=64 time=0.068 ms 64 bytes from 192.168.100.2: icmp_seq=3 ttl=64 time=0.043 ms 64 bytes from 192.168.100.2: icmp_seq=4 ttl=64 time=0.099 ms You can delete the container magnum container-delete demo-container While doing this demo, i missed adding branch name while cloning Magnum source code, when i installed Magnum all package dependencies where installed from master, who was Mitaka instead of Liberty, which broke my environment. I suffered the following issues: Issues with packages ImportError: No module named MySQLdb Was solved installing MySQL-python from pip instead of yum pip install MySQL-python Issues with policies, admin privileges weren’t recognized by Magnum api.</summary></entry></feed>