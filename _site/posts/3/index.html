<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>OpenStack things</title>
  <meta name="description" content="OpenStack, Docker, Ansible, Ceph, Linux">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://217.182.136.201:8080/posts/3/">
  
  
  <link rel="alternate" type="application/rss+xml" title="OpenStack things" href="http://217.182.136.201:8080/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="egongu90">
  <meta name="twitter:title" content="OpenStack things">
  <meta name="twitter:description" content="OpenStack, Docker, Ansible, Ceph, Linux">
  
    <meta name="twitter:creator" content="egongu90">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">OpenStack things</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="https://github.com/yous/whiteglass">GitHub</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">

  

  

  <ul class="post-list">
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/ceph-ansible-baremetal-deployment/">Ceph-ansible baremetal deployment</a>
          </h1>

          <p class="post-meta">Mar 17, 2016 • 
  
  
    
      <a href="/categories/linux/">Linux</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
      <a href="/categories/various/">Various</a>
    
  
    
  
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p>How many times you tried to install Ceph? How many fails with no reason?
All Ceph operator should agree with me when i say that Ceph installer doesn’t really works as expected so far.
Yes, i’m talking about ceph-deploy and the main reason why i’m posting this guide about deploying Ceph with Ansible.</p>

<p>At this post, i will show how to install a Ceph cluster with Ansible on baremetal servers.
My configuration is as follows:</p>
<ol>
<li type="disc">3 x ceph monitors 8GB of RAM each one</li>
<li type="disc">3 x OSD nodes 16GB of RAM and 3x100 GB of Disk</li>
<li type="disc">1 x RadosGateway node 8GB of RAM</li></ol>

<p>First, download Ceph-Ansible playbooks</p>
<pre>
git clone https://github.com/ceph/ceph-ansible/
Cloning into 'ceph-ansible'...
remote: Counting objects: 5764, done.
remote: Compressing objects: 100% (38/38), done.
remote: Total 5764 (delta 7), reused 0 (delta 0), pack-reused 5726
Receiving objects: 100% (5764/5764), 1.12 MiB | 1.06 MiB/s, done.
Resolving deltas: 100% (3465/3465), done.
Checking connectivity... done.
</pre>
<p>Move to the newly created folder called ceph-ansible</p>
<pre>
cd ceph-ansible/
</pre>
<p>Copy sample vars files, we will configure our environment in these variable files.</p>
<pre>
cp site.yml.sample site.yml
cp group_vars/all.sample group_vars/all
cp group_vars/mons.sample group_vars/mons
cp group_vars/osds.sample group_vars/osds
</pre>
<p>Next step is configure the inventory with our servers, i don’t really like use /etc/ansible/host file, i prefer create a new file per environment inside playbook’s folder.</p>

<p>Create a file with the following content, use you own IPs to match your servers on the desired role inside the cluster</p>
<pre>
[root@ansible ~]# vi inventory_hosts

[mons]
192.168.1.48
192.168.1.49
192.168.1.52

[osds]
192.168.1.50
192.168.1.53
192.168.1.54

[rgws]
192.168.1.55

</pre>
<p>Test connectivity to you servers pinging them through Ansible ping module</p>
<pre>
[root@ansible ~]# ansible -m ping -i inventory_hosts all
192.168.1.48 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.50 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.55 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.53 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.49 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.54 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}

192.168.1.52 | success &gt;&gt; {
    "changed": false,
    "ping": "pong"
}
</pre>
<p>Edit site.yml file, i will remove/comment mds nodes since i’m not going to use them.</p>
<pre>
[root@ansible ~]# vi site.yml

- hosts: mons
  become: True
  roles:
  - ceph-mon

- hosts: agents
  become: True
  roles:
  - ceph-agent

- hosts: osds
  become: True
  roles:
  - ceph-osd

#- hosts: mdss
#  become: True
#  roles:
#  - ceph-mds

- hosts: rgws
  become: True
  roles:
  - ceph-rgw

- hosts: restapis
  become: True
  roles:
  - ceph-restapi

</pre>
<p>Edit main variable file, here we are going to configure our environment</p>
<pre>
[root@ansible ~]# vi group_vars/all
</pre>
<p>Here we configure from where ceph packages are going to be installed, for now we use upstream code with the stable release Infernalis.</p>
<pre>
## Configure package origin
ceph_origin: upstream
ceph_stable: true
ceph_stable_release: infernalis
</pre>
<p>Configure interface on which monitor will be listening</p>
<pre>
## Monitor options
monitor_interface: eth2
</pre>
<p>Here we configure some OSD options, like journal size and what networks will be used by public and cluster data replication</p>
<pre>
## OSD options
journal_size: 1024
public_network: 192.168.1.0/24
cluster_network: 192.168.200.0/24
</pre>
<p>Edit osds variable file</p>
<pre>
[root@ansible ~]# vi group_vars/osds
</pre>
<p>I will use auto discovery option to allow ceph ansible select empy or not used devices in my servers to create OSDs.</p>
<pre>
# Declare devices
osd_auto_discovery: True
journal_collocation: True
</pre>
<p>Of course you can use other options, i’ll highly suggest you to read variable comments, as they provide valuable information about usage.
We’re ready to deploy  ceph with ansible with our custom inventory_hosts file.</p>
<pre>
[root@ansible ~]# ansible-playbook site.yml -i inventory_hosts
</pre>

<p>After a while, you will have a fully functional ceph cluster.</p>

<p>Maybe you find some issues or bugs when running the playbooks. 
There is a lot of efforts to fix issues on upstream repository. If a new bug is encountered, please, post a issue right here.
https://github.com/ceph/ceph-ansible/issues</p>

<p>You can check your cluster status with ceph -s. we can see all OSDs are up and pgs active/clean.</p>
<pre>
[root@ceph-mon1 ~]# ceph -s
    cluster 5ff692ab-2150-41a4-8b6d-001a4da21c9c
     health HEALTH_OK
     monmap e1: 3 mons at {ceph-mon1=192.168.200.141:6789/0,ceph-mon2=192.168.200.180:6789/0,ceph-mon3=192.168.200.232:6789/0}
            election epoch 6, quorum 0,1,2 ceph-mon1,ceph-mon2,ceph-mon3
     osdmap e10: 9 osds: 9 up, 9 in
            flags sortbitwise
      pgmap v32: 64 pgs, 1 pools, 0 bytes data, 0 objects
            102256 kB used, 896 GB / 896 GB avail
                  64 active+clean
</pre>
<p>We are going to do some tests.
Create a pool</p>
<pre>
[root@ceph-mon1 ~]# ceph osd pool create test 128 128
pool 'test' created
</pre>
<p>Create a file big file</p>
<pre>
[root@ceph-mon1 ~]# dd if=/dev/zero of=/tmp/sample.txt bs=2M count=1000
1000+0 records in
1000+0 records out
2097152000 bytes (2.1 GB) copied, 16.7386 s, 125 MB/s
</pre>
<p>Upload the file to rados</p>
<pre>
[root@ceph-mon1 ~]# rados -p test put sample /tmp/sample.txt 
</pre>
<p>Check om which placement groups your file is saved</p>
<pre>
[root@ceph-mon1 ~]# ceph osd map test sample
osdmap e13 pool 'test' (1) object 'sample' -&gt; pg 1.bddbf0b9 (1.39) -&gt; up ([1,0], p1) acting ([1,0], p1)
</pre>
<p>Query the placement group where you file was uploaded, a similar output will prompts</p>
<pre>
[root@ceph-mon1 ~]# ceph pg 1.39 query
{
    "state": "active+clean",
    "snap_trimq": "[]",
    "epoch": 13,
    "up": [
        1,
        0
    ],
    "acting": [
        1,
        0
    ],
    "actingbackfill": [
        "0",
        "1"
    ],
    "info": {
        "pgid": "1.39",
        "last_update": "13'500",
        "last_complete": "13'500",
        "log_tail": "0'0",
        "last_user_version": 500,
        "last_backfill": "MAX",
        "last_backfill_bitwise": 0,
        "purged_snaps": "[]",
        "history": {
            "epoch_created": 11,
            "last_epoch_started": 12,
            "last_epoch_clean": 13,
            "last_epoch_split": 0,
            "last_epoch_marked_full": 0,
            "same_up_since": 11,
            "same_interval_since": 11,
            "same_primary_since": 11,
            "last_scrub": "0'0",
            "last_scrub_stamp": "2016-03-16 21:13:08.883121",
            "last_deep_scrub": "0'0",
            "last_deep_scrub_stamp": "2016-03-16 21:13:08.883121",
            "last_clean_scrub_stamp": "0.000000"
        },
        "stats": {
            "version": "13'500",
            "reported_seq": "505",
            "reported_epoch": "13",
            "state": "active+clean",
            "last_fresh": "2016-03-16 21:24:40.930724",
            "last_change": "2016-03-16 21:14:09.874086",
            "last_active": "2016-03-16 21:24:40.930724",
            "last_peered": "2016-03-16 21:24:40.930724",
            "last_clean": "2016-03-16 21:24:40.930724",
            "last_became_active": "0.000000",
            "last_became_peered": "0.000000",
            "last_unstale": "2016-03-16 21:24:40.930724",
            "last_undegraded": "2016-03-16 21:24:40.930724",
            "last_fullsized": "2016-03-16 21:24:40.930724",
            "mapping_epoch": 11,
            "log_start": "0'0",
            "ondisk_log_start": "0'0",
            "created": 11,
            "last_epoch_clean": 13,
            "parent": "0.0",
            "parent_split_bits": 0,
            "last_scrub": "0'0",
            "last_scrub_stamp": "2016-03-16 21:13:08.883121",
            "last_deep_scrub": "0'0",
            "last_deep_scrub_stamp": "2016-03-16 21:13:08.883121",
            "last_clean_scrub_stamp": "0.000000",
            "log_size": 500,
            "ondisk_log_size": 500,
            "stats_invalid": "0",
            "stat_sum": {
                "num_bytes": 2097152000,
                "num_objects": 1,
                "num_object_clones": 0,
                "num_object_copies": 2,
                "num_objects_missing_on_primary": 0,
                "num_objects_degraded": 0,
                "num_objects_misplaced": 0,
                "num_objects_unfound": 0,
                "num_objects_dirty": 1,
                "num_whiteouts": 0,
                "num_read": 0,
                "num_read_kb": 0,
                "num_write": 500,
                "num_write_kb": 2048000,
                "num_scrub_errors": 0,
                "num_shallow_scrub_errors": 0,
                "num_deep_scrub_errors": 0,
                "num_objects_recovered": 0,
                "num_bytes_recovered": 0,
                "num_keys_recovered": 0,
                "num_objects_omap": 0,
                "num_objects_hit_set_archive": 0,
                "num_bytes_hit_set_archive": 0,
                "num_flush": 0,
                "num_flush_kb": 0,
                "num_evict": 0,
                "num_evict_kb": 0,
                "num_promote": 0,
                "num_flush_mode_high": 0,
                "num_flush_mode_low": 0,
                "num_evict_mode_some": 0,
                "num_evict_mode_full": 0
            },
            "up": [
                1,
                0
            ],
            "acting": [
                1,
                0
            ],
            "blocked_by": [],
            "up_primary": 1,
            "acting_primary": 1
        },
        "empty": 0,
        "dne": 0,
        "incomplete": 0,
        "last_epoch_started": 12,
        "hit_set_history": {
            "current_last_update": "0'0",
            "history": []
        }
    },
    "peer_info": [
        {
            "peer": "0",
            "pgid": "1.39",
            "last_update": "13'500",
            "last_complete": "13'500",
            "log_tail": "0'0",
            "last_user_version": 0,
            "last_backfill": "MAX",
            "last_backfill_bitwise": 0,
            "purged_snaps": "[]",
            "history": {
                "epoch_created": 11,
                "last_epoch_started": 12,
                "last_epoch_clean": 13,
                "last_epoch_split": 0,
                "last_epoch_marked_full": 0,
                "same_up_since": 0,
                "same_interval_since": 0,
                "same_primary_since": 0,
                "last_scrub": "0'0",
                "last_scrub_stamp": "2016-03-16 21:13:08.883121",
                "last_deep_scrub": "0'0",
                "last_deep_scrub_stamp": "2016-03-16 21:13:08.883121",
                "last_clean_scrub_stamp": "0.000000"
            },
            "stats": {
                "version": "0'0",
                "reported_seq": "0",
                "reported_epoch": "0",
                "state": "inactive",
                "last_fresh": "0.000000",
                "last_change": "0.000000",
                "last_active": "0.000000",
                "last_peered": "0.000000",
                "last_clean": "0.000000",
                "last_became_active": "0.000000",
                "last_became_peered": "0.000000",
                "last_unstale": "0.000000",
                "last_undegraded": "0.000000",
                "last_fullsized": "0.000000",
                "mapping_epoch": 0,
                "log_start": "0'0",
                "ondisk_log_start": "0'0",
                "created": 0,
                "last_epoch_clean": 0,
                "parent": "0.0",
                "parent_split_bits": 0,
                "last_scrub": "0'0",
                "last_scrub_stamp": "0.000000",
                "last_deep_scrub": "0'0",
                "last_deep_scrub_stamp": "0.000000",
                "last_clean_scrub_stamp": "0.000000",
                "log_size": 0,
                "ondisk_log_size": 0,
                "stats_invalid": "0",
                "stat_sum": {
                    "num_bytes": 0,
                    "num_objects": 0,
                    "num_object_clones": 0,
                    "num_object_copies": 0,
                    "num_objects_missing_on_primary": 0,
                    "num_objects_degraded": 0,
                    "num_objects_misplaced": 0,
                    "num_objects_unfound": 0,
                    "num_objects_dirty": 0,
                    "num_whiteouts": 0,
                    "num_read": 0,
                    "num_read_kb": 0,
                    "num_write": 0,
                    "num_write_kb": 0,
                    "num_scrub_errors": 0,
                    "num_shallow_scrub_errors": 0,
                    "num_deep_scrub_errors": 0,
                    "num_objects_recovered": 0,
                    "num_bytes_recovered": 0,
                    "num_keys_recovered": 0,
                    "num_objects_omap": 0,
                    "num_objects_hit_set_archive": 0,
                    "num_bytes_hit_set_archive": 0,
                    "num_flush": 0,
                    "num_flush_kb": 0,
                    "num_evict": 0,
                    "num_evict_kb": 0,
                    "num_promote": 0,
                    "num_flush_mode_high": 0,
                    "num_flush_mode_low": 0,
                    "num_evict_mode_some": 0,
                    "num_evict_mode_full": 0
                },
                "up": [],
                "acting": [],
                "blocked_by": [],
                "up_primary": -1,
                "acting_primary": -1
            },
            "empty": 0,
            "dne": 0,
            "incomplete": 0,
            "last_epoch_started": 12,
            "hit_set_history": {
                "current_last_update": "0'0",
                "history": []
            }
        }
    ],
    "recovery_state": [
        {
            "name": "Started\/Primary\/Active",
            "enter_time": "2016-03-16 21:13:36.769083",
            "might_have_unfound": [],
            "recovery_progress": {
                "backfill_targets": [],
                "waiting_on_backfill": [],
                "last_backfill_started": "MIN",
                "backfill_info": {
                    "begin": "MIN",
                    "end": "MIN",
                    "objects": []
                },
                "peer_backfill_info": [],
                "backfills_in_flight": [],
                "recovering": [],
                "pg_backend": {
                    "pull_from_peer": [],
                    "pushing": []
                }
            },
            "scrub": {
                "scrubber.epoch_start": "0",
                "scrubber.active": 0,
                "scrubber.waiting_on": 0,
                "scrubber.waiting_on_whom": []
            }
        },
        {
            "name": "Started",
            "enter_time": "2016-03-16 21:13:09.216260"
        }
    ],
    "agent_state": {}
}
</pre>

<p>That’s all for now.
Regards, Eduardo Gonzalez</p>


        </div>
        
          <p class="post-continue">
            <a href="/ceph-ansible-baremetal-deployment/">Read on &rarr;</a>
          </p>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/nova-vnc-flows-under-the-hood/">Nova VNC flows under the hood</a>
          </h1>

          <p class="post-meta">Mar 2, 2016 • 
  
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/openstack/">OpenStack</a>
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p>Most OpenStack deployments has a VNC console implemented with nova-novncproxy. This service gives the final user the ability to log into their instances in a web based method through a browser.</p>

<p>At this post i’m going to show how a vnc console request works under the hood while using the following command or lauching a vnc session through Horizon.</p>
<pre># nova get-vnc-console INSTANCE novnc
</pre>
<p>First of all, a user connects to NOVA and issues a VNC console request for an instance. Nova API needs to validate the user issuing an authentication request to keystone.
The user receives a token with nova’s endpoint URL in the catalog, with that endpoint and the token, the user makes a request against nova calling for a VNC session.</p>
<pre>GET http://192.168.200.208:5000/v2.0 -H "Accept: application/json" -H \
"User-Agent: python-keystoneclient"

GET http://192.168.200.208:8774/v2/ -H "User-Agent: python-novaclient" -H \
"Accept: application/json" -H "X-Auth-Token: {SHA1}3b6262df9eaba5da33c1004805187806322201f1"
</pre>
<p>If a name instead of an instance ID is used in the request, Nova need to check his database to match that name with his corresponding ID, as we can see in the following request.</p>
<pre>
GET http://192.168.200.208:8774/v2/ee84411cdb8148d28674b129ef482f31/servers?name=test1 \
-H "User-Agent: python-novaclient" -H "Accept: application/json" \
-H "X-Auth-Token: {SHA1}3b6262df9eaba5da33c1004805187806322201f1"

RESP BODY: {"servers": [{"id": "9165dbda-f54e-4186-b2cb-e6ca05ac53ee", \
"links": [{"href": "http://192.168.200.208:8774/v2/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee", "rel": "self"},\
 {"href": "http://192.168.200.208:8774/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee", \
"rel": "bookmark"}], "name": "test1"}]}
</pre>
<p>Once the ID is matched with the name, Nova check information about the instance (I thought it was to validate if is in ACTIVE status, but i realized that even when is in STOPPED status the request is made it anyway).</p>
<pre>
GET http://192.168.200.208:8774/v2/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee\
 -H "User-Agent: python-novaclient" -H "Accept: application/json" \
 -H "X-Auth-Token: {SHA1}3b6262df9eaba5da33c1004805187806322201f1"

RESP BODY: {"server": {"status": "ACTIVE", "updated": "2016-03-02T17:28:45Z", "hostId": "ca3a874dcad9079fcc6a0b10b0e2efaa394bc66b5335197fdd9c2498", "OS-EXT-SRV-ATTR:host": "liberty", "addresses": {"private": [{"OS-EXT-IPS-MAC:mac_addr": "fa:16:3e:aa:1c:32", "version": 4, "addr": "10.0.0.6", "OS-EXT-IPS:type": "fixed"}]}, "links": [{"href": "http://192.168.200.208:8774/v2/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee", "rel": "self"}, {"href": "http://192.168.200.208:8774/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee", "rel": "bookmark"}], "key_name": null, "image": {"id": "bf31eadd-c5f4-40f8-9ddb-30f688ca5e5f", "links": [{"href": "http://192.168.200.208:8774/ee84411cdb8148d28674b129ef482f31/images/bf31eadd-c5f4-40f8-9ddb-30f688ca5e5f", "rel": "bookmark"}]}, "OS-EXT-STS:task_state": null, "OS-EXT-STS:vm_state": "active", "OS-EXT-SRV-ATTR:instance_name": "instance-0000000a", "OS-SRV-USG:launched_at": "2016-03-02T17:28:45.000000", "OS-EXT-SRV-ATTR:hypervisor_hostname": "liberty", "flavor": {"id": "1", "links": [{"href": "http://192.168.200.208:8774/ee84411cdb8148d28674b129ef482f31/flavors/1", "rel": "bookmark"}]}, "id": "9165dbda-f54e-4186-b2cb-e6ca05ac53ee", "security_groups": [{"name": "default"}], "OS-SRV-USG:terminated_at": null, "OS-EXT-AZ:availability_zone": "nova", "user_id": "d9164a323be649c0a8c5c80fdd5bd585", "name": "test1", "created": "2016-03-02T17:28:34Z", "tenant_id": "ee84411cdb8148d28674b129ef482f31", "OS-DCF:diskConfig": "MANUAL", "os-extended-volumes:volumes_attached": [], "accessIPv4": "", "accessIPv6": "", "progress": 0, "OS-EXT-STS:power_state": 1, "config_drive": "", "metadata": {}}}
</pre>
<p>When we get the information, nova-api POST a request to nova-consoleauth for a VNC console.</p>
<pre>
POST http://192.168.200.208:8774/v2/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee/action \
-H "User-Agent: python-novaclient" -H "Content-Type: application/json" \
-H "Accept: application/json" -H "X-Auth-Token: {SHA1}3b6262df9eaba5da33c1004805187806322201f1"\
-d '{"os-getVNCConsole": {"type": "novnc"}}'


DEBUG nova.api.openstack.wsgi [req-2201b9d6-5711-46d3-ac4d-669094f07527 \
d9164a323be649c0a8c5c80fdd5bd585 ee84411cdb8148d28674b129ef482f31 - - -] \
Action: 'action', calling method: , body: {"os-getVNCConsole": {"type": "novnc"}} \
_process_stack /usr/lib/python2.7/site-packages/nova/api/openstack/wsgi.py:789
</pre>
<p>Nova-consoleauth receives the console request and create an access URL while generates a temporary token for the vnc console.</p>
<pre>
INFO nova.consoleauth.manager [req-d4def6f9-1ab9-4626-b6a8-d81643ea5eb4 d9164a323be649c0a8c5c80fdd5bd585 ee84411cdb8148d28674b129ef482f31 - - -] \
Received Token: 3dfcd011-28f1-4cf3-8f5c-8cd18de4560e, \
{'instance_uuid': u'9165dbda-f54e-4186-b2cb-e6ca05ac53ee', \
'access_url': u'http://192.168.200.208:6080/vnc_auto.html?token=3dfcd011-28f1-4cf3-8f5c-8cd18de4560e',\
 'token': u'3dfcd011-28f1-4cf3-8f5c-8cd18de4560e', 'last_activity_at': 1456940028.356214, \
'internal_access_path': None, 'console_type': u'novnc', 'host': u'liberty', 'port': u'5900'}
</pre>
<p>Nova-consoleauth answer to nova-api who also answers to the user with an access URL.
This URL got the following content on it:</p>
<ul>
	<li>HTTP or HTTPS connection to nova-novncproxy IP</li>
	<li>Nova-novncproxy port</li>
	<li>A token to validate the VNC connection</li>
</ul>
<pre>RESP BODY: {"console": {"url": "http://192.168.200.208:6080/vnc_auto.html?token=3dfcd011-28f1-4cf3-8f5c-8cd18de4560e", "type": "novnc"}}

+-------+--------------------------------------------------------------------------------------+
| Type  | Url                                                                                  |
+-------+--------------------------------------------------------------------------------------+
| novnc | http://192.168.200.208:6080/vnc_auto.html?token=3dfcd011-28f1-4cf3-8f5c-8cd18de4560e |
+-------+--------------------------------------------------------------------------------------+
</pre>
<p>Until now, nova-novncproxy service can be stopped or isn’t used at all, is at this point the when proxy server enter into the game.
The user connects through a web browser to the nova-novncproxy’s URL provided by nova before.</p>
<pre>
DEBUG nova.console.websocketproxy [-] 192.168.200.1: \
new handler Process vmsg /usr/lib/python2.7/site-packages/websockify/websocket.py:828
</pre>
<p>Nova-vncproxy validate the issued token with the URL against nova-consoleauth.</p>
<pre>
nova.consoleauth.manager [req-399c7b58-700a-4779-b215-b12d10056813 - - - - -] \
Checking Token: 3dfcd011-28f1-4cf3-8f5c-8cd18de4560e, True
</pre>
<p>When the token is validated, nova-novncproxy maps compute’s node private IP (at this case port 5900) with the nova-novncproxy public IP(6080 port).</p>
<pre>
INFO nova.console.websocketproxy [req-399c7b58-700a-4779-b215-b12d10056813 - - - - -]\
   7: connect info: {u'instance_uuid': u'9165dbda-f54e-4186-b2cb-e6ca05ac53ee', u'\
internal_access_path': None, u'last_activity_at': 1456940028.356214, \
u'console_type': u'novnc', u'host': u'liberty', u'token': u'3dfcd011-28f1-4cf3-8f5c-8cd18de4560e', \
u'access_url': u'http://192.168.200.208:6080/vnc_auto.html?token=3dfcd011-28f1-4cf3-8f5c-8cd18de4560e'\
, u'port': u'5900'}
</pre>
<p>We can see how the python novncproxy process binds both IPs/port.</p>
<pre>
# ps aux | grep vnc
nova     14840  1.2  0.7 362096 41000 ?        S    18:53   0:14 /usr/bin/python2 /usr/bin/nova-novncproxy --web /usr/share/novnc/

# netstat -putona | grep 14840
tcp        0      0 192.168.200.208:6080    192.168.200.1:59918     ESTABLISHED 14840/python2        keepalive (3,13/0/0)
tcp        0      0 192.168.122.73:57764    192.168.122.73:5900     ESTABLISHED 14840/python2        keepalive (3,13/0/0)
</pre>
<p>Nova-novncproxy starts the connection between the instance and user’s browser session.</p>
<pre>
INFO nova.console.websocketproxy [req-399c7b58-700a-4779-b215-b12d10056813 - - - - -]\
   7: connecting to: liberty:5900
</pre>
<p>Libvirt connects a vnc console into the instance, as we can see at the xml provided by virsh command.
Also, port 5900 now is binded at qemu-kvm process.</p>

<pre><code>
# virsh dumpxml 2
...
&lt;graphics type='vnc' port='5900' autoport='yes' listen='0.0.0.0' keymap='en-us'&gt;
     &lt;listen type='address' address='0.0.0.0'/&gt;
   &lt;/graphics&gt;
...
</code></pre>
<pre># netstat -putona | grep 5900
tcp        0      0 0.0.0.0:5900            0.0.0.0:*               LISTEN      5910/qemu-kvm        off (0.00/0/0)
tcp        0      0 192.168.122.73:5900     192.168.122.73:57702    ESTABLISHED 5910/qemu-kvm        off (0.00/0/0)
tcp        0      0 192.168.122.73:57702    192.168.122.73:5900     ESTABLISHED 11118/python2        keepalive (1,92/0/0)
</pre>
<p>Nova-novncproxy keeps the connection alive until browser session ends.</p>
<pre>
DEBUG nova.console.websocketproxy [-] \
Reaing zombies, active child count is 1 vmsg /usr/lib/python2.7/site-packages/websockify/websocket.py:828
</pre>
<p>When a token is not valid while authenticating against nova-consoleauth, we can see a message like the following.</p>
<pre>
INFO nova.console.websocketproxy [req-9164b32d-3ce1-441b-82c7-6c23c9a354d0 - - - - -] \
handler exception: The token '3dfcd011-28f1-4cf3-8f5c-8cd18de4560e' is invalid or has expired
</pre>

<p>Regards.
 Eduardo Gonzalez</p>

        </div>
        
          <p class="post-continue">
            <a href="/nova-vnc-flows-under-the-hood/">Read on &rarr;</a>
          </p>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/ansible-ini_file-module-simplifying-your-devops-life/">Ansible ini_file module, simplifying your DevOps life</a>
          </h1>

          <p class="post-meta">Feb 24, 2016 • 
  
  
    
      <a href="/categories/linux/">Linux</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
      <a href="/categories/various/">Various</a>
    
  
    
  
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <table>
  <tbody>
    <tr>
      <td>If you don’t read docs, one day you’ll realize that your an idiot as i am</td>
      <td>was.</td>
    </tr>
  </tbody>
</table>

<p>A few days back, I’ve realized that i was using wrong all Ansible modules power since i started with it. What happened?</p>

<p>Most of the time i use Ansible is related to OpenStack configuration jobs. Almost, all OpenStack projects use INI formatted files for their configuration files.
When i started using Ansible, I searched on Google how to configure any kind of file with Ansible modules. Almost all blogs/forums that i saw, talked about lineinfile module. So i used these guidelines on my next few months, now i realize that i was using in the wrong way Ansible modules.</p>

<p>Ansible have a module called ini_file, you change values inside INI formatted files in a easy way , you don’t need to use complicated regular expressions to change a value in a file.</p>

<p>Here you have ini_file module usage docs: <a href="http://docs.ansible.com/ansible/ini_file_module.html" target="_blank">http://docs.ansible.com/ansible/ini_file_module.html</a></p>

<p>We are going to change Neutron user password in his dump config file, so we create a simple task on which we can see how ini_file module can be used.</p>
<pre>
- hosts: localhost
  tasks:
  - name: Change neutron user password
    ini_file:
      dest: ~/neutron.conf
      section: keystone_authtoken
      option: password
      value: 12345
</pre>
<p>Once the task has been applied, we can see how the values are applied in a proper ini style.</p>
<pre>
cat neutron.conf
[keystone_authtoken]
password = 12345
</pre>
<p>How many times you need to make a change in an INI formatted configuration file with Ansible and used lineinfile module?
If the answer is many times, it’s OK, you are a dump like me.</p>

<p>Regards, Eduardo Gonzalez</p>

        </div>
        
          <p class="post-continue">
            <a href="/ansible-ini_file-module-simplifying-your-devops-life/">Read on &rarr;</a>
          </p>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/working-with-affinityanti-affinity-groups-openstack/">Working with affinity/anti-affinity groups OpenStack</a>
          </h1>

          <p class="post-meta">Feb 15, 2016 • 
  
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/openstack/">OpenStack</a>
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p>In a previous post, you learned how to segregate resources with <a href="http://egonzalez.org/openstack-segregation-with-availability-zones-and-host-aggregates/" target="_blank">Availability Zones and Host Aggregates</a>, those methods allows the end user to specify where and on which types of resources their instances should be running.</p>

<p>At this post, you will learn how specify to nova where nova-scheduler should schedule your instances based on two policies. These policies define if instances should share the same hypervisor (affinity rule) or if not depending of user needs(anti-affinity rule).</p>

<p>First, you need to modify nova.conf and allow nova-scheduler to filter based on affinity rules. Add <code>ServerGroupAntiAffinityFilter</code> and <code>ServerGroupAffinityFilter</code> filters to scheduler default filter option.</p>
<pre>
# vi /etc/nova.conf

scheduler_default_filters=RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,CoreFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter
</pre>
<p>Restart nova-scheduler to apply changes</p>
<pre>
systemctl restart openstack-nova-scheduler
</pre>
<p>Once nova-scheduler has been restarted, we can create a group of servers based on affinity policy (All instances at this group will be launched in the same hypervisor)</p>
<pre>
nova server-group-create instancestogethergroup affinity
+--------------------------------------+------------------------+---------------+---------+----------+
| Id                                   | Name                   | Policies      | Members | Metadata |
+--------------------------------------+------------------------+---------------+---------+----------+
| 27abe662-c37e-431c-9715-0d2137fc5519 | instancestogethergroup | [u'affinity'] | []      | {}       |
+--------------------------------------+------------------------+---------------+---------+----------+
</pre>
<p>Now create two instances, add <code>--hint group=GROUP-ID</code> option to specify the group where instances will be members.</p>
<pre>
nova boot --image a6d7a606-f725-480a-9b1b-7b3ae39b93d4 --flavor m1.tiny --nic net-id=154da7a8-fa49-415e-9d35-c840b144a8df --hint group=27abe662-c37e-431c-9715-0d2137fc5519 affinity1
nova boot --image a6d7a606-f725-480a-9b1b-7b3ae39b93d4 --flavor m1.tiny --nic net-id=154da7a8-fa49-415e-9d35-c840b144a8df --hint group=27abe662-c37e-431c-9715-0d2137fc5519 affinity2
</pre>
<p>Ensure the instances are properly mapped to the group.</p>
<pre>
nova server-group-get 27abe662-c37e-431c-9715-0d2137fc5519 
+--------------------------------------+------------------------+---------------+------------------------------------------------------------------------------------+----------+
| Id                                   | Name                   | Policies      | Members                                                                            | Metadata |
+--------------------------------------+------------------------+---------------+------------------------------------------------------------------------------------+----------+
| 27abe662-c37e-431c-9715-0d2137fc5519 | instancestogethergroup | [u'affinity'] | [u'b8b72a0a-c981-430e-a909-13d23d928655', u'8affefff-0072-47e3-8d11-2ddf26e48b82'] | {}       |
+--------------------------------------+------------------------+---------------+------------------------------------------------------------------------------------+----------+
</pre>
<p>Once instances are running, ensure they share the same hypervisor as we specify in the affinity policy.</p>
<pre>
# nova show affinity1 | grep hypervisor_hostname
| OS-EXT-SRV-ATTR:hypervisor_hostname  | compute2az
# nova show affinity2 | grep hypervisor_hostname
| OS-EXT-SRV-ATTR:hypervisor_hostname  | compute2az  
</pre>
<p>Now we create an anti-affinity policy based group.</p>
<pre>
nova server-group-create farinstancesgroup anti-affinity
+--------------------------------------+-------------------+--------------------+---------+----------+
| Id                                   | Name              | Policies           | Members | Metadata |
+--------------------------------------+-------------------+--------------------+---------+----------+
| 988a9fd2-3a97-481e-b083-fee36b33009d | farinstancesgroup | [u'anti-affinity'] | []      | {}       |
+--------------------------------------+-------------------+--------------------+---------+----------+
</pre>
<p>Launch two instances and attach them to the anti-affinity group.</p>
<pre>
nova boot --image a6d7a606-f725-480a-9b1b-7b3ae39b93d4 --flavor m1.tiny --nic net-id=154da7a8-fa49-415e-9d35-c840b144a8df --hint group=988a9fd2-3a97-481e-b083-fee36b33009d anti-affinity1
nova boot --image a6d7a606-f725-480a-9b1b-7b3ae39b93d4 --flavor m1.tiny --nic net-id=154da7a8-fa49-415e-9d35-c840b144a8df --hint group=988a9fd2-3a97-481e-b083-fee36b33009d anti-affinity2
</pre>
<p>Ensure the instances are in the anti-affinity group</p>
<pre>
nova server-group-get 988a9fd2-3a97-481e-b083-fee36b33009d 
+--------------------------------------+-------------------+--------------------+------------------------------------------------------------------------------------+----------+
| Id                                   | Name              | Policies           | Members                                                                            | Metadata |
+--------------------------------------+-------------------+--------------------+------------------------------------------------------------------------------------+----------+
| 988a9fd2-3a97-481e-b083-fee36b33009d | farinstancesgroup | [u'anti-affinity'] | [u'cfb45193-9a7c-436f-ac2d-59a7a9a854ae', u'25dc8671-0c9a-4774-90cf-7394380f91ef'] | {}       |
+--------------------------------------+-------------------+--------------------+------------------------------------------------------------------------------------+----------+
</pre>
<p>Once instances are running, ensure they are in different hypervisors as we specify in the anti-affinity policy.</p>
<pre>
# nova show anti-affinity1 | grep hypervisor_hostname
| OS-EXT-SRV-ATTR:hypervisor_hostname  | compute2az
# nova show anti-affinity2 | grep hypervisor_hostname
| OS-EXT-SRV-ATTR:hypervisor_hostname  | compute1az   
</pre>

<p>Regards, Eduardo Gonzalez</p>

        </div>
        
          <p class="post-continue">
            <a href="/working-with-affinityanti-affinity-groups-openstack/">Read on &rarr;</a>
          </p>
        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/migrate-from-keystone-v2-0-to-keystone-v3-openstack-liberty/">Migrate from keystone v2.0 to keystone v3 OpenStack Liberty</a>
          </h1>

          <p class="post-meta">Feb 2, 2016 • 
  
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/openstack/">OpenStack</a>
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p>Migrate from keystone v2.0 to v3 isn’t as easy like just changing the endpoints at the database, every service must be configured to authenticate against keystone v3.</p>

<p>I’ve been working on that the past few days looking for a method, with the purpose of facilitate operators life’s who need this kind of migration.
I have to thank Adam Young work, i followed his blog to make a first configuration idea, after that, i configured all core services to make use of keystone v3.
If you want to check Adam’s blog, follow this link: <a href="http://adam.younglogic.com/2015/05/rdo-v3-only/" target="_blank">http://adam.younglogic.com/2015/05/rdo-v3-only/</a></p>

<p>I used OpenStack Liberty installed with RDO packstack over CentOS 7 servers.
The example IP used is <code>192.168.200.168</code>, use your own according your needs.
Password used for all services is <code>PASSWD1234</code>, use your own password, you can locate your passwords at the packstack answer file.</p>

<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Horizon</strong></ins></p>

<p>First we configure Horizon with keystone v3 as below:</p>
<pre>
vi /etc/openstack-dashboard/local_settings

OPENSTACK_API_VERSIONS = {
    "identity": 3
}

OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'Default'
</pre>

<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>keystone</strong></ins></p>

<p>Check your current identity endpoints</p>
<pre>
mysql  --user keystone_admin --password=PASSWD1234  keystone -e "select interface, url from endpoint where service_id =  (select id from service where service.type = 'identity');"
</pre>
<p>Change your public, admin and internal endpoints with v3 at the end, instead of v2.0</p>
<pre>
mysql  --user keystone_admin --password=PASSWD1234   keystone -e "update endpoint set   url  = 'http://192.168.200.178:5000/v3' where  interface ='internal' and  service_id =  (select id from service where service.type = 'identity');"

mysql  --user keystone_admin --password=PASSWD1234   keystone -e "update endpoint set   url  = 'http://192.168.200.178:5000/v3' where  interface ='public' and  service_id =  (select id from service where service.type = 'identity');"

mysql  --user keystone_admin --password=PASSWD1234   keystone -e "update endpoint set   url  = 'http://192.168.200.178:35357/v3' where  interface ='admin' and  service_id =  (select id from service where service.type = 'identity');"
</pre>
<p>Ensure the endpoints are properly created</p>
<pre>
mysql  --user keystone_admin --password=KEYSTONE_DB_PW   keystone -e "select interface, url from endpoint where service_id =  (select id from service where service.type = 'identity');"
</pre>
<p>Create a source file or edit keystonerc_admin with the following data</p>
<pre>
vi v3_keystone

unset OS_SERVICE_TOKEN
export OS_USERNAME=admin
export OS_PASSWORD=PASSWD1234
export OS_AUTH_URL=http://192.168.200.178:5000/v3
export OS_PROJECT_NAME=admin
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USER_DOMAIN_NAME=Default
export OS_REGION_NAME=RegionOne
export PS1='[\u@\h \W(keystone_admin)]\$ '
export OS_IDENTITY_API_VERSION=3
</pre>
<p>Comment both pipelines, in public_api and admin_api</p>
<pre>
vi /usr/share/keystone/keystone-dist-paste.ini

[pipeline:public_api]
# The last item in this pipeline must be public_service or an equivalent
# application. It cannot be a filter.
#pipeline = sizelimit url_normalize request_id build_auth_context token_auth admin_token_auth json_body ec2_extension user_crud_extension public_service

[pipeline:admin_api]
# The last item in this pipeline must be admin_service or an equivalent
# application. It cannot be a filter.
#pipeline = sizelimit url_normalize request_id build_auth_context token_auth admin_token_auth json_body ec2_extension s3_extension crud_extension admin_service
</pre>
<p>Comment v2.0 entries in composite:main and admin sections.</p>
<pre>
[composite:main]
use = egg:Paste#urlmap
#/v2.0 = public_api
/v3 = api_v3
/ = public_version_api

[composite:admin]
use = egg:Paste#urlmap
#/v2.0 = admin_api
/v3 = api_v3
/ = admin_version_api
</pre>
<p>Restart httpd to apply changes</p>
<pre>
systemctl restart httpd
</pre>
<p>Check whether keystone and horizon are properly working
The command below should prompt an user list, if not, check configuration in previous steps</p>
<pre>
openstack user list
</pre>

<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Glance</strong></ins></p>

<p>Edit the following files, with the content below:</p>
<pre>
vi /etc/glance/glance-api.conf 
vi /etc/glance/glance-registry.conf 
vi /etc/glance/glance-cache.conf 

[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = glance
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre>
<p>Comment the following lines:</p>
<pre>
#auth_host=127.0.0.1
#auth_port=35357
#auth_protocol=http
#identity_uri=http://192.168.200.178:35357
#admin_user=glance
#admin_password=PASSWD1234
#admin_tenant_name=services
</pre>
<p>Those lines, should be commented in all the other OpenStack core services at keystone_authtoken section</p>

<p>Edit the files below and comment the lines inside keystone_authtoken section.</p>
<pre>
vi /usr/share/glance/glance-api-dist.conf 
vi /usr/share/glance/glance-registry-dist.conf 

[keystone_authtoken]
#admin_tenant_name = %SERVICE_TENANT_NAME%
#admin_user = %SERVICE_USER%
#admin_password = %SERVICE_PASSWORD%
#auth_host = 127.0.0.1
#auth_port = 35357
#auth_protocol = http
</pre>
<p>Restart glance services</p>
<pre>
openstack-service restart glance
</pre>
<p>Ensure glance service is working</p>
<pre>
openstack image list
</pre>

<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Nova</strong></ins></p>

<p>Edit the file below and comment the lines inside keystone_authtoken</p>
<pre>
vi /usr/share/nova/nova-dist.conf

[keystone_authtoken]
#auth_host = 127.0.0.1
#auth_port = 35357
#auth_protocol = http
</pre>
<p>Edit nova.conf and add the auth content inside keystone_authtoken, don’t forget to comment the lines related to the last auth method, which were commented in glance section.</p>
<pre>
vi /etc/nova/nova.conf

[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = nova
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre>
<p>Configure nova authentication against neutron</p>
<pre>
[neutron]
          
auth_plugin = password
auth_url = http://192.168.200.178:35357
username = neutron
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre>
<p>Restart nova services to apply changes</p>
<pre>
openstack-service restart nova
</pre>
<p>Check if nova works</p>
<pre>
openstack hypervisor list
</pre>

<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Neutron</strong></ins></p>

<p>Comment or remove the following entries at api-paste.ini and add the new version auth lines</p>
<pre>
vi /etc/neutron/api-paste.ini 

[filter:authtoken]
#identity_uri=http://192.168.200.178:35357
#admin_user=neutron
#admin_password=PASSWD1234
#auth_uri=http://192.168.200.178:5000/v2.0
#admin_tenant_name=services

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = neutron
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre>
<p>Configure v3 authentication for metadata service, remember comment the old auth lines</p>
<pre>
vi /etc/neutron/metadata_agent.ini

[DEFAULT]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = neutron
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre>
<p>Configure neutron server with v3 auth</p>
<pre>
vi /etc/neutron/neutron.conf

nova_admin_auth_url = http://192.168.200.178:5000
# nova_admin_tenant_id =1fb93c84c6474c5ea92c0ed5f7d4a6a7
nova_admin_tenant_name = services


[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = neutron
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000

#auth_uri = http://192.168.200.178:5000/v2.0
#identity_uri = http://192.168.200.178:35357
#admin_tenant_name = services
#admin_user = neutron
#admin_password = PASSWD1234
</pre>
<p>Configure neutron auth against nova services</p>
<pre>
[nova]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = nova
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre>
<p>Restart neutron services to apply changes</p>
<pre>
openstack-service restart neutron
</pre>
<p>Test correct neutron funtionality</p>
<pre>
openstack network list
</pre>

<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Cinder</strong></ins></p>

<p>Edit api-paste.ini with the following content</p>
<pre>
vi /etc/cinder/api-paste.ini 

[filter:authtoken]
paste.filter_factory = keystonemiddleware.auth_token:filter_factory
auth_plugin = password
auth_url = http://192.168.200.178:35357
username = cinder
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
#admin_tenant_name=services
#auth_uri=http://192.168.200.178:5000/v2.0
#admin_user=cinder
#identity_uri=http://192.168.200.178:35357
#admin_password=PASSWD1234
</pre>
<p>Restart cinder services to apply changes</p>
<pre>
openstack-service restart cinder
</pre>
<p>Ensure cinder is properly running</p>
<pre>
openstack volume create --size 1 testvolume
openstack volume list
</pre>
<p>Now, you can check if nova is working fine, create an instance and ensure it is in ACTIVE state.</p>
<pre>
openstack server create --flavor m1.tiny --image cirros --nic net-id=a1aa6336-9ae2-4ffb-99f5-1b6d1130989c testinstance
openstack server list
</pre>
<p>If any error occurs, review configuration files</p>

<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Swift</strong></ins></p>

<p>Configure proxy server auth agains keystone v3</p>
<pre>
vi /etc/swift/proxy-server.conf

[filter:authtoken]
log_name = swift
signing_dir = /var/cache/swift
paste.filter_factory = keystonemiddleware.auth_token:filter_factory
auth_plugin = password
auth_url = http://192.168.200.178:35357
username = swift
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000

#auth_uri = http://192.168.200.178:5000/v2.0
#identity_uri = http://192.168.200.178:35357
#admin_tenant_name = services
#admin_user = swift
#admin_password = PASSWD1234
delay_auth_decision = 1
cache = swift.cache
include_service_catalog = False
</pre>
<p>Restart swift services to apply changes</p>
<pre>
openstack-service restart swift
</pre>
<p>Swift commands must be issued with python-openstackclient instead of swiftclient
If done with swiftclient a -V 3 option must be used in order to avoid issues</p>

<p>Check if swift works fine</p>
<pre>
openstack container create testcontainer
</pre>

<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Ceilometer</strong></ins></p>

<p>Configure ceilometer service in order to authenticate agains keystone v3</p>
<pre>
[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = ceilometer
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000

[service_credentials]

os_auth_url = http://controller:5000/v3
os_username = ceilometer
os_tenant_name = services
os_password = PASSWD1234
os_endpoint_type = internalURL
os_region_name = RegionOne
</pre>
<p>Restart ceilometer services</p>
<pre>
openstack-service restart ceilometer
</pre>
<p>Check ceilometer funtionality</p>
<pre>
ceilometer statistics -m memory
</pre>
<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Heat</strong></ins></p>

<p>Configure Heat authentication, since trusts are not stable use password auth method</p>
<pre>
vi /etc/heat/heat.conf

# Allowed values: password, trusts
#deferred_auth_method = trusts
deferred_auth_method = password
</pre>
<p>Configure auth_uri and keystone_authtoken section</p>
<pre>
# From heat.common.config
#
# Unversioned keystone url in format like http://0.0.0.0:5000. (string value)
#auth_uri =
auth_uri = http://192.168.200.178:5000

[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = heat
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000

#admin_user=heat
#admin_password=PASSWD1234
#admin_tenant_name=services
#identity_uri=http://192.168.200.178:35357
#auth_uri=http://192.168.200.178:5000/v2.0
</pre>
<p>Comment or remove heat-dist auth entries in order to avoid conflicts with your config files</p>
<pre>
vi /usr/share/heat/heat-dist.conf 

[keystone_authtoken]
#auth_host = 127.0.0.1
#auth_port = 35357
#auth_protocol = http
#auth_uri = http://127.0.0.1:5000/v2.0
#signing_dir = /tmp/keystone-signing-heat
</pre>
<p>Restart heat services to apply changes</p>
<pre>
openstack-service restart heat
</pre>
<p>Ensure heat authentication is properly configured with a simple heat template</p>
<pre>
heat stack-create --template-file sample.yaml teststack
</pre>
<p>Most issues occurs in the authentication between nova and neutron services, if instances does not launch as expected, review [nova] and [neutron] sections.</p>

<p>Best regards, Eduardo Gonzalez</p>

        </div>
        
          <p class="post-continue">
            <a href="/migrate-from-keystone-v2-0-to-keystone-v3-openstack-liberty/">Read on &rarr;</a>
          </p>
        
      </li>
    
  </ul>

  
  <div class="pagination">
    
      <a class="previous" href="/posts/4/">&laquo; Older</a>
    

    
      <a class="next" href="/posts/2/">Newer &raquo;</a>
    
  </div>



</div>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy;  - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://217.182.136.201:8080/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
