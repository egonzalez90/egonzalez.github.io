<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenStack Stuff | articles in the "Linux, OpenStack, Various" category</title>
    <link rel="shortcut icon" type="image/png" href="https://egonzalez90.github.io/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="https://egonzalez90.github.io/favicon.ico">
    <link href="https://egonzalez90.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="OpenStack Stuff Full Atom Feed" />
    <link href="https://egonzalez90.github.io/feeds/linux-openstack-various.atom.xml" type="application/atom+xml" rel="alternate" title="OpenStack Stuff Categories Atom Feed" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Eduardo Gonzalez" />
</head>
<body>
    <header>
        <nav>
            <ul>

                <li class="ephemeral selected"><a href="https://egonzalez90.github.io/category/linux-openstack-various.html">Linux, OpenStack, Various</a></li>
                <li><a href="https://egonzalez90.github.io/">Home</a></li>
                <li><a href="https://docs.openstack.org">OpenStack</a></li>
                <li><a href="https://www.linkedin.com/in/eduardogonzalezgutierrez">Linkedin</a></li>
                <li><a href="https://github.com/egonzalez90">Github</a></li>
                <li><a href="https://egonzalez90.github.io/archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="https://egonzalez90.github.io/">OpenStack Stuff</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Jan 31, 2017</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/openstack-keystone-zero-downtime-upgrade-process-n-to-o.html" rel="bookmark" title="Permanent Link to &quot;OpenStack Keystone Zero-Downtime upgrade process (N to O)&quot;">OpenStack Keystone Zero-Downtime upgrade process (N to O)</a>
                </h2>

                
                

                <p>This blog post will show Keystone upgrade procedure from OpenStack
Newton to Ocata release with zero-downtime.</p>
<p>In the case of doing this in production, please read release notes,
ensure a proper configuration, do database backups and test the upgrade
a thousand times.</p>
<p>Keystone upgrade will need to stop one node in order to use it as
upgrade server.<br>
In the case of a PoC this is not an issue, but in a production
environment, Keystone loads may be intensive and stopping a node for a
while may decrease other nodes performance more than expected.<br>
For this reason I prefer orchestrate the upgrade from an external
Docker container. With this method all nodes will be fully running
almost all the time.</p>
<ul>
<li>New container won't start any service, just will sync the database
    schema with new Keystone version avoiding stop a node to orchestrate
    the upgrade.</li>
<li>The Docker image is provided by OpenStack Kolla project, if already
    using Kolla this upgrade won't be needed as kolla-ansible already
    provide an upgrade method.</li>
<li>At the moment of writing of this blog, Ocata packages were not
    released into stable repositories. For this reason I use DLRN
    repositories.</li>
<li>If Ocata is released please do not use DLRN, use stable packages
    instead.</li>
<li>Use stable Ocata Docker image if available with tag 4.0.x and will
    avoid repository configuration and package upgrades.</li>
<li>NOTE: Upgrade may need more steps depending of your own
    configuration, i.e, if using fernet token more steps are necessary
    during the upgrade.</li>
<li>All Keystone nodes are behind HAproxy.</li>
</ul>
<h3>Prepare the upgrade</h3>
<p>Start Keystone Docker container with host networking (needed to
communicate with database nodes directly) and root user (needed to
install packages).</p>
<div class="highlight"><pre><span></span>(host)# docker run -ti --net host -u 0 kolla/centos-binary-keystone:3.0.2 bash
</pre></div>


<p>Download Delorean CentOS trunk repositories</p>
<div class="highlight"><pre><span></span>(keystone-upgrade)# curl -Lo /etc/yum.repos.d/delorean.repo http://buildlogs.centos.org/centos/7/cloud/x86_64/rdo-trunk-master-tested/delorean.repo
(keystone-upgrade)# curl -Lo /etc/yum.repos.d/delorean-deps.repo http://trunk.rdoproject.org/centos7/delorean-deps.repo
</pre></div>


<p>Disable Newton repository</p>
<div class="highlight"><pre><span></span>(keystone-upgrade)# yum-config-manager --disable centos-openstack-newton
</pre></div>


<p>Ensure Newton repository is not longer used by the system</p>
<div class="highlight"><pre><span></span>(keystone-upgrade)# yum repolist | grep -i openstack
delorean                        delorean-openstack-glance-0bf9d805886c2  565+255
</pre></div>


<p>Update all packages in the Docker container to bump keystone version to
Ocata.</p>
<div class="highlight"><pre><span></span>(keystone-upgrade)# yum clean all &amp;&amp; yum update -y
</pre></div>


<p>Configure keystone.conf file, this are my settings. Review you
configuration and ensure all is correctly, otherwise may cause issues in
the database.<br>
An important option is default_domain_id, this value is for backward
compatible with users created under default domain.</p>
<div class="highlight"><pre><span></span><span class="p">(</span><span class="n">keystone</span><span class="o">-</span><span class="n">upgrade</span><span class="p">)</span><span class="err">#</span> <span class="n">egrep</span> <span class="o">^</span><span class="p">[</span><span class="o">^</span><span class="err">#</span><span class="p">]</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">keystone</span><span class="o">/</span><span class="n">keystone</span><span class="p">.</span><span class="n">conf</span> 
<span class="p">[</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="n">debug</span> <span class="o">=</span> <span class="n">False</span>
<span class="n">log_file</span> <span class="o">=</span> <span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">keystone</span><span class="o">/</span><span class="n">keystone</span><span class="p">.</span><span class="n">log</span>
<span class="n">secure_proxy_ssl_header</span> <span class="o">=</span> <span class="n">HTTP_X_FORWARDED_PROTO</span>
<span class="p">[</span><span class="n">database</span><span class="p">]</span>
<span class="n">connection</span> <span class="o">=</span> <span class="n">mysql</span><span class="o">+</span><span class="nl">pymysql</span><span class="p">:</span><span class="c1">//keystone:ickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB@192.168.100.10:3306/keystone</span>
<span class="n">max_retries</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="p">[</span><span class="n">cache</span><span class="p">]</span>
<span class="n">backend</span> <span class="o">=</span> <span class="n">oslo_cache</span><span class="p">.</span><span class="n">memcache_pool</span>
<span class="n">enabled</span> <span class="o">=</span> <span class="n">True</span>
<span class="n">memcache_servers</span> <span class="o">=</span> <span class="mf">192.168.100.215</span><span class="o">:</span><span class="mi">11211</span><span class="p">,</span><span class="mf">192.168.100.170</span><span class="o">:</span><span class="mi">11211</span>
<span class="p">[</span><span class="n">identity</span><span class="p">]</span>
<span class="n">default_domain_id</span> <span class="o">=</span> <span class="k">default</span>
<span class="p">[</span><span class="n">token</span><span class="p">]</span>
<span class="n">provider</span> <span class="o">=</span> <span class="n">uuid</span>
</pre></div>


<p>Check migrate version in the database.<br>
As you will notice, contract/data_migrate/expand are in the same
version</p>
<div class="highlight"><pre><span></span>(mariadb)# mysql -ukeystone -pickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB -h192.168.100.10 keystone -e &quot;select * from migrate_version;&quot; 
Warning: Using a password on the command line interface can be insecure.
+-----------------------+--------------------------------------------------------------------------+---------+
| repository_id         | repository_path                                                          | version |
+-----------------------+--------------------------------------------------------------------------+---------+
| keystone              | /usr/lib/python2.7/site-packages/keystone/common/sql/migrate_repo        |     109 |
| keystone_contract     | /usr/lib/python2.7/site-packages/keystone/common/sql/contract_repo       |       4 |
| keystone_data_migrate | /usr/lib/python2.7/site-packages/keystone/common/sql/data_migration_repo |       4 |
| keystone_expand       | /usr/lib/python2.7/site-packages/keystone/common/sql/expand_repo         |       4 |
+-----------------------+--------------------------------------------------------------------------+---------+
</pre></div>


<p>Before start upgrading the database schema, you will need add SUPER
privileges in the database to keystone user or set
log_bin_trust_function_creators to True.<br>
In my opinion is safer set the value to True, I don't want keystone
with SUPER privileges.</p>
<div class="highlight"><pre><span></span>(mariadb)# mysql -uroot -pnkLMrBibfMTRqOGBAP3UAxdO4kOFfEaPptGM5UDL -h192.168.100.10 keystone -e &quot;set global log_bin_trust_function_creators=1;&quot;
</pre></div>


<p>Now use Rally, tempest or some tool to test/benchmarch keystone service
during upgrade.<br>
If don't want to use one of those tools, just use this for command.</p>
<div class="highlight"><pre><span></span>(host)# for i in {1000..6000} ; do openstack user create --password $i $i; done
</pre></div>


<h3>Start Upgrade</h3>
<p>Check database status before upgrade using Doctor, this may raise issues
in the configuration. Some of them may be ignored(Please, ensure is not
an issue before ignoring).<br>
As example, I'm not using fernet tokens and errors appear about missing
folder.</p>
<div class="highlight"><pre><span></span>(keystone-upgrade)# keystone-manage doctor
</pre></div>


<p>Remove obsoleted tokens</p>
<div class="highlight"><pre><span></span>(keystone-upgrade)# keystone-manage token_flush
</pre></div>


<p>Now, expand the database schema to latest version, in keystone.log can
see the status.<br>
Check in the logs if some error is raised before jump to the next step.</p>
<div class="highlight"><pre><span></span>(keystone-upgrade)# keystone-manage db_sync --expand


2017-01-31 13:42:02.772 306 INFO migrate.versioning.api [-] 4 -&gt; 5... 
2017-01-31 13:42:03.004 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:03.005 306 INFO migrate.versioning.api [-] 5 -&gt; 6... 
2017-01-31 13:42:03.310 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:03.310 306 INFO migrate.versioning.api [-] 6 -&gt; 7... 
2017-01-31 13:42:03.670 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:03.671 306 INFO migrate.versioning.api [-] 7 -&gt; 8... 
2017-01-31 13:42:03.984 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:03.985 306 INFO migrate.versioning.api [-] 8 -&gt; 9... 
2017-01-31 13:42:04.185 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:04.185 306 INFO migrate.versioning.api [-] 9 -&gt; 10... 
2017-01-31 13:42:07.202 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:07.202 306 INFO migrate.versioning.api [-] 10 -&gt; 11... 
2017-01-31 13:42:07.481 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:07.481 306 INFO migrate.versioning.api [-] 11 -&gt; 12... 
2017-01-31 13:42:11.334 306 INFO migrate.versioning.api [-] done
2017-01-31 13:42:11.334 306 INFO migrate.versioning.api [-] 12 -&gt; 13... 
2017-01-31 13:42:11.560 306 INFO migrate.versioning.api [-] done
</pre></div>


<p>After expand the database, migrate it to latest version.<br>
Ensure there are not errors in Keystone logs.</p>
<div class="highlight"><pre><span></span>(keystone-upgrade)# keystone-manage db_sync --migrate

#keystone.log
2017-01-31 13:42:58.771 314 INFO migrate.versioning.api [-] 4 -&gt; 5... 
2017-01-31 13:42:58.943 314 INFO migrate.versioning.api [-] done
2017-01-31 13:42:58.943 314 INFO migrate.versioning.api [-] 5 -&gt; 6... 
2017-01-31 13:42:59.143 314 INFO migrate.versioning.api [-] done
2017-01-31 13:42:59.143 314 INFO migrate.versioning.api [-] 6 -&gt; 7... 
2017-01-31 13:42:59.340 314 INFO migrate.versioning.api [-] done
2017-01-31 13:42:59.341 314 INFO migrate.versioning.api [-] 7 -&gt; 8... 
2017-01-31 13:42:59.698 314 INFO migrate.versioning.api [-] done
2017-01-31 13:42:59.699 314 INFO migrate.versioning.api [-] 8 -&gt; 9... 
2017-01-31 13:42:59.852 314 INFO migrate.versioning.api [-] done
2017-01-31 13:42:59.852 314 INFO migrate.versioning.api [-] 9 -&gt; 10... 
2017-01-31 13:43:00.135 314 INFO migrate.versioning.api [-] done
2017-01-31 13:43:00.135 314 INFO migrate.versioning.api [-] 10 -&gt; 11... 
2017-01-31 13:43:00.545 314 INFO migrate.versioning.api [-] done
2017-01-31 13:43:00.545 314 INFO migrate.versioning.api [-] 11 -&gt; 12... 
2017-01-31 13:43:00.703 314 INFO migrate.versioning.api [-] done
2017-01-31 13:43:00.703 314 INFO migrate.versioning.api [-] 12 -&gt; 13... 
2017-01-31 13:43:00.854 314 INFO migrate.versioning.api [-] done
</pre></div>


<p>Now, see migrate_version table, you will notice that expand and
data_migrate are in the latest version, but contract still in the
previous version.</p>
<div class="highlight"><pre><span></span>(mariadb)# mysql -ukeystone -pickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB -h192.168.100.10 keystone -e &quot;select * from migrate_version;&quot;
+-----------------------+--------------------------------------------------------------------------+---------+
| repository_id         | repository_path                                                          | version |
+-----------------------+--------------------------------------------------------------------------+---------+
| keystone              | /usr/lib/python2.7/site-packages/keystone/common/sql/migrate_repo        |     109 |
| keystone_contract     | /usr/lib/python2.7/site-packages/keystone/common/sql/contract_repo       |       4 |
| keystone_data_migrate | /usr/lib/python2.7/site-packages/keystone/common/sql/data_migration_repo |      13 |
| keystone_expand       | /usr/lib/python2.7/site-packages/keystone/common/sql/expand_repo         |      13 |
+-----------------------+--------------------------------------------------------------------------+---------+
</pre></div>


<h3>Every Keystone node, one by one</h3>
<p>Go to keystone nodes.<br>
Stop Keystone services, in my case using wsgi inside Apache</p>
<div class="highlight"><pre><span></span>(keystone_nodes)# systemctl stop httpd
</pre></div>


<p>Configure Ocata repositories as made in the Docker container.<br>
Update packages, if you have Keystone sharing the node with other
OpenStack service, do not update all packages as it will break other
services.<br>
Update only required packages.</p>
<div class="highlight"><pre><span></span>(keystone_nodes)# yum clean all &amp;&amp; yum update -y
</pre></div>


<p>Configure Keystone configuration file to the desired state. Your
configuration may change.</p>
<div class="highlight"><pre><span></span><span class="p">(</span><span class="n">keystone_nodes</span><span class="p">)</span><span class="err">#</span> <span class="n">egrep</span> <span class="o">^</span><span class="p">[</span><span class="o">^</span><span class="err">#</span><span class="p">]</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">keystone</span><span class="o">/</span><span class="n">keystone</span><span class="p">.</span><span class="n">conf</span> 
<span class="p">[</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="n">debug</span> <span class="o">=</span> <span class="n">False</span>
<span class="n">log_file</span> <span class="o">=</span> <span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">log</span><span class="o">/</span><span class="n">keystone</span><span class="o">/</span><span class="n">keystone</span><span class="p">.</span><span class="n">log</span>
<span class="n">secure_proxy_ssl_header</span> <span class="o">=</span> <span class="n">HTTP_X_FORWARDED_PROTO</span>
<span class="p">[</span><span class="n">database</span><span class="p">]</span>
<span class="n">connection</span> <span class="o">=</span> <span class="n">mysql</span><span class="o">+</span><span class="nl">pymysql</span><span class="p">:</span><span class="c1">//keystone:ickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB@192.168.100.10:3306/keystone</span>
<span class="n">max_retries</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="p">[</span><span class="n">cache</span><span class="p">]</span>
<span class="n">backend</span> <span class="o">=</span> <span class="n">oslo_cache</span><span class="p">.</span><span class="n">memcache_pool</span>
<span class="n">enabled</span> <span class="o">=</span> <span class="n">True</span>
<span class="n">memcache_servers</span> <span class="o">=</span> <span class="mf">192.168.100.215</span><span class="o">:</span><span class="mi">11211</span><span class="p">,</span><span class="mf">192.168.100.170</span><span class="o">:</span><span class="mi">11211</span>
<span class="p">[</span><span class="n">identity</span><span class="p">]</span>
<span class="n">default_domain_id</span> <span class="o">=</span> <span class="k">default</span>
<span class="p">[</span><span class="n">token</span><span class="p">]</span>
<span class="n">provider</span> <span class="o">=</span> <span class="n">uuid</span>
</pre></div>


<p>Start Keystone service.</p>
<div class="highlight"><pre><span></span>(keystone_nodes)# systemctl start httpd
</pre></div>


<h3>Finish Upgrade</h3>
<p>After all the nodes are updated to latest version (please ensure all
nodes are using latest packages, if not will fail).<br>
Contract Keystone database schema.<br>
Look at keystone.log for errors.</p>
<div class="highlight"><pre><span></span>(keystone-upgrade)# keystone-manage db_sync --contract

keystone.log

2017-01-31 13:57:52.164 322 INFO migrate.versioning.api [-] 4 -&gt; 5... 
2017-01-31 13:57:52.379 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:52.379 322 INFO migrate.versioning.api [-] 5 -&gt; 6... 
2017-01-31 13:57:52.969 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:52.969 322 INFO migrate.versioning.api [-] 6 -&gt; 7... 
2017-01-31 13:57:53.462 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:53.462 322 INFO migrate.versioning.api [-] 7 -&gt; 8... 
2017-01-31 13:57:53.793 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:53.793 322 INFO migrate.versioning.api [-] 8 -&gt; 9... 
2017-01-31 13:57:53.957 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:53.957 322 INFO migrate.versioning.api [-] 9 -&gt; 10... 
2017-01-31 13:57:54.111 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:54.112 322 INFO migrate.versioning.api [-] 10 -&gt; 11... 
2017-01-31 13:57:54.853 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:54.853 322 INFO migrate.versioning.api [-] 11 -&gt; 12... 
2017-01-31 13:57:56.727 322 INFO migrate.versioning.api [-] done
2017-01-31 13:57:56.728 322 INFO migrate.versioning.api [-] 12 -&gt; 13... 
2017-01-31 13:57:59.529 322 INFO migrate.versioning.api [-] done
</pre></div>


<p>Now if we look at migrate_version table, will see that contract version
is latest and match with the other version (Ensure all are in the same
version).<br>
This means the database upgrade has been successfully implemented.</p>
<div class="highlight"><pre><span></span>(mariadb)# mysql -ukeystone -pickvaHC9opkwbz8z8sy28aLiFNezc7Z6Fm34frcB -h192.168.100.10 keystone -e &quot;select * from migrate_version;&quot;
+-----------------------+--------------------------------------------------------------------------+---------+
| repository_id         | repository_path                                                          | version |
+-----------------------+--------------------------------------------------------------------------+---------+
| keystone              | /usr/lib/python2.7/site-packages/keystone/common/sql/migrate_repo        |     109 |
| keystone_contract     | /usr/lib/python2.7/site-packages/keystone/common/sql/contract_repo       |      13 |
| keystone_data_migrate | /usr/lib/python2.7/site-packages/keystone/common/sql/data_migration_repo |      13 |
| keystone_expand       | /usr/lib/python2.7/site-packages/keystone/common/sql/expand_repo         |      13 |
+-----------------------+--------------------------------------------------------------------------+---------+
</pre></div>


<p>Remove log_bin_trust_function_creators value.</p>
<div class="highlight"><pre><span></span>(mariadb)# mysql -uroot -pnkLMrBibfMTRqOGBAP3UAxdO4kOFfEaPptGM5UDL -h192.168.100.10 keystone -e &quot;set global log_bin_trust_function_creators=0;&quot;
</pre></div>


<p>After finish the upgrade, Rally tests should not have any error. **If
using HAproxy for load balance Keystone service, some errors may happen
due a connection drop while stopping Keystone service and re-balance to
other Keystone node. This can be avoided putting the node to update in
Maintenance Mode in HAproxy backend.</p>
<p>Have to thank Keystone team in #openstack-keystone IRC channel for the
help provided with a couple of issues.</p>
<p>Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/openstack-keystone-zero-downtime-upgrade-process-n-to-o.html">posted at 17:00</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/linux-openstack-various.html" rel="tag">Linux, OpenStack, Various</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/contract.html" class="tags">contract</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/database.html" class="tags">database</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/docker.html" class="tags">docker</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/downtime.html" class="tags">Downtime</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/expand.html" class="tags">expand</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/keystone.html" class="tags">keystone</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/kolla.html" class="tags">kolla</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/migrate.html" class="tags">migrate</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/newton.html" class="tags">newton</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/ocata.html" class="tags">ocata</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/schema.html" class="tags">schema</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/upgrade.html" class="tags">Upgrade</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/zero.html" class="tags">Zero</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/zero-downtime.html" class="tags">zero-downtime</a>
                </div>
		<a href="https://egonzalez90.github.io/openstack-keystone-zero-downtime-upgrade-process-n-to-o.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Jul 04, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/midonet-integration-with-openstack-mitaka.html" rel="bookmark" title="Permanent Link to &quot;MidoNet Integration with OpenStack Mitaka&quot;">MidoNet Integration with OpenStack Mitaka</a>
                </h2>

                
                

                <p>MidoNet is an Open Source network virtualization software for IaaS
infrastructure.<br>
It decouples your IaaS cloud from your network hardware, creating an
intelligent software abstraction layer between your end hosts and your
physical network.<br>
This network abstraction layer allows the cloud operator to move what
has traditionally been hardware-based network appliances into a
software-based multi-tenant virtual domain.</p>
<p>This definition from MidoNet documentation explains what MidoNet is and
what MidoNet does.</p>
<p>At this I will post cover my experiences integrating MidoNet with
OpenStack.<br>
I used the following configurations:</p>
<p>All servers have CentOS 7.2 installed<br>
OpenStack has been previously installed from RDO packages with
multinode Packstack</p>
<ul>
<li>
<p>x3 NSDB nodes (Casandra and Zookeeper services)</p>
</li>
<li>
<p>x2 Gateway Nodes (Midolman Agent)</p>
</li>
<li>
<p>x1 OpenStack Controller (MidoNet Cluster)</p>
</li>
<li>
<p>x1 OpenStack compute node (Midolman Agent)</p>
</li>
</ul>
<p><strong>NSDB NODES</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Cassandra repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/datastax.repo
[datastax]
name = DataStax Repo for Apache Cassandra
baseurl = http://rpm.datastax.com/community
enabled = 1
gpgcheck = 1
gpgkey = https://rpm.datastax.com/rpm/repo_key
EOF
</pre></div>


<p>Add Midonet repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repo cache and update packages</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p><strong>Zookeeper Configuration</strong><br>
Install Zookeeper, java and dependencies</p>
<div class="highlight"><pre><span></span>yum install -y java-1.7.0-openjdk-headless zookeeper zkdump nmap-ncat
</pre></div>


<p>Edit zookeeper configuration file</p>
<div class="highlight"><pre><span></span>vi /etc/zookeeper/zoo.cfg
</pre></div>


<p>Add all NSDB nodes at the configuration file</p>
<div class="highlight"><pre><span></span>server.1=nsdb1:2888:3888
server.2=nsdb2:2888:3888
server.3=nsdb3:2888:3888
autopurge.snapRetainCount=10
autopurge.purgeInterval =12
</pre></div>


<p>Create zookeeper folder on which zookeeper will store data, change the
owner to zookeeper user</p>
<div class="highlight"><pre><span></span>mkdir /var/lib/zookeeper/data
chown zookeeper:zookeeper /var/lib/zookeeper/data
</pre></div>


<p>Create myid file at zookeeper data folder, the ID should match with the
NSDB node number, insert that number as follows:</p>
<div class="highlight"><pre><span></span>#NSDB1
echo 1 &gt; /var/lib/zookeeper/data/myid
#NSDB2
echo 2 &gt; /var/lib/zookeeper/data/myid
#NSDB3
echo 3 &gt; /var/lib/zookeeper/data/myid
</pre></div>


<p>Create java folder and create a softlink to it</p>
<div class="highlight"><pre><span></span>mkdir -p /usr/java/default/bin/
ln -s /usr/lib/jvm/jre-1.7.0-openjdk/bin/java /usr/java/default/bin/java
</pre></div>


<p>Start and enable Zookeeper service</p>
<div class="highlight"><pre><span></span>systemctl enable zookeeper.service
systemctl start zookeeper.service
</pre></div>


<p>Test if zookeeper is working locally</p>
<div class="highlight"><pre><span></span>echo ruok | nc 127.0.0.1 2181
imok
</pre></div>


<p>Test if zookeeper is working at NSDB remote nodes</p>
<div class="highlight"><pre><span></span>echo stat | nc nsdb3 2181

Zookeeper version: 3.4.5--1, built on 02/08/2013 12:25 GMT
Clients:
 /192.168.100.172:35306[0](queued=0,recved=1,sent=0)

Latency min/avg/max: 0/0/0
Received: 1
Sent: 0
Connections: 1
Outstanding: 0
Zxid: 0x100000000
Mode: follower
Node count: 4
</pre></div>


<p><strong>Cassandra configuration</strong><br>
Install Java and Cassandra dependencies</p>
<div class="highlight"><pre><span></span>yum install -y java-1.8.0-openjdk-headless dsc22
</pre></div>


<p>Edit cassandra yaml file</p>
<div class="highlight"><pre><span></span>vi /etc/cassandra/conf/cassandra.yaml
</pre></div>


<p>Change cluster_name to midonet<br>
Configure seed_provider seeds to match all NSDB nodes<br>
Configure listen_address and rpc_address to match the hostname of the
self node</p>
<div class="highlight"><pre><span></span>cluster_name: &#39;midonet&#39;
....
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: &quot;nsdb1,nsdb2,nsdb3&quot;

listen_address: nsdb1
rpc_address: nsdb1
</pre></div>


<p>Edit cassandra's init script in order to fix a bug in the init script</p>
<div class="highlight"><pre><span></span>vi /etc/init.d/cassandra
</pre></div>


<p>Add the next two lines after #Casandra startup</p>
<div class="highlight"><pre><span></span>case &quot;$1&quot; in
    start)
        # Cassandra startup
        echo -n &quot;Starting Cassandra: &quot;
        mkdir -p /var/run/cassandra #Add this line
        chown cassandra:cassandra /var/run/cassandra #Add this line
        su $CASSANDRA_OWNR -c &quot;$CASSANDRA_PROG -p $pid_file&quot; &gt; $log_file 2&gt;&amp;1
        retval=$?
        [ $retval -eq 0 ] &amp;&amp; touch $lock_file
        echo &quot;OK&quot;
        ;;
</pre></div>


<p>Start and enable Cassandra service</p>
<div class="highlight"><pre><span></span>systemctl enable cassandra.service
systemctl start cassandra.service
</pre></div>


<p>Check if all NSDB nodes join the cluster</p>
<div class="highlight"><pre><span></span>nodetool --host 127.0.0.1 status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address          Load       Tokens       Owns (effective)  Host ID                               Rack
UN  192.168.100.172  89.1 KB    256          70.8%             3f1ecedd-8caf-4938-84ad-8614d2134557  rack1
UN  192.168.100.224  67.64 KB   256          60.7%             cb36c999-a6e1-4d98-a4dd-d4230b41df08  rack1
UN  192.168.100.195  25.78 KB   256          68.6%             4758bae8-9300-4e57-9a61-5b1107082964  rack1
</pre></div>


<p><strong>Configure OpenStack Controller Nodes (On which Neutron Server is
running)</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Midonet Repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repos cache and update the system</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p>Create an OpenStack user for MidoNet, change the password to match your
own</p>
<div class="highlight"><pre><span></span># openstack user create --password temporal midonet
+----------+----------------------------------+
| Field    | Value                            |
+----------+----------------------------------+
| email    | None                             |
| enabled  | True                             |
| id       | ac25c5a77e7c4e4598ccadea89e09969 |
| name     | midonet                          |
| username | midonet                          |
+----------+----------------------------------+
</pre></div>


<p>Add admin role at tenant services to Midonet user</p>
<div class="highlight"><pre><span></span># openstack role add --project services --user midonet admin
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | bca2c6e1f3da42b0ba82aee401398a8a |
| name      | admin                            |
+-----------+----------------------------------+
</pre></div>


<p>Create MidoNet service at Keystone</p>
<div class="highlight"><pre><span></span># openstack service create --name midonet --description &quot;MidoNet API Service&quot; midonet
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | MidoNet API Service              |
| enabled     | True                             |
| id          | 499059c4a3a040cfb632411408a2be4c |
| name        | midonet                          |
| type        | midonet                          |
+-------------+----------------------------------+
</pre></div>


<p><strong>Clean up neutron server</strong><br>
Stop neutron services</p>
<div class="highlight"><pre><span></span>openstack-service stop neutron
</pre></div>


<p>Remove neutron database and recreate it again</p>
<div class="highlight"><pre><span></span>mysql -u root -p
DROP DATABASE neutron;
Query OK, 157 rows affected (11.50 sec)

MariaDB [(none)]&gt; CREATE DATABASE neutron;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO &#39;neutron&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;ab4f81b1040a495e&#39;;
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO &#39;neutron&#39;@&#39;%&#39; IDENTIFIED BY &#39;ab4f81b1040a495e&#39;;
Query OK, 0 rows affected (0.00 sec)
MariaDB [(none)]&gt; exit
Bye
</pre></div>


<p>Remove plugin.ini symbolic link to ml2_conf.ini</p>
<div class="highlight"><pre><span></span>#rm /etc/neutron/plugin.ini 
rm: remove symbolic link ‘/etc/neutron/plugin.ini’? y
</pre></div>


<p>Remove br-tun tunnel used by neutron in all the nodes</p>
<div class="highlight"><pre><span></span>ovs-vsctl del-br br-tun
</pre></div>


<p>Install MidoNet packages and remove ml2 package</p>
<div class="highlight"><pre><span></span>yum install -y openstack-neutron python-networking-midonet python-neutronclient
yum remove openstack-neutron-ml2
</pre></div>


<p>Make a backup of neutron configuration file</p>
<div class="highlight"><pre><span></span>cp /etc/neutron/neutron.conf neutron.conf.bak
</pre></div>


<p>Edit neutron configuration file</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/neutron.conf
</pre></div>


<p>Most of the options are already configured by our older neutron
configuration, change the ones who apply to match this configuration</p>
<div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">core_plugin</span> <span class="o">=</span> <span class="s">midonet.neutron.plugin_v2.MidonetPluginV2</span>
<span class="na">service_plugins</span> <span class="o">=</span> <span class="s">midonet.neutron.services.l3.l3_midonet.MidonetL3ServicePlugin</span>
<span class="na">dhcp_agent_notification</span> <span class="o">=</span> <span class="s">False</span>
<span class="na">allow_overlapping_ips</span> <span class="o">=</span> <span class="s">True</span>
<span class="na">rpc_backend</span> <span class="o">=</span> <span class="s">rabbit</span>
<span class="na">auth_strategy</span> <span class="o">=</span> <span class="s">keystone</span>
<span class="na">notify_nova_on_port_status_changes</span> <span class="o">=</span> <span class="s">true</span>
<span class="na">notify_nova_on_port_data_changes</span> <span class="o">=</span> <span class="s">true</span>
<span class="na">nova_url</span> <span class="o">=</span> <span class="s">http://controller:8774/v2</span>

<span class="k">[database]</span>
<span class="na">connection</span> <span class="o">=</span> <span class="s">mysql+pymysql://neutron:ab4f81b1040a495e@controller/neutron</span>

<span class="k">[oslo_messaging_rabbit]</span>
<span class="na">rabbit_host</span> <span class="o">=</span> <span class="s">controller</span>
<span class="na">rabbit_userid</span> <span class="o">=</span> <span class="s">guest</span>
<span class="na">rabbit_password</span> <span class="o">=</span> <span class="s">guest</span>

<span class="k">[keystone_authtoken]</span>
<span class="na">auth_uri</span> <span class="o">=</span> <span class="s">http://controller:5000/v2.0</span>
<span class="na">admin_user</span><span class="o">=</span><span class="s">neutron</span>
<span class="na">admin_tenant_name</span><span class="o">=</span><span class="s">services</span>
<span class="na">identity_uri</span><span class="o">=</span><span class="s">http://controller:35357</span>
<span class="na">admin_password</span><span class="o">=</span><span class="s">d88f0bd060d64c33</span>

<span class="k">[nova]</span>
<span class="na">region_name</span> <span class="o">=</span> <span class="s">RegionOne</span>
<span class="na">auth_url</span> <span class="o">=</span> <span class="s">http://controller:35357</span>
<span class="na">auth_type</span> <span class="o">=</span> <span class="s">password</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">9ca36d15e4824d93</span>
<span class="na">project_domain_id</span> <span class="o">=</span> <span class="s">default</span>
<span class="na">project_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">tenant_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">user_domain_id</span> <span class="o">=</span> <span class="s">default</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">nova</span>

<span class="k">[oslo_concurrency]</span>
<span class="na">lock_path</span> <span class="o">=</span> <span class="s">/var/lib/neutron/tmp</span>
</pre></div>


<p>At my deployment these are the options I had to change to configure
midonet</p>
<div class="highlight"><pre><span></span><span class="gh">diff /etc/neutron/neutron.conf neutron.conf.bak </span>
33c33
&lt; core_plugin = midonet.neutron.plugin_v2.MidonetPluginV2
<span class="gd">---</span>
&gt; core_plugin = neutron.plugins.ml2.plugin.Ml2Plugin
37c37
&lt; service_plugins = midonet.neutron.services.l3.l3_midonet.MidonetL3ServicePlugin
<span class="gd">---</span>
&gt; service_plugins =router
120c120
&lt; dhcp_agent_notification = False
<span class="gd">---</span>
&gt; #dhcp_agent_notification = true
1087c1087,1088
&lt; lock_path = /var/lib/neutron/tmp
<span class="gd">---</span>
&gt; lock_path = $state_path/lock
&gt;
</pre></div>


<p>Create midonet plugins folder</p>
<div class="highlight"><pre><span></span>mkdir /etc/neutron/plugins/midonet
</pre></div>


<p>Create a file called midonet.ini</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/plugins/midonet/midonet.ini
</pre></div>


<p>Configure midonet.ini file to match your own configuration options</p>
<div class="highlight"><pre><span></span><span class="k">[MIDONET]</span>
<span class="c1"># MidoNet API URL</span>
<span class="na">midonet_uri</span> <span class="o">=</span> <span class="s">http://controller:8181/midonet-api</span>
<span class="c1"># MidoNet administrative user in Keystone</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">midonet</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">temporal</span>
<span class="c1"># MidoNet administrative user&#39;s tenant</span>
<span class="na">project_id</span> <span class="o">=</span> <span class="s">services</span>
</pre></div>


<p>Create a symbolic link from midonet.ini to plugin.ini</p>
<div class="highlight"><pre><span></span>ln -s /etc/neutron/plugins/midonet/midonet.ini /etc/neutron/plugin.ini
</pre></div>


<p>Sync and populate database tables with Midonet plugin</p>
<div class="highlight"><pre><span></span>su -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/midonet/midonet.ini upgrade head&quot; neutron
su -s /bin/sh -c &quot;neutron-db-manage --subproject networking-midonet upgrade head&quot; neutron
</pre></div>


<p>Restart nova api and neutron server services</p>
<div class="highlight"><pre><span></span>systemctl restart openstack-nova-api.service
systemctl restart neutron-server
</pre></div>


<p>Install midonet cluster package</p>
<div class="highlight"><pre><span></span>yum install -y midonet-cluster
</pre></div>


<p>Configure midonet.conf file</p>
<div class="highlight"><pre><span></span>vi /etc/midonet/midonet.conf
</pre></div>


<p>Add all NSDB nodes at zookeeper_hosts</p>
<div class="highlight"><pre><span></span><span class="k">[zookeeper]</span>
<span class="na">zookeeper_hosts</span> <span class="o">=</span> <span class="s">nsdb1:2181,nsdb2:2181,nsdb3:2181</span>
</pre></div>


<p>Configure midonet to make use of NSDB nodes as Zookeeper and cassandra
hosts</p>
<div class="highlight"><pre><span></span>cat &lt;&lt; EOF | mn-conf set -t default
zookeeper {
    zookeeper_hosts = &quot;nsdb1:2181,nsdb2:2181,nsdb3:2181&quot;
}

cassandra {
    servers = &quot;nsdb1,nsdb2,nsdb3&quot;
}
EOF
</pre></div>


<p>Set cassandra replication factor to 3</p>
<div class="highlight"><pre><span></span>echo &quot;cassandra.replication_factor : 3&quot; | mn-conf set -t default
</pre></div>


<p>Grab your admin token</p>
<div class="highlight"><pre><span></span>#egrep ^admin_token /etc/keystone/keystone.conf 
admin_token = 7b84d89b32c34b71a697eb1a270807ab
</pre></div>


<p>Configure Midonet to auth with keystone</p>
<div class="highlight"><pre><span></span>cat &lt;&lt; EOF | mn-conf set -t default
cluster.auth {
    provider_class = &quot;org.midonet.cluster.auth.keystone.KeystoneService&quot;
    admin_role = &quot;admin&quot;
    keystone.tenant_name = &quot;admin&quot;
    keystone.admin_token = &quot;7b84d89b32c34b71a697eb1a270807ab&quot;
    keystone.host = controller
    keystone.port = 35357
}
EOF
</pre></div>


<p>Start and enable midonet cluster service</p>
<div class="highlight"><pre><span></span>systemctl enable midonet-cluster.service
systemctl start midonet-cluster.service
</pre></div>


<p>Install midonet CLI</p>
<div class="highlight"><pre><span></span>yum install -y python-midonetclient
</pre></div>


<p>Create a file at you home directory with midonet auth info</p>
<div class="highlight"><pre><span></span>#vi ~/.midonetrc

[cli]
api_url = http://controller:8181/midonet-api
username = admin
password = temporal
project_id = admin
</pre></div>


<p><strong>Configure Compute nodes</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Midonet repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repos cache and update the system</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p>Edit qemu.conf</p>
<div class="highlight"><pre><span></span>vi /etc/libvirt/qemu.conf
</pre></div>


<p>Configure with the following options, by default all these options are
commented, you can paste it all wherever you want</p>
<div class="highlight"><pre><span></span>user = &quot;root&quot;
group = &quot;root&quot;

cgroup_device_acl = [
    &quot;/dev/null&quot;, &quot;/dev/full&quot;, &quot;/dev/zero&quot;,
    &quot;/dev/random&quot;, &quot;/dev/urandom&quot;,
    &quot;/dev/ptmx&quot;, &quot;/dev/kvm&quot;, &quot;/dev/kqemu&quot;,
    &quot;/dev/rtc&quot;,&quot;/dev/hpet&quot;, &quot;/dev/vfio/vfio&quot;,
    &quot;/dev/net/tun&quot;
]
</pre></div>


<p>Restart libvirtd service</p>
<div class="highlight"><pre><span></span>systemctl restart libvirtd.service
</pre></div>


<p>Install nova-network package</p>
<div class="highlight"><pre><span></span>yum install -y openstack-nova-network
</pre></div>


<p>Disable Nova Network service and restart Nova compute service</p>
<div class="highlight"><pre><span></span>systemctl disable openstack-nova-network.service
systemctl restart openstack-nova-compute.service
</pre></div>


<p>Install Midolman agent and java packages</p>
<div class="highlight"><pre><span></span>yum install -y java-1.8.0-openjdk-headless midolman
</pre></div>


<p>Configure midolman.conf</p>
<div class="highlight"><pre><span></span>vi /etc/midolman/midolman.conf
</pre></div>


<p>Add all nsdb nodes as zookeeper hosts</p>
<div class="highlight"><pre><span></span><span class="k">[zookeeper]</span>
<span class="na">zookeeper_hosts</span> <span class="o">=</span> <span class="s">nsdb1:2181,nsdb2:2181,nsdb3:2181</span>
</pre></div>


<p>Configure each compute node with an appropiate flavor located at
/etc/midolman/ folder, the have different hardware resources configured,
use the one that better match your compute host capabilities</p>
<div class="highlight"><pre><span></span>mn-conf template-set -h local -t agent-compute-medium
cp /etc/midolman/midolman-env.sh.compute.medium /etc/midolman/midolman-env.sh
</pre></div>


<p>Configure metadata, issue the following commands only once, it will
automatically populate the configuration to all midonet agents</p>
<div class="highlight"><pre><span></span>echo &quot;agent.openstack.metadata.nova_metadata_url : \&quot;http://controller:8775\&quot;&quot; | mn-conf set -t default
echo &quot;agent.openstack.metadata.shared_secret : 2bfeb930a90d435d&quot; | mn-conf set -t default
echo &quot;agent.openstack.metadata.enabled : true&quot; | mn-conf set -t default
</pre></div>


<p>Allow metadata trafic at iptables</p>
<div class="highlight"><pre><span></span>iptables -I INPUT 1 -i metadata -j ACCEPT
</pre></div>


<p>Remove br-tun bridge</p>
<div class="highlight"><pre><span></span>ovs-vsctl del-br br-tun
</pre></div>


<p>Start and enable midolman agent service</p>
<div class="highlight"><pre><span></span>systemctl enable midolman.service
systemctl start midolman.service
</pre></div>


<p><strong>Gateway nodes configuration</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Midonet repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repos cache and update the system</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p>Install Midolman agent and java packages</p>
<div class="highlight"><pre><span></span>yum install -y java-1.8.0-openjdk-headless midolman
</pre></div>


<p>Configure midolman.conf</p>
<div class="highlight"><pre><span></span>vi /etc/midolman/midolman.conf
</pre></div>


<p>Add all nsdb nodes as zookeeper hosts</p>
<div class="highlight"><pre><span></span><span class="k">[zookeeper]</span>
<span class="na">zookeeper_hosts</span> <span class="o">=</span> <span class="s">nsdb1:2181,nsdb2:2181,nsdb3:2181</span>
</pre></div>


<p>Configure each gateway node with an appropiate flavor located at
/etc/midolman/ folder, the have different hardware resources configured,
use the one that better match your gateway host capabilities</p>
<div class="highlight"><pre><span></span>mn-conf template-set -h local -t agent-gateway-medium
cp /etc/midolman/midolman-env.sh.gateway.medium /etc/midolman/midolman-env.sh
</pre></div>


<p>Grab the metadata shared secret located at nova.conf at any of your nova
nodes</p>
<div class="highlight"><pre><span></span># egrep ^metadata_proxy_shared_secret /etc/nova/nova.conf 
metadata_proxy_shared_secret =2bfeb930a90d435d
</pre></div>


<p>Allow metadata trafic at iptables</p>
<div class="highlight"><pre><span></span>iptables -I INPUT 1 -i metadata -j ACCEPT
</pre></div>


<p>Start and enable midolman agent service</p>
<div class="highlight"><pre><span></span>systemctl enable midolman.service
systemctl start midolman.service
</pre></div>


<p><strong>Configure encapsulation and register nodes</strong><br>
Enter to midonet CLI from a controller node</p>
<div class="highlight"><pre><span></span>midonet-cli
</pre></div>


<p>Create the tunnel zone with VXLAN encapsulation</p>
<div class="highlight"><pre><span></span>midonet&gt; tunnel-zone create name tz type vxlan
tzone0
midonet&gt; list tunnel-zone
tzone tzone0 name tz type vxlan
</pre></div>


<p>List hosts discovered by midonet, should be all the nodes where you
configured midonet agents(midolman)</p>
<div class="highlight"><pre><span></span>midonet&gt; list host
host host0 name gateway2 alive true addresses fe80:0:0:0:0:11ff:fe00:1102,169.254.123.1,fe80:0:0:0:0:11ff:fe00:1101,127.0.0.1,0:0:0:0:0:0:0:1,192.168.200.176,fe80:0:0:0:5054:ff:fef9:b2a0,169.254.169.254,fe80:0:0:0:7874:d6ff:fe5b:dea8,192.168.100.227,fe80:0:0:0:5054:ff:fed9:9cc0,fe80:0:0:0:5054:ff:fe4a:e39b,192.168.1.86 flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
host host1 name gateway1 alive true addresses 169.254.169.254,fe80:0:0:0:3cd1:23ff:feac:a3c2,192.168.1.87,fe80:0:0:0:5054:ff:fea8:da91,127.0.0.1,0:0:0:0:0:0:0:1,fe80:0:0:0:5054:ff:feec:92c1,192.168.200.232,fe80:0:0:0:0:11ff:fe00:1102,169.254.123.1,fe80:0:0:0:0:11ff:fe00:1101,192.168.100.141,fe80:0:0:0:5054:ff:fe20:30fb flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
host host2 name compute1 alive true addresses fe80:0:0:0:0:11ff:fe00:1101,169.254.123.1,127.0.0.1,0:0:0:0:0:0:0:1,fe80:0:0:0:0:11ff:fe00:1102,192.168.100.173,fe80:0:0:0:5054:ff:fe06:161,fe80:0:0:0:5054:ff:fee3:eb48,192.168.200.251,fe80:0:0:0:5054:ff:fe8d:d22,192.168.1.93,169.254.169.254,fe80:0:0:0:48cb:adff:fe69:f07b flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
</pre></div>


<p>Register each of the nodes at the VXLAN zone we created before</p>
<div class="highlight"><pre><span></span>midonet&gt; tunnel-zone tzone0 add member host host0 address 192.168.100.227
zone tzone0 host host0 address 192.168.100.227
midonet&gt; tunnel-zone tzone0 add member host host1 address 192.168.100.141
zone tzone0 host host1 address 192.168.100.141
midonet&gt; tunnel-zone tzone0 add member host host2 address 192.168.100.173
zone tzone0 host host2 address 192.168.100.173
</pre></div>


<p><strong>Create Networks at Neutron</strong><br>
Create an external network</p>
<div class="highlight"><pre><span></span># neutron net-create ext-net --router:external
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:47:30                  |
| description           |                                      |
| id                    | dc15245e-4391-4514-b489-8976373046a3 |
| is_default            | False                                |
| name                  | ext-net                              |
| port_security_enabled | True                                 |
| provider:network_type | midonet                              |
| router:external       | True                                 |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
| updated_at            | 2016-07-03T14:47:30                  |
+-----------------------+--------------------------------------+
</pre></div>


<p>Create an external subnet in the network we created before, use you own
IP ranges to match your environment</p>
<div class="highlight"><pre><span></span># neutron subnet-create ext-net 192.168.200.0/24 --name ext-subnet   
 --allocation-pool start=192.168.200.225,end=192.168.200.240   
 --disable-dhcp --gateway 192.168.200.1
Created a new subnet:
+-------------------+--------------------------------------------------------+
| Field             | Value                                                  |
+-------------------+--------------------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;192.168.200.225&quot;, &quot;end&quot;: &quot;192.168.200.240&quot;} |
| cidr              | 192.168.200.0/24                                       |
| created_at        | 2016-07-03T14:50:46                                    |
| description       |                                                        |
| dns_nameservers   |                                                        |
| enable_dhcp       | False                                                  |
| gateway_ip        | 192.168.200.1                                          |
| host_routes       |                                                        |
| id                | 234dcc9a-2878-4799-b564-bf3a1bd52cad                   |
| ip_version        | 4                                                      |
| ipv6_address_mode |                                                        |
| ipv6_ra_mode      |                                                        |
| name              | ext-subnet                                             |
| network_id        | dc15245e-4391-4514-b489-8976373046a3                   |
| subnetpool_id     |                                                        |
| tenant_id         | 2f7ee2716b3b4140be57b4a5b26401e3                       |
| updated_at        | 2016-07-03T14:50:46                                    |
+-------------------+--------------------------------------------------------+
</pre></div>


<p>Create a tenant network and a subnet on it</p>
<div class="highlight"><pre><span></span># neutron net-create demo-net
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:51:39                  |
| description           |                                      |
| id                    | 075ba699-dc4c-4625-8e0d-0a258a9aeb7d |
| name                  | demo-net                             |
| port_security_enabled | True                                 |
| provider:network_type | midonet                              |
| router:external       | False                                |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
| updated_at            | 2016-07-03T14:51:39                  |
+-----------------------+--------------------------------------+
# neutron subnet-create demo-net 10.0.20.0/24 --name demo-subnet 
Created a new subnet:
+-------------------+----------------------------------------------+
| Field             | Value                                        |
+-------------------+----------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;10.0.20.2&quot;, &quot;end&quot;: &quot;10.0.20.254&quot;} |
| cidr              | 10.0.20.0/24                                 |
| created_at        | 2016-07-03T14:52:32                          |
| description       |                                              |
| dns_nameservers   |                                              |
| enable_dhcp       | True                                         |
| gateway_ip        | 10.0.20.1                                    |
| host_routes       |                                              |
| id                | b299d899-33a3-4bfa-aff4-fda071545bdf         |
| ip_version        | 4                                            |
| ipv6_address_mode |                                              |
| ipv6_ra_mode      |                                              |
| name              | demo-subnet                                  |
| network_id        | 075ba699-dc4c-4625-8e0d-0a258a9aeb7d         |
| subnetpool_id     |                                              |
| tenant_id         | 2f7ee2716b3b4140be57b4a5b26401e3             |
| updated_at        | 2016-07-03T14:52:32                          |
+-------------------+----------------------------------------------+
</pre></div>


<p>Create a tenant router</p>
<div class="highlight"><pre><span></span># neutron router-create router1
Created a new router:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| description           |                                      |
| external_gateway_info |                                      |
| id                    | 258942d8-9d82-4ebd-b829-c7bdfcc973f5 |
| name                  | router1                              |
| routes                |                                      |
| status                | ACTIVE                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
+-----------------------+--------------------------------------+
</pre></div>


<p>Attach the tenant subnet interface we created before to the router</p>
<div class="highlight"><pre><span></span># neutron router-interface-add router1 demo-subnet
Added interface 06c85a56-368c-4d79-bbf0-4bb077f163e5 to router router1.
</pre></div>


<p>Set the external network as router gateway</p>
<div class="highlight"><pre><span></span># neutron router-gateway-set router1 ext-net
Set gateway for router router1
</pre></div>


<p>Now, you can create an instance at tenant network</p>
<div class="highlight"><pre><span></span># nova boot --flavor m1.tiny --image 80871834-29dd-4100-b038-f5f83f126204 --nic net-id=075ba699-dc4c-4625-8e0d-0a258a9aeb7d test1
+--------------------------------------+-----------------------------------------------------+
| Property                             | Value                                               |
+--------------------------------------+-----------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                              |
| OS-EXT-AZ:availability_zone          |                                                     |
| OS-EXT-SRV-ATTR:host                 | -                                                   |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                                   |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000000a                                   |
| OS-EXT-STS:power_state               | 0                                                   |
| OS-EXT-STS:task_state                | scheduling                                          |
| OS-EXT-STS:vm_state                  | building                                            |
| OS-SRV-USG:launched_at               | -                                                   |
| OS-SRV-USG:terminated_at             | -                                                   |
| accessIPv4                           |                                                     |
| accessIPv6                           |                                                     |
| adminPass                            | q2Cq4kxePSLL                                        |
| config_drive                         |                                                     |
| created                              | 2016-07-03T15:46:19Z                                |
| flavor                               | m1.tiny (1)                                         |
| hostId                               |                                                     |
| id                                   | b8aa46f9-186c-4594-8428-f8dbb16a5e16                |
| image                                | cirros image (80871834-29dd-4100-b038-f5f83f126204) |
| key_name                             | -                                                   |
| metadata                             | {}                                                  |
| name                                 | test1                                               |
| os-extended-volumes:volumes_attached | []                                                  |
| progress                             | 0                                                   |
| security_groups                      | default                                             |
| status                               | BUILD                                               |
| tenant_id                            | 2f7ee2716b3b4140be57b4a5b26401e3                    |
| updated                              | 2016-07-03T15:46:20Z                                |
| user_id                              | a2482a91a1f14750b372445d28b07c75                    |
+--------------------------------------+-----------------------------------------------------+
# nova list
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| ID                                   | Name  | Status | Task State | Power State | Networks            |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| b8aa46f9-186c-4594-8428-f8dbb16a5e16 | test1 | ACTIVE | -          | Running     | demo-net=10.0.20.11 |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
</pre></div>


<p>Ensure the instance gets IP and the metadata service is properly running</p>
<div class="highlight"><pre><span></span># nova console-log test1
...#Snipp from the output
Sending discover...
Sending select for 10.0.20.11...
Lease of 10.0.20.11 obtained, lease time 86400
cirros-ds &#39;net&#39; up at 7.92
checking http://169.254.169.254/2009-04-04/instance-id
successful after 1/20 tries: up 8.22. iid=i-0000000a
...
</pre></div>


<p>If you login to the instance through VNC you should be able to ping
another instances</p>
<p><strong>Edge router configuration</strong><br>
Create a new router</p>
<div class="highlight"><pre><span></span># neutron router-create edge-router
Created a new router:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| description           |                                      |
| external_gateway_info |                                      |
| id                    | 5ecadb64-cb0d-4f95-a00e-aa1dd20a2012 |
| name                  | edge-router                          |
| routes                |                                      |
| status                | ACTIVE                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
+-----------------------+--------------------------------------+
</pre></div>


<p>Attach the external subnet interface to the router</p>
<div class="highlight"><pre><span></span># neutron router-interface-add edge-router ext-subnet
Added interface e37f1986-c6b1-47f4-8268-02b837ceac17 to router edge-router.
</pre></div>


<p>Create an uplink network</p>
<div class="highlight"><pre><span></span># neutron net-create uplink-network --tenant_id admin --provider:network_type uplink
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:57:15                  |
| description           |                                      |
| id                    | 77173ed4-6106-4515-af1c-3683897955f9 |
| name                  | uplink-network                       |
| port_security_enabled | True                                 |
| provider:network_type | uplink                               |
| router:external       | False                                |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | admin                                |
| updated_at            | 2016-07-03T14:57:15                  |
+-----------------------+--------------------------------------+
</pre></div>


<p>Create a subnet in the uplink network</p>
<div class="highlight"><pre><span></span># neutron subnet-create --tenant_id admin --disable-dhcp --name uplink-subnet uplink-network 192.168.1.0/24
Created a new subnet:
+-------------------+--------------------------------------------------+
| Field             | Value                                            |
+-------------------+--------------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;192.168.1.2&quot;, &quot;end&quot;: &quot;192.168.1.254&quot;} |
| cidr              | 192.168.1.0/24                                   |
| created_at        | 2016-07-03T15:06:28                              |
| description       |                                                  |
| dns_nameservers   |                                                  |
| enable_dhcp       | False                                            |
| gateway_ip        | 192.168.1.1                                      |
| host_routes       |                                                  |
| id                | 4e98e789-20d3-45fd-a3b5-9bcf02d8a832             |
| ip_version        | 4                                                |
| ipv6_address_mode |                                                  |
| ipv6_ra_mode      |                                                  |
| name              | uplink-subnet                                    |
| network_id        | 77173ed4-6106-4515-af1c-3683897955f9             |
| subnetpool_id     |                                                  |
| tenant_id         | admin                                            |
| updated_at        | 2016-07-03T15:06:28                              |
+-------------------+--------------------------------------------------+
</pre></div>


<p>Create a port for each of the gateway nodes, interface should match with
the NIC you want to use for binding the gateway nodes and a IP address
for the same purposes</p>
<div class="highlight"><pre><span></span># neutron port-create uplink-network --binding:host_id gateway1 --binding:profile type=dict interface_name=eth1 --fixed-ip ip_address=192.168.1.199
Created a new port:
+-----------------------+--------------------------------------------------------------------------------------+
| Field                 | Value                                                                                |
+-----------------------+--------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                 |
| allowed_address_pairs |                                                                                      |
| binding:host_id       | compute1                                                                             |
| binding:profile       | {&quot;interface_name&quot;: &quot;eth1&quot;}                                                           |
| binding:vif_details   | {&quot;port_filter&quot;: true}                                                                |
| binding:vif_type      | midonet                                                                              |
| binding:vnic_type     | normal                                                                               |
| created_at            | 2016-07-03T15:10:06                                                                  |
| description           |                                                                                      |
| device_id             |                                                                                      |
| device_owner          |                                                                                      |
| extra_dhcp_opts       |                                                                                      |
| fixed_ips             | {&quot;subnet_id&quot;: &quot;4e98e789-20d3-45fd-a3b5-9bcf02d8a832&quot;, &quot;ip_address&quot;: &quot;192.168.1.199&quot;} |
| id                    | 7b4f54dd-2b41-42ba-9c5c-cda4640dc550                                                 |
| mac_address           | fa:16:3e:44:a8:c9                                                                    |
| name                  |                                                                                      |
| network_id            | 77173ed4-6106-4515-af1c-3683897955f9                                                 |
| port_security_enabled | True                                                                                 |
| security_groups       | 0cf3e33e-dbd6-4b42-a0bd-6679b5eed4e1                                                 |
| status                | ACTIVE                                                                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3                                                     |
| updated_at            | 2016-07-03T15:10:06                                                                  |
+-----------------------+--------------------------------------------------------------------------------------+
</pre></div>


<p>Attach each of the ports to the edge router</p>
<div class="highlight"><pre><span></span># neutron router-interface-add edge-router port=7b4f54dd-2b41-42ba-9c5c-cda4640dc550
Added interface 7b4f54dd-2b41-42ba-9c5c-cda4640dc550 to router edge-router.
</pre></div>


<p>At this point you have to decide if use border routers with BGP enabled
or static routes.<br>
Use one of the following links to configure your use case:  </p>
<p><a href="https://docs.midonet.org/docs/latest/operations-guide/content/bgp_uplink_configuration.html">https://docs.midonet.org/docs/latest/operations-guide/content/bgp_uplink_configuration.html</a>  </p>
<p><a href="https://docs.midonet.org/docs/latest/operations-guide/content/static_setup.html">https://docs.midonet.org/docs/latest/operations-guide/content/static_setup.html</a></p>
<p><strong>Issues I faced during configuration of Midonet</strong></p>
<p>Midolman agent don't start:<br>
It was caused because midolman-env.sh file has more RAM configured as
the one of my server.<br>
Edit the file to match your server resources</p>
<div class="highlight"><pre><span></span># egrep ^MAX_HEAP_SIZE /etc/midolman/midolman-env.sh
MAX_HEAP_SIZE=&quot;2048M&quot;
</pre></div>


<p>Instances doesn't boot with the following error:</p>
<div class="highlight"><pre><span></span>could not open /dev/net/tun: Permission denied
</pre></div>


<p>I had to remove br-tun bridges at ovs, if not, ovs locks the device and
midolman cannot create the tunnel beetwen compute nodes and gateway
nodes.</p>
<div class="highlight"><pre><span></span>ovs-vsctl del-br br-tun
</pre></div>


<p>This post is my experience integrating Midonet into OpenStack, maybe
some things are not correct, if you find any issue, please advise me to
fix it.<br>
Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/midonet-integration-with-openstack-mitaka.html">posted at 21:48</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/linux-openstack-various.html" rel="tag">Linux, OpenStack, Various</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/cloud.html" class="tags">cloud</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/config.html" class="tags">config</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/install.html" class="tags">install</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/integration.html" class="tags">integration</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/manual.html" class="tags">manual</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/midokura.html" class="tags">midokura</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/midonet.html" class="tags">midonet</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/mitaka.html" class="tags">mitaka</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/network.html" class="tags">network</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/packages.html" class="tags">packages</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/packstack.html" class="tags">packstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rdo.html" class="tags">rdo</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/sdn.html" class="tags">sdn</a>
                </div>
		<a href="https://egonzalez90.github.io/midonet-integration-with-openstack-mitaka.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Oct 01, 2014</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/fix-bash-shellshock-en-diferentes-so-debian-rhel-y-solaris-shellshock.html" rel="bookmark" title="Permanent Link to &quot;Fix Bash (ShellShock) en diferentes SO: Debian, RHEL y Solaris&quot;">Fix Bash (ShellShock) en diferentes SO: Debian, RHEL y Solaris</a>
                </h2>

                
                

                <p>Veremos la forma de instalar los parches de Bash en diferentes sistemas
Linux<!--more--></p>
<h1>Debian 6</h1>
<p>Primero deberemos añadir el repositorio LTS. Lo haremos añadiendo esta
línea a el archivo siguiente archivo</p>
<blockquote>
<p>vi /etc/apt/sources.list/</p>
<p>deb http://ftp.us.debian.org/debian squeeze-lts main non-free contrib</p>
</blockquote>
<p>A continuación importaremos la clave del servidor de Debian</p>
<blockquote>
<p>gpg --keyserver pgpkeys.mit.edu --recv-key 8B48AD6246925553</p>
</blockquote>
<p>La exportaremos y añadiremos para que al actualizar la reconozca, es
posible que estos pasos no sean necesarios, pero normalmente suele dar
el error de la clave</p>
<blockquote>
<p>gpg -a --export 8B48AD6246925553 | apt-key add –</p>
</blockquote>
<p>A partir de aquí será lo igual que con Debian 7</p>
<h1>Debian 7</h1>
<p>Actualizaremos la lista de repositorios</p>
<blockquote>
<p>apt-get update</p>
</blockquote>
<p>Lo siguiente es actualizar solo el paquete bash</p>
<blockquote>
<p>apt-get install --only-upgrade bash</p>
</blockquote>
<h1>RHEL-CentOS 5, 6 y 7</h1>
<p>Directamente actualizando el paquete bash</p>
<blockquote>
<p>yum update bash</p>
</blockquote>
<p>O instalándolo desde el paquete .rpm disponible en la página de RHEL, o
en los repositorios de CentOS, Scientific Linux, etc</p>
<blockquote>
<p>rpm –Uvh paquetebash.rpm</p>
</blockquote>
<p>Puedes comprobar que esta la ultima versión instalada viendo la fecha y
la versión del paquete mediante</p>
<blockquote>
<p>rpm –qi bash</p>
</blockquote>
<h1>Oracle Linux 6</h1>
<p>En Oracle Linux primero deberemos añadir los repositorios de paquetes
públicos, para ello iremos a la siguiente ruta.</p>
<blockquote>
<p>cd /etc/yum.repos.d</p>
</blockquote>
<p>A continuación descargaremos el repositorio público dentro de la misma
carpeta:</p>
<blockquote>
<p>wget http://public-yum.oracle.com/public-yum-ol6.repo</p>
</blockquote>
<p>Por ultimo actualizaremos bash a su última versión disponible</p>
<blockquote>
<p>yum update bash</p>
</blockquote>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/fix-bash-shellshock-en-diferentes-so-debian-rhel-y-solaris-shellshock.html">posted at 17:20</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/linux-openstack-various.html" rel="tag">Linux, OpenStack, Various</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/actualizar.html" class="tags">actualizar</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/bash.html" class="tags">bash</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/bug.html" class="tags">bug</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/centos.html" class="tags">centos</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/debian.html" class="tags">Debian</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/fix.html" class="tags">fix</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/oracle.html" class="tags">oracle</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/red-hat.html" class="tags">red hat</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rhel.html" class="tags">rhel</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/shellshock.html" class="tags">shellshock</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/solaris.html" class="tags">solaris</a>
                </div>
		<a href="https://egonzalez90.github.io/fix-bash-shellshock-en-diferentes-so-debian-rhel-y-solaris-shellshock.html#disqus_thread">Click to read and post comments</a>
            </article>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="https://egonzalez90.github.io/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>