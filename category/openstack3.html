<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenStack Stuff | articles in the "OpenStack" category | Page 3</title>
    <link rel="shortcut icon" type="image/png" href="https://egonzalez90.github.io/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="https://egonzalez90.github.io/favicon.ico">
    <link href="https://egonzalez90.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="OpenStack Stuff Full Atom Feed" />
    <link href="https://egonzalez90.github.io/feeds/openstack.atom.xml" type="application/atom+xml" rel="alternate" title="OpenStack Stuff Categories Atom Feed" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Eduardo Gonzalez" />
</head>
<body>
    <header>
        <nav>
            <ul>

                <li class="ephemeral selected"><a href="https://egonzalez90.github.io/category/openstack3.html">OpenStack</a></li>
                <li><a href="https://egonzalez90.github.io/">Home</a></li>
                <li><a href="https://docs.openstack.org">OpenStack</a></li>
                <li><a href="https://www.linkedin.com/in/eduardogonzalezgutierrez">Linkedin</a></li>
                <li><a href="https://github.com/egonzalez90">Github</a></li>
                <li><a href="https://egonzalez90.github.io/archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="https://egonzalez90.github.io/">OpenStack Stuff</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Feb 02, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/migrate-from-keystone-v2-0-to-keystone-v3-openstack-liberty.html" rel="bookmark" title="Permanent Link to &quot;Migrate from keystone v2.0 to keystone v3 OpenStack Liberty&quot;">Migrate from keystone v2.0 to keystone v3 OpenStack Liberty</a>
                </h2>

                
                

                <p>Migrate from keystone v2.0 to v3 isn't as easy like just changing the
endpoints at the database, every service must be configured to
authenticate against keystone v3.</p>
<p>I've been working on that the past few days looking for a method, with
the purpose of facilitate operators life's who need this kind of
migration.<br>
I have to thank Adam Young work, i followed his blog to make a first
configuration idea, after that, i configured all core services to make
use of keystone v3.<br>
If you want to check Adam's blog, follow this link:
<a href="http://adam.younglogic.com/2015/05/rdo-v3-only/">http://adam.younglogic.com/2015/05/rdo-v3-only/</a></p>
<p>I used OpenStack Liberty installed with RDO packstack over CentOS 7
servers.<br>
The example IP used is <code>192.168.200.168</code>, use your own according your
needs.<br>
Password used for all services is <code>PASSWD1234</code>, use your own password,
you can locate your passwords at the packstack answer file.</p>
<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Horizon</strong></ins></p>
<p>First we configure Horizon with keystone v3 as below:</p>
<div class="highlight"><pre><span></span>vi /etc/openstack-dashboard/local_settings

OPENSTACK_API_VERSIONS = {
    &quot;identity&quot;: 3
}

OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &#39;Default&#39;
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>keystone</strong></ins></p>
<p>Check your current identity endpoints</p>
<div class="highlight"><pre><span></span>mysql  --user keystone_admin --password=PASSWD1234  keystone -e &quot;select interface, url from endpoint where service_id =  (select id from service where service.type = &#39;identity&#39;);&quot;
</pre></div>


<p>Change your public, admin and internal endpoints with v3 at the end,
instead of v2.0</p>
<div class="highlight"><pre><span></span><span class="nt">mysql</span>  <span class="nt">--user</span> <span class="nt">keystone_admin</span> <span class="nt">--password</span><span class="o">=</span><span class="nt">PASSWD1234</span>   <span class="nt">keystone</span> <span class="nt">-e</span> <span class="s2">&quot;update endpoint set   url  = &#39;http://192.168.200.178:5000/v3&#39; where  interface =&#39;internal&#39; and  service_id =  (select id from service where service.type = &#39;identity&#39;);&quot;</span>

<span class="nt">mysql</span>  <span class="nt">--user</span> <span class="nt">keystone_admin</span> <span class="nt">--password</span><span class="o">=</span><span class="nt">PASSWD1234</span>   <span class="nt">keystone</span> <span class="nt">-e</span> <span class="s2">&quot;update endpoint set   url  = &#39;http://192.168.200.178:5000/v3&#39; where  interface =&#39;public&#39; and  service_id =  (select id from service where service.type = &#39;identity&#39;);&quot;</span>

<span class="nt">mysql</span>  <span class="nt">--user</span> <span class="nt">keystone_admin</span> <span class="nt">--password</span><span class="o">=</span><span class="nt">PASSWD1234</span>   <span class="nt">keystone</span> <span class="nt">-e</span> <span class="s2">&quot;update endpoint set   url  = &#39;http://192.168.200.178:35357/v3&#39; where  interface =&#39;admin&#39; and  service_id =  (select id from service where service.type = &#39;identity&#39;);&quot;</span>
</pre></div>


<p>Ensure the endpoints are properly created</p>
<div class="highlight"><pre><span></span>mysql  --user keystone_admin --password=KEYSTONE_DB_PW   keystone -e &quot;select interface, url from endpoint where service_id =  (select id from service where service.type = &#39;identity&#39;);&quot;
</pre></div>


<p>Create a source file or edit keystonerc_admin with the following data</p>
<div class="highlight"><pre><span></span>vi v3_keystone

unset OS_SERVICE_TOKEN
export OS_USERNAME=admin
export OS_PASSWORD=PASSWD1234
export OS_AUTH_URL=http://192.168.200.178:5000/v3
export OS_PROJECT_NAME=admin
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USER_DOMAIN_NAME=Default
export OS_REGION_NAME=RegionOne
export PS1=&#39;[\u@\h \W(keystone_admin)]\$ &#39;
export OS_IDENTITY_API_VERSION=3
</pre></div>


<p>Comment both pipelines, in public_api and admin_api</p>
<div class="highlight"><pre><span></span>vi /usr/share/keystone/keystone-dist-paste.ini

[pipeline:public_api]
# The last item in this pipeline must be public_service or an equivalent
# application. It cannot be a filter.
#pipeline = sizelimit url_normalize request_id build_auth_context token_auth admin_token_auth json_body ec2_extension user_crud_extension public_service

[pipeline:admin_api]
# The last item in this pipeline must be admin_service or an equivalent
# application. It cannot be a filter.
#pipeline = sizelimit url_normalize request_id build_auth_context token_auth admin_token_auth json_body ec2_extension s3_extension crud_extension admin_service
</pre></div>


<p>Comment v2.0 entries in composite:main and admin sections.</p>
<div class="highlight"><pre><span></span><span class="k">[composite:main]</span>
<span class="na">use</span> <span class="o">=</span> <span class="s">egg:Paste#urlmap</span>
<span class="c1">#/v2.0 = public_api</span>
<span class="na">/v3</span> <span class="o">=</span> <span class="s">api_v3</span>
<span class="na">/</span> <span class="o">=</span> <span class="s">public_version_api</span>

<span class="k">[composite:admin]</span>
<span class="na">use</span> <span class="o">=</span> <span class="s">egg:Paste#urlmap</span>
<span class="c1">#/v2.0 = admin_api</span>
<span class="na">/v3</span> <span class="o">=</span> <span class="s">api_v3</span>
<span class="na">/</span> <span class="o">=</span> <span class="s">admin_version_api</span>
</pre></div>


<p>Restart httpd to apply changes</p>
<div class="highlight"><pre><span></span>systemctl restart httpd
</pre></div>


<p>Check whether keystone and horizon are properly working<br>
The command below should prompt an user list, if not, check
configuration in previous steps</p>
<div class="highlight"><pre><span></span>openstack user list
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Glance</strong></ins></p>
<p>Edit the following files, with the content below:</p>
<div class="highlight"><pre><span></span>vi /etc/glance/glance-api.conf 
vi /etc/glance/glance-registry.conf 
vi /etc/glance/glance-cache.conf

[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = glance
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre></div>


<p>Comment the following lines:</p>
<div class="highlight"><pre><span></span>#auth_host=127.0.0.1
#auth_port=35357
#auth_protocol=http
#identity_uri=http://192.168.200.178:35357
#admin_user=glance
#admin_password=PASSWD1234
#admin_tenant_name=services
</pre></div>


<p>Those lines, should be commented in all the other OpenStack core
services at keystone_authtoken section</p>
<p>Edit the files below and comment the lines inside keystone_authtoken
section.</p>
<div class="highlight"><pre><span></span>vi /usr/share/glance/glance-api-dist.conf 
vi /usr/share/glance/glance-registry-dist.conf

[keystone_authtoken]
#admin_tenant_name = %SERVICE_TENANT_NAME%
#admin_user = %SERVICE_USER%
#admin_password = %SERVICE_PASSWORD%
#auth_host = 127.0.0.1
#auth_port = 35357
#auth_protocol = http
</pre></div>


<p>Restart glance services</p>
<div class="highlight"><pre><span></span>openstack-service restart glance
</pre></div>


<p>Ensure glance service is working</p>
<div class="highlight"><pre><span></span>openstack image list
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Nova</strong></ins></p>
<p>Edit the file below and comment the lines inside keystone_authtoken</p>
<div class="highlight"><pre><span></span>vi /usr/share/nova/nova-dist.conf

[keystone_authtoken]
#auth_host = 127.0.0.1
#auth_port = 35357
#auth_protocol = http
</pre></div>


<p>Edit nova.conf and add the auth content inside keystone_authtoken,
don't forget to comment the lines related to the last auth method, which
were commented in glance section.</p>
<div class="highlight"><pre><span></span>vi /etc/nova/nova.conf

[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = nova
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre></div>


<p>Configure nova authentication against neutron</p>
<div class="highlight"><pre><span></span><span class="k">[neutron]</span>

<span class="na">auth_plugin</span> <span class="o">=</span> <span class="s">password</span>
<span class="na">auth_url</span> <span class="o">=</span> <span class="s">http://192.168.200.178:35357</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">neutron</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">PASSWD1234</span>
<span class="na">project_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">user_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">project_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">auth_uri</span><span class="o">=</span><span class="s">http://192.168.200.178:5000</span>
</pre></div>


<p>Restart nova services to apply changes</p>
<div class="highlight"><pre><span></span>openstack-service restart nova
</pre></div>


<p>Check if nova works</p>
<div class="highlight"><pre><span></span>openstack hypervisor list
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Neutron</strong></ins></p>
<p>Comment or remove the following entries at api-paste.ini and add the new
version auth lines</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/api-paste.ini

[filter:authtoken]
#identity_uri=http://192.168.200.178:35357
#admin_user=neutron
#admin_password=PASSWD1234
#auth_uri=http://192.168.200.178:5000/v2.0
#admin_tenant_name=services

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = neutron
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre></div>


<p>Configure v3 authentication for metadata service, remember comment the
old auth lines</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/metadata_agent.ini

[DEFAULT]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = neutron
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre></div>


<p>Configure neutron server with v3 auth</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/neutron.conf

nova_admin_auth_url = http://192.168.200.178:5000
# nova_admin_tenant_id =1fb93c84c6474c5ea92c0ed5f7d4a6a7
nova_admin_tenant_name = services


[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = neutron
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000

#auth_uri = http://192.168.200.178:5000/v2.0
#identity_uri = http://192.168.200.178:35357
#admin_tenant_name = services
#admin_user = neutron
#admin_password = PASSWD1234
</pre></div>


<p>Configure neutron auth against nova services</p>
<div class="highlight"><pre><span></span><span class="k">[nova]</span>

<span class="na">auth_plugin</span> <span class="o">=</span> <span class="s">password</span>
<span class="na">auth_url</span> <span class="o">=</span> <span class="s">http://192.168.200.178:35357</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">nova</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">PASSWD1234</span>
<span class="na">project_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">user_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">project_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">auth_uri</span><span class="o">=</span><span class="s">http://192.168.200.178:5000</span>
</pre></div>


<p>Restart neutron services to apply changes</p>
<div class="highlight"><pre><span></span>openstack-service restart neutron
</pre></div>


<p>Test correct neutron funtionality</p>
<div class="highlight"><pre><span></span>openstack network list
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Cinder</strong></ins></p>
<p>Edit api-paste.ini with the following content</p>
<div class="highlight"><pre><span></span>vi /etc/cinder/api-paste.ini

[filter:authtoken]
paste.filter_factory = keystonemiddleware.auth_token:filter_factory
auth_plugin = password
auth_url = http://192.168.200.178:35357
username = cinder
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
#admin_tenant_name=services
#auth_uri=http://192.168.200.178:5000/v2.0
#admin_user=cinder
#identity_uri=http://192.168.200.178:35357
#admin_password=PASSWD1234
</pre></div>


<p>Restart cinder services to apply changes</p>
<div class="highlight"><pre><span></span>openstack-service restart cinder
</pre></div>


<p>Ensure cinder is properly running</p>
<div class="highlight"><pre><span></span>openstack volume create --size 1 testvolume
openstack volume list
</pre></div>


<p>Now, you can check if nova is working fine, create an instance and
ensure it is in ACTIVE state.</p>
<div class="highlight"><pre><span></span>openstack server create --flavor m1.tiny --image cirros --nic net-id=a1aa6336-9ae2-4ffb-99f5-1b6d1130989c testinstance
openstack server list
</pre></div>


<p>If any error occurs, review configuration files</p>
<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Swift</strong></ins></p>
<p>Configure proxy server auth agains keystone v3</p>
<div class="highlight"><pre><span></span>vi /etc/swift/proxy-server.conf

[filter:authtoken]
log_name = swift
signing_dir = /var/cache/swift
paste.filter_factory = keystonemiddleware.auth_token:filter_factory
auth_plugin = password
auth_url = http://192.168.200.178:35357
username = swift
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000

#auth_uri = http://192.168.200.178:5000/v2.0
#identity_uri = http://192.168.200.178:35357
#admin_tenant_name = services
#admin_user = swift
#admin_password = PASSWD1234
delay_auth_decision = 1
cache = swift.cache
include_service_catalog = False
</pre></div>


<p>Restart swift services to apply changes</p>
<div class="highlight"><pre><span></span>openstack-service restart swift
</pre></div>


<p>Swift commands must be issued with python-openstackclient instead of
swiftclient<br>
If done with swiftclient a -V 3 option must be used in order to avoid
issues</p>
<p>Check if swift works fine</p>
<div class="highlight"><pre><span></span>openstack container create testcontainer
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Ceilometer</strong></ins></p>
<p>Configure ceilometer service in order to authenticate agains keystone v3</p>
<div class="highlight"><pre><span></span><span class="k">[keystone_authtoken]</span>

<span class="na">auth_plugin</span> <span class="o">=</span> <span class="s">password</span>
<span class="na">auth_url</span> <span class="o">=</span> <span class="s">http://192.168.200.178:35357</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">ceilometer</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">PASSWD1234</span>
<span class="na">project_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">user_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">project_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">auth_uri</span><span class="o">=</span><span class="s">http://192.168.200.178:5000</span>

<span class="k">[service_credentials]</span>

<span class="na">os_auth_url</span> <span class="o">=</span> <span class="s">http://controller:5000/v3</span>
<span class="na">os_username</span> <span class="o">=</span> <span class="s">ceilometer</span>
<span class="na">os_tenant_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">os_password</span> <span class="o">=</span> <span class="s">PASSWD1234</span>
<span class="na">os_endpoint_type</span> <span class="o">=</span> <span class="s">internalURL</span>
<span class="na">os_region_name</span> <span class="o">=</span> <span class="s">RegionOne</span>
</pre></div>


<p>Restart ceilometer services</p>
<div class="highlight"><pre><span></span>openstack-service restart ceilometer
</pre></div>


<p>Check ceilometer funtionality</p>
<div class="highlight"><pre><span></span>ceilometer statistics -m memory
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Heat</strong></ins></p>
<p>Configure Heat authentication, since trusts are not stable use password
auth method</p>
<div class="highlight"><pre><span></span>vi /etc/heat/heat.conf

# Allowed values: password, trusts
#deferred_auth_method = trusts
deferred_auth_method = password
</pre></div>


<p>Configure auth_uri and keystone_authtoken section</p>
<div class="highlight"><pre><span></span># From heat.common.config
#
# Unversioned keystone url in format like http://0.0.0.0:5000. (string value)
#auth_uri =
auth_uri = http://192.168.200.178:5000

[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = heat
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000

#admin_user=heat
#admin_password=PASSWD1234
#admin_tenant_name=services
#identity_uri=http://192.168.200.178:35357
#auth_uri=http://192.168.200.178:5000/v2.0
</pre></div>


<p>Comment or remove heat-dist auth entries in order to avoid conflicts
with your config files</p>
<div class="highlight"><pre><span></span>vi /usr/share/heat/heat-dist.conf

[keystone_authtoken]
#auth_host = 127.0.0.1
#auth_port = 35357
#auth_protocol = http
#auth_uri = http://127.0.0.1:5000/v2.0
#signing_dir = /tmp/keystone-signing-heat
</pre></div>


<p>Restart heat services to apply changes</p>
<div class="highlight"><pre><span></span>openstack-service restart heat
</pre></div>


<p>Ensure heat authentication is properly configured with a simple heat
template</p>
<div class="highlight"><pre><span></span>heat stack-create --template-file sample.yaml teststack
</pre></div>


<p>Most issues occurs in the authentication between nova and neutron
services, if instances does not launch as expected, review [nova] and
[neutron] sections.</p>
<p>Best regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/migrate-from-keystone-v2-0-to-keystone-v3-openstack-liberty.html">posted at 19:43</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/openstack.html" rel="tag">OpenStack</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/auth.html" class="tags">auth</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/auth_url.html" class="tags">auth_url</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/core.html" class="tags">core</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/guide.html" class="tags">guide</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/how-to.html" class="tags">how to</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/keystone.html" class="tags">keystone</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/migrate.html" class="tags">migrate</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/packstack.html" class="tags">packstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rdo.html" class="tags">rdo</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/services.html" class="tags">services</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/tutorial.html" class="tags">Tutorial</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/v20.html" class="tags">v2.0</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/v3.html" class="tags">v3</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/version.html" class="tags">version</a>
                </div>
		<a href="https://egonzalez90.github.io/migrate-from-keystone-v2-0-to-keystone-v3-openstack-liberty.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Jan 27, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/configure-neutron-dvr-openstack-liberty.html" rel="bookmark" title="Permanent Link to &quot;Configure Neutron DVR OpenStack Liberty&quot;">Configure Neutron DVR OpenStack Liberty</a>
                </h2>

                
                

                <p>Distributed Virtual Routers aka DVR were created to avoid single point
of failure on neutron nodes.<br>
When using standard routers, all the traffic is passing out through
Neutron servers. Inside network servers, router namespaces are created
routing all traffic and NAT forwarding between instances and public
networks. When a network node falls down, instance traffic will no
longer be available until a new namespace is created and executed in
another network node.<br>
Distributed routers is a way to avoid the SPOF neutron nodes were. When
using DVR, router namespaces, are directly created inside compute nodes
where all instance and l3 traffic are routed.</p>
<p>If you want to know more about DVR check this awesome links:  </p>
<p><a href="http://blog.gampel.net/2014/12/openstack-neutron-distributed-virtual.html">http://blog.gampel.net/2014/12/openstack-neutron-distributed-virtual.html</a><br>
<a href="http://blog.gampel.net/2014/12/openstack-dvr2-floating-ips.html">http://blog.gampel.net/2014/12/openstack-dvr2-floating-ips.html</a><br>
<a href="http://blog.gampel.net/2015/01/openstack-DVR-SNAT.html">http://blog.gampel.net/2015/01/openstack-DVR-SNAT.html</a></p>
<p>A previous OpenStack Liberty installation is required, mine was done
with RDO packstack.</p>
<p><strong><ins datetime="2016-01-27T18:47:58+00:00">Configure all Neutron
Servers</ins></strong></p>
<p>Edit ml2 configuration file with the following:</p>
<div class="highlight"><pre><span></span># vi /etc/neutron/plugins/ml2/ml2_conf.ini

mechanism_drivers = openvswitch,l2population
type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
vni_ranges = 10:100
vxlan_group = 224.1.1.1
enable_security_group = True
</pre></div>


<p>Edit neutron configuration file, enable DVR and uncomment dvr_base_mac
option</p>
<div class="highlight"><pre><span></span># vi /etc/neutron/neutron.conf

router_distributed = True
dvr_base_mac = fa:16:3f:00:00:00
</pre></div>


<p>Configure l3 agent to use dvr_snat</p>
<div class="highlight"><pre><span></span># vi /etc/neutron/l3_agent.ini

agent_mode = dvr_snat
</pre></div>


<p>Restart neutron server</p>
<div class="highlight"><pre><span></span>systemctl restart neutron-server
</pre></div>


<p><strong><ins datetime="2016-01-27T18:47:58+00:00">Configure all Compute
Nodes</ins></strong></p>
<p>Install ml2 package</p>
<div class="highlight"><pre><span></span>yum install openstack-neutron-ml2
</pre></div>


<p>Edit openvswitch agent file as below:</p>
<div class="highlight"><pre><span></span># vi /etc/neutron/plugins/ml2/openvswitch_agent.ini

l2_population = True
arp_responder = True
enable_distributed_routing = True
</pre></div>


<p>Enable DVR and select an interface driver to be used by l3 agent</p>
<div class="highlight"><pre><span></span># vi /etc/neutron/l3_agent.ini

interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver
agent_mode = dvr
</pre></div>


<p>Edit ml2 configuration file as below:</p>
<div class="highlight"><pre><span></span># vi /etc/neutron/plugins/ml2/ml2_conf.ini

type_drivers = flat,vlan,vxlan
tenant_network_types = vxlan
mechanism_drivers = openvswitch,l2population
vni_ranges = 10:100
vxlan_group = 224.1.1.1
enable_security_group = True
</pre></div>


<p>Start and enable metadata agent in compute nodes</p>
<div class="highlight"><pre><span></span>systemctl start neutron-l3-agent neutron-metadata-agent
systemctl enable neutron-l3-agent neutron-metadata-agent
</pre></div>


<p>Create an external bridge with an external IP associated on it</p>
<div class="highlight"><pre><span></span># vi /etc/sysconfig/network-scripts/ifcfg-br-ex

DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
BOOTPROTO=static
IPADDR=192.168.100.4                                                          
NETMASK=255.255.255.0
GATEWAY=192.168.100.1
ONBOOT=yes
</pre></div>


<p>Modify an unused interface connected with the same network as the IP
configured with br-ex, edit the interface to be used as OVS port by
br-ex</p>
<div class="highlight"><pre><span></span># vi /etc/sysconfig/network-scripts/ifcfg-eth1
DEVICE=eth1
TYPE=OVSPort
DEVICETYPE=ovs
OVS_BRIDGE=br-ex
ONBOOT=yes
</pre></div>


<p>Restart network service to apply changes on the interfaces and
openvswith-agent</p>
<div class="highlight"><pre><span></span>systemctl restart network
systemctl restart neutron-openvswitch-agent
</pre></div>


<p>Create an external network and a subnet on it</p>
<div class="highlight"><pre><span></span>neutron net-create external_network --provider:network_type flat --provider:physical_network extnet  --router:external --shared
neutron subnet-create --name public_subnet --enable_dhcp=False --allocation-pool=start=192.168.100.100,end=192.168.100.150 --gateway=192.168.100.1 external_network 192.168.100.0/24
</pre></div>


<p>Create a router and associate external network as router gateway</p>
<div class="highlight"><pre><span></span>neutron router-create router1
neutron router-gateway-set router1 external_network
</pre></div>


<p>Create an internal network, a subnet and associate an interface to the
router</p>
<div class="highlight"><pre><span></span>neutron net-create private_network
neutron subnet-create --name private_subnet private_network 10.0.1.0/24
neutron router-interface-add router1 private_subnet
</pre></div>


<p>Boot 2 instances</p>
<div class="highlight"><pre><span></span>nova boot --flavor m1.tiny --image cirros --nic net-id=154da7a8-fa49-415e-9d35-c840b144a8df test1
nova boot --flavor m1.tiny --image cirros --nic net-id=154da7a8-fa49-415e-9d35-c840b144a8df test2
</pre></div>


<p>Create 2 floating ips and associate it to instances</p>
<div class="highlight"><pre><span></span>neutron floatingip-create external_network
neutron floatingip-create external_network
nova floating-ip-associate test1 192.168.100.101
nova floating-ip-associate test2 192.168.100.102
</pre></div>


<p>Test if all works as expected pinging floating ips</p>
<div class="highlight"><pre><span></span># ping 192.168.100.101
# ping 192.168.100.102
</pre></div>


<p>As you can see, in network nodes, a snat namespace is created</p>
<div class="highlight"><pre><span></span># sudo ip netns
qdhcp-154da7a8-fa49-415e-9d35-c840b144a8df
snat-77fef58a-6d0c-4e96-b4b6-5d8e81ebead3
</pre></div>


<p>In compute nodes, a fip namespace per instance with floating ip
associated running on the compute node are created and a qrouter
namespace are created.</p>
<div class="highlight"><pre><span></span># sudo ip netns
fip-4dfdabb0-d2d6-4d4a-8c00-84df834eec8b
qrouter-77fef58a-6d0c-4e96-b4b6-5d8e81ebead3
</pre></div>


<p>Best regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/configure-neutron-dvr-openstack-liberty.html">posted at 20:13</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/openstack.html" rel="tag">OpenStack</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/-configure.html" class="tags">--configure</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/distributed.html" class="tags">distributed</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/dvr.html" class="tags">dvr</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/fip.html" class="tags">fip</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/l3.html" class="tags">l3</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/liberty.html" class="tags">liberty</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/neutron.html" class="tags">neutron</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/packstack.html" class="tags">packstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/qrouter.html" class="tags">qrouter</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rdo.html" class="tags">rdo</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/routers.html" class="tags">routers</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/snat.html" class="tags">snat</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/virtual.html" class="tags">virtual</a>
                </div>
		<a href="https://egonzalez90.github.io/configure-neutron-dvr-openstack-liberty.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Jan 14, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/openstack-segregation-with-availability-zones-and-host-aggregates.html" rel="bookmark" title="Permanent Link to &quot;OpenStack segregation with Availability Zones and Host Aggregates&quot;">OpenStack segregation with Availability Zones and Host Aggregates</a>
                </h2>

                
                

                <p>When a new OpenStack cloud born, usually all servers run over the same
hardware and specifications, often, all servers are in the same
building, room, rack, even a chassis when the cloud is in the first
growth paces.</p>
<p>After a while, workloads increase and the current hardware is not enough
to process that workloads. At this point, your hardware is old and new
hardware is bought. This hardware has different storage disks, CPU, RAM
and so on. You passed from 10's of servers to 100's. DataCenter racks,
rooms and buildings are too small and the growing cloud needs redundancy
between cities or countries.</p>
<p>OpenStack offers a few solutions for that purpose, called Regions,
Cells, Availability Zones and Host Aggregates.<br>
Now, we are going to focus on Availability Zones and Host Aggregates,
which are the way to segregate computational workloads.</p>
<ul>
<li>Host Aggregates:<ul>
<li>Host Aggregates represent a logical set of
    properties/characteristics a group of hosts owns in the form of
    metadata. Imagine some of your servers have SSD disks and the
    other ones SATA, you can map those properties SSD/SATA to a
    group of hosts, when a image or flavor with the metaparameter
    associated is launched, Nova Scheduler will filter the available
    hosts with the meta parameter value and boot the instance on
    hosts with the desired property. Host Aggregates are managed by
    OpenStack admins.</li>
</ul>
</li>
<li>Availability Zones<ul>
<li>Availability Zones represent a logical partition of the
    infrastructure(not necessary but is the common use case) in the
    form of racks, rooms, buildings, etc. Customers can launch
    instances in the desired Availability Zone.</li>
</ul>
</li>
</ul>
<p>Usually, Host Aggregates are mapped to Availability Zones allowing
customers to use the desired set of hardware or characteristics to boot
instances.</p>
<p>At the end of this guide you will know how to:</p>
<ol>
<li>Create Availability Zones and Host Aggregates.</li>
<li>Adding hosts to Host Aggregates and Availability Zones.</li>
<li>Launch instances directly to Availability Zones.</li>
<li>Configure nova scheduler for Host Aggregates usage.</li>
<li>Configure Images and Flavors for scheduling to Host Aggregates.</li>
<li>Launch instances based on flavors and image parameters.</li>
</ol>
<p>Let's start: \00/</p>
<p>Create two Host Aggregate called <code>"az1-ag"/"az2-ag"</code>, this command also,
will create two Availability Zones called <code>"az1"/"az2"</code>.<br>
By default, when a Host Aggregate is created with an Availability Zone,
a metadata key called <code>"availability_zone=NAME_OF_AZ</code>" will be created.</p>
<div class="highlight"><pre><span></span># nova aggregate-create az1-ag az1
+----+--------+-------------------+-------+-------------------------+
| Id | Name   | Availability Zone | Hosts | Metadata                |
+----+--------+-------------------+-------+-------------------------+
| 2  | az1-ag | az1               |       | &#39;availability_zone=az1&#39; |
+----+--------+-------------------+-------+-------------------------+
# nova aggregate-create az2-ag az2
+----+--------+-------------------+-------+-------------------------+
| Id | Name   | Availability Zone | Hosts | Metadata                |
+----+--------+-------------------+-------+-------------------------+
| 3  | az2-ag | az2               |       | &#39;availability_zone=az2&#39; |
+----+--------+-------------------+-------+-------------------------+
</pre></div>


<p>Add one or more compute nodes to Host Aggregates.</p>
<div class="highlight"><pre><span></span># nova aggregate-add-host 2 compute1az
Host compute1az has been successfully added for aggregate 2 
+----+--------+-------------------+--------------+-------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                |
+----+--------+-------------------+--------------+-------------------------+
| 2  | az1-ag | az1               | &#39;compute1az&#39; | &#39;availability_zone=az1&#39; |
+----+--------+-------------------+--------------+-------------------------+
# nova aggregate-add-host 3 compute2az
Host compute2az has been successfully added for aggregate 3 
+----+--------+-------------------+--------------+-------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                |
+----+--------+-------------------+--------------+-------------------------+
| 3  | az2-ag | az2               | &#39;compute2az&#39; | &#39;availability_zone=az2&#39; |
+----+--------+-------------------+--------------+-------------------------+
</pre></div>


<p>Details about a Host Aggregate can be reviewed with:</p>
<div class="highlight"><pre><span></span># nova aggregate-details az1-ag
+----+--------+-------------------+--------------+-------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                |
+----+--------+-------------------+--------------+-------------------------+
| 2  | az1-ag | az1               | &#39;compute1az&#39; | &#39;availability_zone=az1&#39; |
+----+--------+-------------------+--------------+-------------------------+
# nova aggregate-details az2-ag
+----+--------+-------------------+--------------+-------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                |
+----+--------+-------------------+--------------+-------------------------+
| 3  | az2-ag | az2               | &#39;compute2az&#39; | &#39;availability_zone=az2&#39; |
+----+--------+-------------------+--------------+-------------------------+
</pre></div>


<p>List Availability Zones and check status.</p>
<div class="highlight"><pre><span></span><span class="c1"># nova availability-zone-list</span>
<span class="s s-Atom">+-----------------------+----------------------------------------+</span>
<span class="p">|</span> <span class="nv">Name</span>                  <span class="p">|</span> <span class="nv">Status</span>                                 <span class="p">|</span>
<span class="s s-Atom">+-----------------------+----------------------------------------+</span>
<span class="p">|</span> <span class="s s-Atom">internal</span>              <span class="p">|</span> <span class="s s-Atom">available</span>                              <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">controlleraz</span>       <span class="p">|</span>                                        <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">conductor</span>   <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">16.000000</span> <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">consoleauth</span> <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">16.000000</span> <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">scheduler</span>   <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">16.000000</span> <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">cert</span>        <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">13.000000</span> <span class="p">|</span>
<span class="p">|</span> <span class="s s-Atom">az2</span>                   <span class="p">|</span> <span class="s s-Atom">available</span>                              <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">compute2az</span>         <span class="p">|</span>                                        <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">compute</span>     <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">12.000000</span> <span class="p">|</span>
<span class="p">|</span> <span class="s s-Atom">az1</span>                   <span class="p">|</span> <span class="s s-Atom">available</span>                              <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">compute1az</span>         <span class="p">|</span>                                        <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">compute</span>     <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">12.000000</span> <span class="p">|</span>
<span class="s s-Atom">+-----------------------+----------------------------------------+</span>
</pre></div>


<p>Other method you can use:</p>
<div class="highlight"><pre><span></span># nova service-list
+----+------------------+--------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host         | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-consoleauth | controlleraz | internal | enabled | up    | 2016-01-14T19:54:36.000000 | -               |
| 2  | nova-scheduler   | controlleraz | internal | enabled | up    | 2016-01-14T19:54:36.000000 | -               |
| 3  | nova-conductor   | controlleraz | internal | enabled | up    | 2016-01-14T19:54:35.000000 | -               |
| 4  | nova-cert        | controlleraz | internal | enabled | up    | 2016-01-14T19:54:33.000000 | -               |
| 5  | nova-compute     | compute2az   | az2      | enabled | up    | 2016-01-14T19:54:32.000000 | -               |
| 6  | nova-compute     | compute1az   | az1      | enabled | up    | 2016-01-14T19:54:32.000000 | -               |
+----+------------------+--------------+----------+---------+-------+----------------------------+-----------------+
</pre></div>


<p>Launch two instances using <code>"--availability-zone AZ</code>" option, you can
even select the compute node to use, just use
<code>"--availability-zone AZ:COMPUTE_NODE</code>".</p>
<div class="highlight"><pre><span></span># nova boot --flavor m1.tiny --image cirros --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 --availability-zone az1 instanceaz1
# nova boot --flavor m1.tiny --image cirros --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 --availability-zone az2 instanceaz2
</pre></div>


<p>Ensure the instances are running in the desired Availability Zone.</p>
<div class="highlight"><pre><span></span># nova show instanceaz1 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az1
# nova show instanceaz2 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az2
</pre></div>


<p>List Glance images.</p>
<div class="highlight"><pre><span></span># glance image-list
+--------------------------------------+----------+
| ID                                   | Name     |
+--------------------------------------+----------+
| a6d7a606-f725-480a-9b1b-7b3ae39b93d4 | cirros   |
| a6540d72-dff7-4fb1-bc64-a8ea69e65178 | imageaz1 |
| 9c7e2d55-0b96-43e2-9231-88e426edb350 | imageaz2 |
+--------------------------------------+----------+
</pre></div>


<p>Update the images with custom properties, i use <code>"availability_zone"</code>
because is the default meta parameter a Host Aggregate owns when is
inside Availability Zones.</p>
<div class="highlight"><pre><span></span># glance image-update --property availability_zone=az1 a6540d72-dff7-4fb1-bc64-a8ea69e65178
+-------------------+--------------------------------------+
| Property          | Value                                |
+-------------------+--------------------------------------+
| availability_zone | az1                                  |
| checksum          | 133eae9fb1c98f45894a4e60d8736619     |
| container_format  | bare                                 |
| created_at        | 2016-01-14T19:59:04Z                 |
| disk_format       | qcow2                                |
| id                | a6540d72-dff7-4fb1-bc64-a8ea69e65178 |
| min_disk          | 0                                    |
| min_ram           | 0                                    |
| name              | imageaz1                             |
| owner             | 0571c6769c3f46acb195eeb01b87ae38     |
| protected         | False                                |
| size              | 13200896                             |
| status            | active                               |
| tags              | []                                   |
| updated_at        | 2016-01-14T20:13:05Z                 |
| virtual_size      | None                                 |
| visibility        | private                              |
+-------------------+--------------------------------------+
# glance image-update --property availability_zone=az2 9c7e2d55-0b96-43e2-9231-88e426edb350
+-------------------+--------------------------------------+
| Property          | Value                                |
+-------------------+--------------------------------------+
| availability_zone | az2                                  |
| checksum          | 133eae9fb1c98f45894a4e60d8736619     |
| container_format  | bare                                 |
| created_at        | 2016-01-14T19:59:10Z                 |
| disk_format       | qcow2                                |
| id                | 9c7e2d55-0b96-43e2-9231-88e426edb350 |
| min_disk          | 0                                    |
| min_ram           | 0                                    |
| name              | imageaz2                             |
| owner             | 0571c6769c3f46acb195eeb01b87ae38     |
| protected         | False                                |
| size              | 13200896                             |
| status            | active                               |
| tags              | []                                   |
| updated_at        | 2016-01-14T20:13:27Z                 |
| virtual_size      | None                                 |
| visibility        | private                              |
+-------------------+--------------------------------------+
</pre></div>


<p>Boot two instances, now we use images with custom properties, those
properties will map to Availability Zones(you can use other type of
parameters mapping to Host Aggregates characteristics).</p>
<div class="highlight"><pre><span></span># nova boot --flavor m1.tiny --image imageaz1 --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 instanceimageaz1
# nova boot --flavor m1.tiny --image imageaz2 --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 instanceimageaz2
</pre></div>


<p>Ensure the instances booted in the desired Availability Zone.</p>
<div class="highlight"><pre><span></span># nova show instanceimageaz1 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az1
# nova show instanceimageaz2 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az2
</pre></div>


<p>Other method to launch instances is with parameters in flavors.<br>
Create two flavors.</p>
<div class="highlight"><pre><span></span># nova flavor-create --is-public true flavoraz1 6 512 1 1
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| ID | Name      | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| 7  | flavoraz1 | 512       | 1    | 0         |      | 1     | 1.0         | True      |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
# nova flavor-create --is-public true flavoraz2 7 512 1 1
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| ID | Name      | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| 8  | flavoraz2 | 512       | 1    | 0         |      | 1     | 1.0         | True      |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
</pre></div>


<p>Add metadata to a Host Aggregate with some characteristic property as
can be fast HD or cheap HW.</p>
<div class="highlight"><pre><span></span># nova aggregate-set-metadata az1-ag fast=true
Metadata has been successfully updated for aggregate 2.
+----+--------+-------------------+--------------+--------------------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                             |
+----+--------+-------------------+--------------+--------------------------------------+
| 2  | az1-ag | az1               | &#39;compute1az&#39; | &#39;availability_zone=az1&#39;, &#39;fast=true&#39; |
+----+--------+-------------------+--------------+--------------------------------------+
# nova aggregate-set-metadata az2-ag cheap=true
Metadata has been successfully updated for aggregate 3.
+----+--------+-------------------+--------------+---------------------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                              |
+----+--------+-------------------+--------------+---------------------------------------+
| 3  | az2-ag | az2               | &#39;compute2az&#39; | &#39;availability_zone=az2&#39;, &#39;cheap=true&#39; |
+----+--------+-------------------+--------------+---------------------------------------+
</pre></div>


<p>Update the previous created flavors with the associated metadata key
with the Host Aggregate.</p>
<div class="highlight"><pre><span></span># nova flavor-key flavoraz1 set  aggregate_instance_extra_specs:fast=true
# nova flavor-key flavoraz2 set  aggregate_instance_extra_specs:cheap=true
</pre></div>


<p>Ensure, the properties are properly created.</p>
<div class="highlight"><pre><span></span># nova flavor-show 7 | grep fast | awk &#39;{print$4$5}&#39;
{&quot;aggregate_instance_extra_specs:fast&quot;:&quot;true&quot;}
# nova flavor-show 8 | grep cheap | awk &#39;{print$4$5}&#39;
{&quot;aggregate_instance_extra_specs:cheap&quot;:&quot;true&quot;}
</pre></div>


<p>By default, Nova Scheduler don't allow filtering by extra Specs inserted
in flavors or images.<br>
First, ensure the following scheduler filters are allowed in Control
nodes.</p>
<div class="highlight"><pre><span></span># egrep ^scheduler_default_filters /etc/nova/nova.conf 
scheduler_default_filters=AggregateInstanceExtraSpecsFilter,RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter
</pre></div>


<p>If a change has been done in nova.conf file, restart nova services</p>
<div class="highlight"><pre><span></span># openstack-service restart nova
</pre></div>


<p>Boot another two instaces, now using custom flavors</p>
<div class="highlight"><pre><span></span># nova boot --flavor flavoraz1 --image cirros --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 instanceflavoraz1
# nova boot --flavor flavoraz2 --image cirros --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 instanceflavoraz2
</pre></div>


<p>Check where the instances are running.</p>
<div class="highlight"><pre><span></span># nova show instanceflavoraz1 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az1
# nova show instanceflavoraz2 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az2
</pre></div>


<p>That's all for now<br>
Hope this guide helps.<br>
Regards</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/openstack-segregation-with-availability-zones-and-host-aggregates.html">posted at 21:34</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/openstack.html" rel="tag">OpenStack</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/availability-zones.html" class="tags">availability zones</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/flavor.html" class="tags">flavor</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/guide.html" class="tags">guide</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/host-aggregates.html" class="tags">host aggregates</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/image.html" class="tags">image</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/meta.html" class="tags">meta</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/nova.html" class="tags">nova</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/scheduler.html" class="tags">scheduler</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/segregation.html" class="tags">segregation</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/usage.html" class="tags">usage</a>
                </div>
		<a href="https://egonzalez90.github.io/openstack-segregation-with-availability-zones-and-host-aggregates.html#disqus_thread">Click to read and post comments</a>
            </article>

                <div class="clear"></div>
                <div class="pages">

                    <a href="https://egonzalez90.github.io/page/2" class="prev_page">&larr;&nbsp;Previous</a>
                    <a href="https://egonzalez90.github.io/page/4" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 3 of 16</span>
                </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="https://egonzalez90.github.io/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>