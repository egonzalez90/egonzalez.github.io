<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenStack Stuff | articles tagged "manual"</title>
    <link rel="shortcut icon" type="image/png" href="https://egonzalez90.github.io/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="https://egonzalez90.github.io/favicon.ico">
    <link href="https://egonzalez90.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="OpenStack Stuff Full Atom Feed" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Eduardo Gonzalez" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li class="ephemeral selected"><a href="https://egonzalez90.github.io/tag/manual.html">manual</a></li>
                <li><a href="https://egonzalez90.github.io/">Home</a></li>
                <li><a href="https://docs.openstack.org">OpenStack</a></li>
                <li><a href="https://www.linkedin.com/in/eduardogonzalezgutierrez">Linkedin</a></li>
                <li><a href="https://github.com/egonzalez90">Github</a></li>
                <li><a href="https://egonzalez90.github.io/archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="https://egonzalez90.github.io/">OpenStack Stuff</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Jul 04, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/midonet-integration-with-openstack-mitaka.html" rel="bookmark" title="Permanent Link to &quot;MidoNet Integration with OpenStack Mitaka&quot;">MidoNet Integration with OpenStack Mitaka</a>
                </h2>

                
                

                <p>MidoNet is an Open Source network virtualization software for IaaS
infrastructure.<br>
It decouples your IaaS cloud from your network hardware, creating an
intelligent software abstraction layer between your end hosts and your
physical network.<br>
This network abstraction layer allows the cloud operator to move what
has traditionally been hardware-based network appliances into a
software-based multi-tenant virtual domain.</p>
<p>This definition from MidoNet documentation explains what MidoNet is and
what MidoNet does.</p>
<p>At this I will post cover my experiences integrating MidoNet with
OpenStack.<br>
I used the following configurations:</p>
<p>All servers have CentOS 7.2 installed<br>
OpenStack has been previously installed from RDO packages with
multinode Packstack</p>
<ul>
<li>
<p>x3 NSDB nodes (Casandra and Zookeeper services)</p>
</li>
<li>
<p>x2 Gateway Nodes (Midolman Agent)</p>
</li>
<li>
<p>x1 OpenStack Controller (MidoNet Cluster)</p>
</li>
<li>
<p>x1 OpenStack compute node (Midolman Agent)</p>
</li>
</ul>
<p><strong>NSDB NODES</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Cassandra repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/datastax.repo
[datastax]
name = DataStax Repo for Apache Cassandra
baseurl = http://rpm.datastax.com/community
enabled = 1
gpgcheck = 1
gpgkey = https://rpm.datastax.com/rpm/repo_key
EOF
</pre></div>


<p>Add Midonet repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repo cache and update packages</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p><strong>Zookeeper Configuration</strong><br>
Install Zookeeper, java and dependencies</p>
<div class="highlight"><pre><span></span>yum install -y java-1.7.0-openjdk-headless zookeeper zkdump nmap-ncat
</pre></div>


<p>Edit zookeeper configuration file</p>
<div class="highlight"><pre><span></span>vi /etc/zookeeper/zoo.cfg
</pre></div>


<p>Add all NSDB nodes at the configuration file</p>
<div class="highlight"><pre><span></span>server.1=nsdb1:2888:3888
server.2=nsdb2:2888:3888
server.3=nsdb3:2888:3888
autopurge.snapRetainCount=10
autopurge.purgeInterval =12
</pre></div>


<p>Create zookeeper folder on which zookeeper will store data, change the
owner to zookeeper user</p>
<div class="highlight"><pre><span></span>mkdir /var/lib/zookeeper/data
chown zookeeper:zookeeper /var/lib/zookeeper/data
</pre></div>


<p>Create myid file at zookeeper data folder, the ID should match with the
NSDB node number, insert that number as follows:</p>
<div class="highlight"><pre><span></span>#NSDB1
echo 1 &gt; /var/lib/zookeeper/data/myid
#NSDB2
echo 2 &gt; /var/lib/zookeeper/data/myid
#NSDB3
echo 3 &gt; /var/lib/zookeeper/data/myid
</pre></div>


<p>Create java folder and create a softlink to it</p>
<div class="highlight"><pre><span></span>mkdir -p /usr/java/default/bin/
ln -s /usr/lib/jvm/jre-1.7.0-openjdk/bin/java /usr/java/default/bin/java
</pre></div>


<p>Start and enable Zookeeper service</p>
<div class="highlight"><pre><span></span>systemctl enable zookeeper.service
systemctl start zookeeper.service
</pre></div>


<p>Test if zookeeper is working locally</p>
<div class="highlight"><pre><span></span>echo ruok | nc 127.0.0.1 2181
imok
</pre></div>


<p>Test if zookeeper is working at NSDB remote nodes</p>
<div class="highlight"><pre><span></span>echo stat | nc nsdb3 2181

Zookeeper version: 3.4.5--1, built on 02/08/2013 12:25 GMT
Clients:
 /192.168.100.172:35306[0](queued=0,recved=1,sent=0)

Latency min/avg/max: 0/0/0
Received: 1
Sent: 0
Connections: 1
Outstanding: 0
Zxid: 0x100000000
Mode: follower
Node count: 4
</pre></div>


<p><strong>Cassandra configuration</strong><br>
Install Java and Cassandra dependencies</p>
<div class="highlight"><pre><span></span>yum install -y java-1.8.0-openjdk-headless dsc22
</pre></div>


<p>Edit cassandra yaml file</p>
<div class="highlight"><pre><span></span>vi /etc/cassandra/conf/cassandra.yaml
</pre></div>


<p>Change cluster_name to midonet<br>
Configure seed_provider seeds to match all NSDB nodes<br>
Configure listen_address and rpc_address to match the hostname of the
self node</p>
<div class="highlight"><pre><span></span>cluster_name: &#39;midonet&#39;
....
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: &quot;nsdb1,nsdb2,nsdb3&quot;

listen_address: nsdb1
rpc_address: nsdb1
</pre></div>


<p>Edit cassandra's init script in order to fix a bug in the init script</p>
<div class="highlight"><pre><span></span>vi /etc/init.d/cassandra
</pre></div>


<p>Add the next two lines after #Casandra startup</p>
<div class="highlight"><pre><span></span>case &quot;$1&quot; in
    start)
        # Cassandra startup
        echo -n &quot;Starting Cassandra: &quot;
        mkdir -p /var/run/cassandra #Add this line
        chown cassandra:cassandra /var/run/cassandra #Add this line
        su $CASSANDRA_OWNR -c &quot;$CASSANDRA_PROG -p $pid_file&quot; &gt; $log_file 2&gt;&amp;1
        retval=$?
        [ $retval -eq 0 ] &amp;&amp; touch $lock_file
        echo &quot;OK&quot;
        ;;
</pre></div>


<p>Start and enable Cassandra service</p>
<div class="highlight"><pre><span></span>systemctl enable cassandra.service
systemctl start cassandra.service
</pre></div>


<p>Check if all NSDB nodes join the cluster</p>
<div class="highlight"><pre><span></span>nodetool --host 127.0.0.1 status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address          Load       Tokens       Owns (effective)  Host ID                               Rack
UN  192.168.100.172  89.1 KB    256          70.8%             3f1ecedd-8caf-4938-84ad-8614d2134557  rack1
UN  192.168.100.224  67.64 KB   256          60.7%             cb36c999-a6e1-4d98-a4dd-d4230b41df08  rack1
UN  192.168.100.195  25.78 KB   256          68.6%             4758bae8-9300-4e57-9a61-5b1107082964  rack1
</pre></div>


<p><strong>Configure OpenStack Controller Nodes (On which Neutron Server is
running)</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Midonet Repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repos cache and update the system</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p>Create an OpenStack user for MidoNet, change the password to match your
own</p>
<div class="highlight"><pre><span></span># openstack user create --password temporal midonet
+----------+----------------------------------+
| Field    | Value                            |
+----------+----------------------------------+
| email    | None                             |
| enabled  | True                             |
| id       | ac25c5a77e7c4e4598ccadea89e09969 |
| name     | midonet                          |
| username | midonet                          |
+----------+----------------------------------+
</pre></div>


<p>Add admin role at tenant services to Midonet user</p>
<div class="highlight"><pre><span></span># openstack role add --project services --user midonet admin
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | bca2c6e1f3da42b0ba82aee401398a8a |
| name      | admin                            |
+-----------+----------------------------------+
</pre></div>


<p>Create MidoNet service at Keystone</p>
<div class="highlight"><pre><span></span># openstack service create --name midonet --description &quot;MidoNet API Service&quot; midonet
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | MidoNet API Service              |
| enabled     | True                             |
| id          | 499059c4a3a040cfb632411408a2be4c |
| name        | midonet                          |
| type        | midonet                          |
+-------------+----------------------------------+
</pre></div>


<p><strong>Clean up neutron server</strong><br>
Stop neutron services</p>
<div class="highlight"><pre><span></span>openstack-service stop neutron
</pre></div>


<p>Remove neutron database and recreate it again</p>
<div class="highlight"><pre><span></span>mysql -u root -p
DROP DATABASE neutron;
Query OK, 157 rows affected (11.50 sec)

MariaDB [(none)]&gt; CREATE DATABASE neutron;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO &#39;neutron&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;ab4f81b1040a495e&#39;;
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO &#39;neutron&#39;@&#39;%&#39; IDENTIFIED BY &#39;ab4f81b1040a495e&#39;;
Query OK, 0 rows affected (0.00 sec)
MariaDB [(none)]&gt; exit
Bye
</pre></div>


<p>Remove plugin.ini symbolic link to ml2_conf.ini</p>
<div class="highlight"><pre><span></span>#rm /etc/neutron/plugin.ini 
rm: remove symbolic link ‘/etc/neutron/plugin.ini’? y
</pre></div>


<p>Remove br-tun tunnel used by neutron in all the nodes</p>
<div class="highlight"><pre><span></span>ovs-vsctl del-br br-tun
</pre></div>


<p>Install MidoNet packages and remove ml2 package</p>
<div class="highlight"><pre><span></span>yum install -y openstack-neutron python-networking-midonet python-neutronclient
yum remove openstack-neutron-ml2
</pre></div>


<p>Make a backup of neutron configuration file</p>
<div class="highlight"><pre><span></span>cp /etc/neutron/neutron.conf neutron.conf.bak
</pre></div>


<p>Edit neutron configuration file</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/neutron.conf
</pre></div>


<p>Most of the options are already configured by our older neutron
configuration, change the ones who apply to match this configuration</p>
<div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">core_plugin</span> <span class="o">=</span> <span class="s">midonet.neutron.plugin_v2.MidonetPluginV2</span>
<span class="na">service_plugins</span> <span class="o">=</span> <span class="s">midonet.neutron.services.l3.l3_midonet.MidonetL3ServicePlugin</span>
<span class="na">dhcp_agent_notification</span> <span class="o">=</span> <span class="s">False</span>
<span class="na">allow_overlapping_ips</span> <span class="o">=</span> <span class="s">True</span>
<span class="na">rpc_backend</span> <span class="o">=</span> <span class="s">rabbit</span>
<span class="na">auth_strategy</span> <span class="o">=</span> <span class="s">keystone</span>
<span class="na">notify_nova_on_port_status_changes</span> <span class="o">=</span> <span class="s">true</span>
<span class="na">notify_nova_on_port_data_changes</span> <span class="o">=</span> <span class="s">true</span>
<span class="na">nova_url</span> <span class="o">=</span> <span class="s">http://controller:8774/v2</span>

<span class="k">[database]</span>
<span class="na">connection</span> <span class="o">=</span> <span class="s">mysql+pymysql://neutron:ab4f81b1040a495e@controller/neutron</span>

<span class="k">[oslo_messaging_rabbit]</span>
<span class="na">rabbit_host</span> <span class="o">=</span> <span class="s">controller</span>
<span class="na">rabbit_userid</span> <span class="o">=</span> <span class="s">guest</span>
<span class="na">rabbit_password</span> <span class="o">=</span> <span class="s">guest</span>

<span class="k">[keystone_authtoken]</span>
<span class="na">auth_uri</span> <span class="o">=</span> <span class="s">http://controller:5000/v2.0</span>
<span class="na">admin_user</span><span class="o">=</span><span class="s">neutron</span>
<span class="na">admin_tenant_name</span><span class="o">=</span><span class="s">services</span>
<span class="na">identity_uri</span><span class="o">=</span><span class="s">http://controller:35357</span>
<span class="na">admin_password</span><span class="o">=</span><span class="s">d88f0bd060d64c33</span>

<span class="k">[nova]</span>
<span class="na">region_name</span> <span class="o">=</span> <span class="s">RegionOne</span>
<span class="na">auth_url</span> <span class="o">=</span> <span class="s">http://controller:35357</span>
<span class="na">auth_type</span> <span class="o">=</span> <span class="s">password</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">9ca36d15e4824d93</span>
<span class="na">project_domain_id</span> <span class="o">=</span> <span class="s">default</span>
<span class="na">project_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">tenant_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">user_domain_id</span> <span class="o">=</span> <span class="s">default</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">nova</span>

<span class="k">[oslo_concurrency]</span>
<span class="na">lock_path</span> <span class="o">=</span> <span class="s">/var/lib/neutron/tmp</span>
</pre></div>


<p>At my deployment these are the options I had to change to configure
midonet</p>
<div class="highlight"><pre><span></span><span class="gh">diff /etc/neutron/neutron.conf neutron.conf.bak </span>
33c33
&lt; core_plugin = midonet.neutron.plugin_v2.MidonetPluginV2
<span class="gd">---</span>
&gt; core_plugin = neutron.plugins.ml2.plugin.Ml2Plugin
37c37
&lt; service_plugins = midonet.neutron.services.l3.l3_midonet.MidonetL3ServicePlugin
<span class="gd">---</span>
&gt; service_plugins =router
120c120
&lt; dhcp_agent_notification = False
<span class="gd">---</span>
&gt; #dhcp_agent_notification = true
1087c1087,1088
&lt; lock_path = /var/lib/neutron/tmp
<span class="gd">---</span>
&gt; lock_path = $state_path/lock
&gt;
</pre></div>


<p>Create midonet plugins folder</p>
<div class="highlight"><pre><span></span>mkdir /etc/neutron/plugins/midonet
</pre></div>


<p>Create a file called midonet.ini</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/plugins/midonet/midonet.ini
</pre></div>


<p>Configure midonet.ini file to match your own configuration options</p>
<div class="highlight"><pre><span></span><span class="k">[MIDONET]</span>
<span class="c1"># MidoNet API URL</span>
<span class="na">midonet_uri</span> <span class="o">=</span> <span class="s">http://controller:8181/midonet-api</span>
<span class="c1"># MidoNet administrative user in Keystone</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">midonet</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">temporal</span>
<span class="c1"># MidoNet administrative user&#39;s tenant</span>
<span class="na">project_id</span> <span class="o">=</span> <span class="s">services</span>
</pre></div>


<p>Create a symbolic link from midonet.ini to plugin.ini</p>
<div class="highlight"><pre><span></span>ln -s /etc/neutron/plugins/midonet/midonet.ini /etc/neutron/plugin.ini
</pre></div>


<p>Sync and populate database tables with Midonet plugin</p>
<div class="highlight"><pre><span></span>su -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/midonet/midonet.ini upgrade head&quot; neutron
su -s /bin/sh -c &quot;neutron-db-manage --subproject networking-midonet upgrade head&quot; neutron
</pre></div>


<p>Restart nova api and neutron server services</p>
<div class="highlight"><pre><span></span>systemctl restart openstack-nova-api.service
systemctl restart neutron-server
</pre></div>


<p>Install midonet cluster package</p>
<div class="highlight"><pre><span></span>yum install -y midonet-cluster
</pre></div>


<p>Configure midonet.conf file</p>
<div class="highlight"><pre><span></span>vi /etc/midonet/midonet.conf
</pre></div>


<p>Add all NSDB nodes at zookeeper_hosts</p>
<div class="highlight"><pre><span></span><span class="k">[zookeeper]</span>
<span class="na">zookeeper_hosts</span> <span class="o">=</span> <span class="s">nsdb1:2181,nsdb2:2181,nsdb3:2181</span>
</pre></div>


<p>Configure midonet to make use of NSDB nodes as Zookeeper and cassandra
hosts</p>
<div class="highlight"><pre><span></span>cat &lt;&lt; EOF | mn-conf set -t default
zookeeper {
    zookeeper_hosts = &quot;nsdb1:2181,nsdb2:2181,nsdb3:2181&quot;
}

cassandra {
    servers = &quot;nsdb1,nsdb2,nsdb3&quot;
}
EOF
</pre></div>


<p>Set cassandra replication factor to 3</p>
<div class="highlight"><pre><span></span>echo &quot;cassandra.replication_factor : 3&quot; | mn-conf set -t default
</pre></div>


<p>Grab your admin token</p>
<div class="highlight"><pre><span></span>#egrep ^admin_token /etc/keystone/keystone.conf 
admin_token = 7b84d89b32c34b71a697eb1a270807ab
</pre></div>


<p>Configure Midonet to auth with keystone</p>
<div class="highlight"><pre><span></span>cat &lt;&lt; EOF | mn-conf set -t default
cluster.auth {
    provider_class = &quot;org.midonet.cluster.auth.keystone.KeystoneService&quot;
    admin_role = &quot;admin&quot;
    keystone.tenant_name = &quot;admin&quot;
    keystone.admin_token = &quot;7b84d89b32c34b71a697eb1a270807ab&quot;
    keystone.host = controller
    keystone.port = 35357
}
EOF
</pre></div>


<p>Start and enable midonet cluster service</p>
<div class="highlight"><pre><span></span>systemctl enable midonet-cluster.service
systemctl start midonet-cluster.service
</pre></div>


<p>Install midonet CLI</p>
<div class="highlight"><pre><span></span>yum install -y python-midonetclient
</pre></div>


<p>Create a file at you home directory with midonet auth info</p>
<div class="highlight"><pre><span></span>#vi ~/.midonetrc

[cli]
api_url = http://controller:8181/midonet-api
username = admin
password = temporal
project_id = admin
</pre></div>


<p><strong>Configure Compute nodes</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Midonet repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repos cache and update the system</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p>Edit qemu.conf</p>
<div class="highlight"><pre><span></span>vi /etc/libvirt/qemu.conf
</pre></div>


<p>Configure with the following options, by default all these options are
commented, you can paste it all wherever you want</p>
<div class="highlight"><pre><span></span>user = &quot;root&quot;
group = &quot;root&quot;

cgroup_device_acl = [
    &quot;/dev/null&quot;, &quot;/dev/full&quot;, &quot;/dev/zero&quot;,
    &quot;/dev/random&quot;, &quot;/dev/urandom&quot;,
    &quot;/dev/ptmx&quot;, &quot;/dev/kvm&quot;, &quot;/dev/kqemu&quot;,
    &quot;/dev/rtc&quot;,&quot;/dev/hpet&quot;, &quot;/dev/vfio/vfio&quot;,
    &quot;/dev/net/tun&quot;
]
</pre></div>


<p>Restart libvirtd service</p>
<div class="highlight"><pre><span></span>systemctl restart libvirtd.service
</pre></div>


<p>Install nova-network package</p>
<div class="highlight"><pre><span></span>yum install -y openstack-nova-network
</pre></div>


<p>Disable Nova Network service and restart Nova compute service</p>
<div class="highlight"><pre><span></span>systemctl disable openstack-nova-network.service
systemctl restart openstack-nova-compute.service
</pre></div>


<p>Install Midolman agent and java packages</p>
<div class="highlight"><pre><span></span>yum install -y java-1.8.0-openjdk-headless midolman
</pre></div>


<p>Configure midolman.conf</p>
<div class="highlight"><pre><span></span>vi /etc/midolman/midolman.conf
</pre></div>


<p>Add all nsdb nodes as zookeeper hosts</p>
<div class="highlight"><pre><span></span><span class="k">[zookeeper]</span>
<span class="na">zookeeper_hosts</span> <span class="o">=</span> <span class="s">nsdb1:2181,nsdb2:2181,nsdb3:2181</span>
</pre></div>


<p>Configure each compute node with an appropiate flavor located at
/etc/midolman/ folder, the have different hardware resources configured,
use the one that better match your compute host capabilities</p>
<div class="highlight"><pre><span></span>mn-conf template-set -h local -t agent-compute-medium
cp /etc/midolman/midolman-env.sh.compute.medium /etc/midolman/midolman-env.sh
</pre></div>


<p>Configure metadata, issue the following commands only once, it will
automatically populate the configuration to all midonet agents</p>
<div class="highlight"><pre><span></span>echo &quot;agent.openstack.metadata.nova_metadata_url : \&quot;http://controller:8775\&quot;&quot; | mn-conf set -t default
echo &quot;agent.openstack.metadata.shared_secret : 2bfeb930a90d435d&quot; | mn-conf set -t default
echo &quot;agent.openstack.metadata.enabled : true&quot; | mn-conf set -t default
</pre></div>


<p>Allow metadata trafic at iptables</p>
<div class="highlight"><pre><span></span>iptables -I INPUT 1 -i metadata -j ACCEPT
</pre></div>


<p>Remove br-tun bridge</p>
<div class="highlight"><pre><span></span>ovs-vsctl del-br br-tun
</pre></div>


<p>Start and enable midolman agent service</p>
<div class="highlight"><pre><span></span>systemctl enable midolman.service
systemctl start midolman.service
</pre></div>


<p><strong>Gateway nodes configuration</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Midonet repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repos cache and update the system</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p>Install Midolman agent and java packages</p>
<div class="highlight"><pre><span></span>yum install -y java-1.8.0-openjdk-headless midolman
</pre></div>


<p>Configure midolman.conf</p>
<div class="highlight"><pre><span></span>vi /etc/midolman/midolman.conf
</pre></div>


<p>Add all nsdb nodes as zookeeper hosts</p>
<div class="highlight"><pre><span></span><span class="k">[zookeeper]</span>
<span class="na">zookeeper_hosts</span> <span class="o">=</span> <span class="s">nsdb1:2181,nsdb2:2181,nsdb3:2181</span>
</pre></div>


<p>Configure each gateway node with an appropiate flavor located at
/etc/midolman/ folder, the have different hardware resources configured,
use the one that better match your gateway host capabilities</p>
<div class="highlight"><pre><span></span>mn-conf template-set -h local -t agent-gateway-medium
cp /etc/midolman/midolman-env.sh.gateway.medium /etc/midolman/midolman-env.sh
</pre></div>


<p>Grab the metadata shared secret located at nova.conf at any of your nova
nodes</p>
<div class="highlight"><pre><span></span># egrep ^metadata_proxy_shared_secret /etc/nova/nova.conf 
metadata_proxy_shared_secret =2bfeb930a90d435d
</pre></div>


<p>Allow metadata trafic at iptables</p>
<div class="highlight"><pre><span></span>iptables -I INPUT 1 -i metadata -j ACCEPT
</pre></div>


<p>Start and enable midolman agent service</p>
<div class="highlight"><pre><span></span>systemctl enable midolman.service
systemctl start midolman.service
</pre></div>


<p><strong>Configure encapsulation and register nodes</strong><br>
Enter to midonet CLI from a controller node</p>
<div class="highlight"><pre><span></span>midonet-cli
</pre></div>


<p>Create the tunnel zone with VXLAN encapsulation</p>
<div class="highlight"><pre><span></span>midonet&gt; tunnel-zone create name tz type vxlan
tzone0
midonet&gt; list tunnel-zone
tzone tzone0 name tz type vxlan
</pre></div>


<p>List hosts discovered by midonet, should be all the nodes where you
configured midonet agents(midolman)</p>
<div class="highlight"><pre><span></span>midonet&gt; list host
host host0 name gateway2 alive true addresses fe80:0:0:0:0:11ff:fe00:1102,169.254.123.1,fe80:0:0:0:0:11ff:fe00:1101,127.0.0.1,0:0:0:0:0:0:0:1,192.168.200.176,fe80:0:0:0:5054:ff:fef9:b2a0,169.254.169.254,fe80:0:0:0:7874:d6ff:fe5b:dea8,192.168.100.227,fe80:0:0:0:5054:ff:fed9:9cc0,fe80:0:0:0:5054:ff:fe4a:e39b,192.168.1.86 flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
host host1 name gateway1 alive true addresses 169.254.169.254,fe80:0:0:0:3cd1:23ff:feac:a3c2,192.168.1.87,fe80:0:0:0:5054:ff:fea8:da91,127.0.0.1,0:0:0:0:0:0:0:1,fe80:0:0:0:5054:ff:feec:92c1,192.168.200.232,fe80:0:0:0:0:11ff:fe00:1102,169.254.123.1,fe80:0:0:0:0:11ff:fe00:1101,192.168.100.141,fe80:0:0:0:5054:ff:fe20:30fb flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
host host2 name compute1 alive true addresses fe80:0:0:0:0:11ff:fe00:1101,169.254.123.1,127.0.0.1,0:0:0:0:0:0:0:1,fe80:0:0:0:0:11ff:fe00:1102,192.168.100.173,fe80:0:0:0:5054:ff:fe06:161,fe80:0:0:0:5054:ff:fee3:eb48,192.168.200.251,fe80:0:0:0:5054:ff:fe8d:d22,192.168.1.93,169.254.169.254,fe80:0:0:0:48cb:adff:fe69:f07b flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
</pre></div>


<p>Register each of the nodes at the VXLAN zone we created before</p>
<div class="highlight"><pre><span></span>midonet&gt; tunnel-zone tzone0 add member host host0 address 192.168.100.227
zone tzone0 host host0 address 192.168.100.227
midonet&gt; tunnel-zone tzone0 add member host host1 address 192.168.100.141
zone tzone0 host host1 address 192.168.100.141
midonet&gt; tunnel-zone tzone0 add member host host2 address 192.168.100.173
zone tzone0 host host2 address 192.168.100.173
</pre></div>


<p><strong>Create Networks at Neutron</strong><br>
Create an external network</p>
<div class="highlight"><pre><span></span># neutron net-create ext-net --router:external
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:47:30                  |
| description           |                                      |
| id                    | dc15245e-4391-4514-b489-8976373046a3 |
| is_default            | False                                |
| name                  | ext-net                              |
| port_security_enabled | True                                 |
| provider:network_type | midonet                              |
| router:external       | True                                 |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
| updated_at            | 2016-07-03T14:47:30                  |
+-----------------------+--------------------------------------+
</pre></div>


<p>Create an external subnet in the network we created before, use you own
IP ranges to match your environment</p>
<div class="highlight"><pre><span></span># neutron subnet-create ext-net 192.168.200.0/24 --name ext-subnet   
 --allocation-pool start=192.168.200.225,end=192.168.200.240   
 --disable-dhcp --gateway 192.168.200.1
Created a new subnet:
+-------------------+--------------------------------------------------------+
| Field             | Value                                                  |
+-------------------+--------------------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;192.168.200.225&quot;, &quot;end&quot;: &quot;192.168.200.240&quot;} |
| cidr              | 192.168.200.0/24                                       |
| created_at        | 2016-07-03T14:50:46                                    |
| description       |                                                        |
| dns_nameservers   |                                                        |
| enable_dhcp       | False                                                  |
| gateway_ip        | 192.168.200.1                                          |
| host_routes       |                                                        |
| id                | 234dcc9a-2878-4799-b564-bf3a1bd52cad                   |
| ip_version        | 4                                                      |
| ipv6_address_mode |                                                        |
| ipv6_ra_mode      |                                                        |
| name              | ext-subnet                                             |
| network_id        | dc15245e-4391-4514-b489-8976373046a3                   |
| subnetpool_id     |                                                        |
| tenant_id         | 2f7ee2716b3b4140be57b4a5b26401e3                       |
| updated_at        | 2016-07-03T14:50:46                                    |
+-------------------+--------------------------------------------------------+
</pre></div>


<p>Create a tenant network and a subnet on it</p>
<div class="highlight"><pre><span></span># neutron net-create demo-net
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:51:39                  |
| description           |                                      |
| id                    | 075ba699-dc4c-4625-8e0d-0a258a9aeb7d |
| name                  | demo-net                             |
| port_security_enabled | True                                 |
| provider:network_type | midonet                              |
| router:external       | False                                |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
| updated_at            | 2016-07-03T14:51:39                  |
+-----------------------+--------------------------------------+
# neutron subnet-create demo-net 10.0.20.0/24 --name demo-subnet 
Created a new subnet:
+-------------------+----------------------------------------------+
| Field             | Value                                        |
+-------------------+----------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;10.0.20.2&quot;, &quot;end&quot;: &quot;10.0.20.254&quot;} |
| cidr              | 10.0.20.0/24                                 |
| created_at        | 2016-07-03T14:52:32                          |
| description       |                                              |
| dns_nameservers   |                                              |
| enable_dhcp       | True                                         |
| gateway_ip        | 10.0.20.1                                    |
| host_routes       |                                              |
| id                | b299d899-33a3-4bfa-aff4-fda071545bdf         |
| ip_version        | 4                                            |
| ipv6_address_mode |                                              |
| ipv6_ra_mode      |                                              |
| name              | demo-subnet                                  |
| network_id        | 075ba699-dc4c-4625-8e0d-0a258a9aeb7d         |
| subnetpool_id     |                                              |
| tenant_id         | 2f7ee2716b3b4140be57b4a5b26401e3             |
| updated_at        | 2016-07-03T14:52:32                          |
+-------------------+----------------------------------------------+
</pre></div>


<p>Create a tenant router</p>
<div class="highlight"><pre><span></span># neutron router-create router1
Created a new router:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| description           |                                      |
| external_gateway_info |                                      |
| id                    | 258942d8-9d82-4ebd-b829-c7bdfcc973f5 |
| name                  | router1                              |
| routes                |                                      |
| status                | ACTIVE                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
+-----------------------+--------------------------------------+
</pre></div>


<p>Attach the tenant subnet interface we created before to the router</p>
<div class="highlight"><pre><span></span># neutron router-interface-add router1 demo-subnet
Added interface 06c85a56-368c-4d79-bbf0-4bb077f163e5 to router router1.
</pre></div>


<p>Set the external network as router gateway</p>
<div class="highlight"><pre><span></span># neutron router-gateway-set router1 ext-net
Set gateway for router router1
</pre></div>


<p>Now, you can create an instance at tenant network</p>
<div class="highlight"><pre><span></span># nova boot --flavor m1.tiny --image 80871834-29dd-4100-b038-f5f83f126204 --nic net-id=075ba699-dc4c-4625-8e0d-0a258a9aeb7d test1
+--------------------------------------+-----------------------------------------------------+
| Property                             | Value                                               |
+--------------------------------------+-----------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                              |
| OS-EXT-AZ:availability_zone          |                                                     |
| OS-EXT-SRV-ATTR:host                 | -                                                   |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                                   |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000000a                                   |
| OS-EXT-STS:power_state               | 0                                                   |
| OS-EXT-STS:task_state                | scheduling                                          |
| OS-EXT-STS:vm_state                  | building                                            |
| OS-SRV-USG:launched_at               | -                                                   |
| OS-SRV-USG:terminated_at             | -                                                   |
| accessIPv4                           |                                                     |
| accessIPv6                           |                                                     |
| adminPass                            | q2Cq4kxePSLL                                        |
| config_drive                         |                                                     |
| created                              | 2016-07-03T15:46:19Z                                |
| flavor                               | m1.tiny (1)                                         |
| hostId                               |                                                     |
| id                                   | b8aa46f9-186c-4594-8428-f8dbb16a5e16                |
| image                                | cirros image (80871834-29dd-4100-b038-f5f83f126204) |
| key_name                             | -                                                   |
| metadata                             | {}                                                  |
| name                                 | test1                                               |
| os-extended-volumes:volumes_attached | []                                                  |
| progress                             | 0                                                   |
| security_groups                      | default                                             |
| status                               | BUILD                                               |
| tenant_id                            | 2f7ee2716b3b4140be57b4a5b26401e3                    |
| updated                              | 2016-07-03T15:46:20Z                                |
| user_id                              | a2482a91a1f14750b372445d28b07c75                    |
+--------------------------------------+-----------------------------------------------------+
# nova list
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| ID                                   | Name  | Status | Task State | Power State | Networks            |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| b8aa46f9-186c-4594-8428-f8dbb16a5e16 | test1 | ACTIVE | -          | Running     | demo-net=10.0.20.11 |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
</pre></div>


<p>Ensure the instance gets IP and the metadata service is properly running</p>
<div class="highlight"><pre><span></span># nova console-log test1
...#Snipp from the output
Sending discover...
Sending select for 10.0.20.11...
Lease of 10.0.20.11 obtained, lease time 86400
cirros-ds &#39;net&#39; up at 7.92
checking http://169.254.169.254/2009-04-04/instance-id
successful after 1/20 tries: up 8.22. iid=i-0000000a
...
</pre></div>


<p>If you login to the instance through VNC you should be able to ping
another instances</p>
<p><strong>Edge router configuration</strong><br>
Create a new router</p>
<div class="highlight"><pre><span></span># neutron router-create edge-router
Created a new router:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| description           |                                      |
| external_gateway_info |                                      |
| id                    | 5ecadb64-cb0d-4f95-a00e-aa1dd20a2012 |
| name                  | edge-router                          |
| routes                |                                      |
| status                | ACTIVE                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
+-----------------------+--------------------------------------+
</pre></div>


<p>Attach the external subnet interface to the router</p>
<div class="highlight"><pre><span></span># neutron router-interface-add edge-router ext-subnet
Added interface e37f1986-c6b1-47f4-8268-02b837ceac17 to router edge-router.
</pre></div>


<p>Create an uplink network</p>
<div class="highlight"><pre><span></span># neutron net-create uplink-network --tenant_id admin --provider:network_type uplink
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:57:15                  |
| description           |                                      |
| id                    | 77173ed4-6106-4515-af1c-3683897955f9 |
| name                  | uplink-network                       |
| port_security_enabled | True                                 |
| provider:network_type | uplink                               |
| router:external       | False                                |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | admin                                |
| updated_at            | 2016-07-03T14:57:15                  |
+-----------------------+--------------------------------------+
</pre></div>


<p>Create a subnet in the uplink network</p>
<div class="highlight"><pre><span></span># neutron subnet-create --tenant_id admin --disable-dhcp --name uplink-subnet uplink-network 192.168.1.0/24
Created a new subnet:
+-------------------+--------------------------------------------------+
| Field             | Value                                            |
+-------------------+--------------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;192.168.1.2&quot;, &quot;end&quot;: &quot;192.168.1.254&quot;} |
| cidr              | 192.168.1.0/24                                   |
| created_at        | 2016-07-03T15:06:28                              |
| description       |                                                  |
| dns_nameservers   |                                                  |
| enable_dhcp       | False                                            |
| gateway_ip        | 192.168.1.1                                      |
| host_routes       |                                                  |
| id                | 4e98e789-20d3-45fd-a3b5-9bcf02d8a832             |
| ip_version        | 4                                                |
| ipv6_address_mode |                                                  |
| ipv6_ra_mode      |                                                  |
| name              | uplink-subnet                                    |
| network_id        | 77173ed4-6106-4515-af1c-3683897955f9             |
| subnetpool_id     |                                                  |
| tenant_id         | admin                                            |
| updated_at        | 2016-07-03T15:06:28                              |
+-------------------+--------------------------------------------------+
</pre></div>


<p>Create a port for each of the gateway nodes, interface should match with
the NIC you want to use for binding the gateway nodes and a IP address
for the same purposes</p>
<div class="highlight"><pre><span></span># neutron port-create uplink-network --binding:host_id gateway1 --binding:profile type=dict interface_name=eth1 --fixed-ip ip_address=192.168.1.199
Created a new port:
+-----------------------+--------------------------------------------------------------------------------------+
| Field                 | Value                                                                                |
+-----------------------+--------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                 |
| allowed_address_pairs |                                                                                      |
| binding:host_id       | compute1                                                                             |
| binding:profile       | {&quot;interface_name&quot;: &quot;eth1&quot;}                                                           |
| binding:vif_details   | {&quot;port_filter&quot;: true}                                                                |
| binding:vif_type      | midonet                                                                              |
| binding:vnic_type     | normal                                                                               |
| created_at            | 2016-07-03T15:10:06                                                                  |
| description           |                                                                                      |
| device_id             |                                                                                      |
| device_owner          |                                                                                      |
| extra_dhcp_opts       |                                                                                      |
| fixed_ips             | {&quot;subnet_id&quot;: &quot;4e98e789-20d3-45fd-a3b5-9bcf02d8a832&quot;, &quot;ip_address&quot;: &quot;192.168.1.199&quot;} |
| id                    | 7b4f54dd-2b41-42ba-9c5c-cda4640dc550                                                 |
| mac_address           | fa:16:3e:44:a8:c9                                                                    |
| name                  |                                                                                      |
| network_id            | 77173ed4-6106-4515-af1c-3683897955f9                                                 |
| port_security_enabled | True                                                                                 |
| security_groups       | 0cf3e33e-dbd6-4b42-a0bd-6679b5eed4e1                                                 |
| status                | ACTIVE                                                                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3                                                     |
| updated_at            | 2016-07-03T15:10:06                                                                  |
+-----------------------+--------------------------------------------------------------------------------------+
</pre></div>


<p>Attach each of the ports to the edge router</p>
<div class="highlight"><pre><span></span># neutron router-interface-add edge-router port=7b4f54dd-2b41-42ba-9c5c-cda4640dc550
Added interface 7b4f54dd-2b41-42ba-9c5c-cda4640dc550 to router edge-router.
</pre></div>


<p>At this point you have to decide if use border routers with BGP enabled
or static routes.<br>
Use one of the following links to configure your use case:  </p>
<p><a href="https://docs.midonet.org/docs/latest/operations-guide/content/bgp_uplink_configuration.html">https://docs.midonet.org/docs/latest/operations-guide/content/bgp_uplink_configuration.html</a>  </p>
<p><a href="https://docs.midonet.org/docs/latest/operations-guide/content/static_setup.html">https://docs.midonet.org/docs/latest/operations-guide/content/static_setup.html</a></p>
<p><strong>Issues I faced during configuration of Midonet</strong></p>
<p>Midolman agent don't start:<br>
It was caused because midolman-env.sh file has more RAM configured as
the one of my server.<br>
Edit the file to match your server resources</p>
<div class="highlight"><pre><span></span># egrep ^MAX_HEAP_SIZE /etc/midolman/midolman-env.sh
MAX_HEAP_SIZE=&quot;2048M&quot;
</pre></div>


<p>Instances doesn't boot with the following error:</p>
<div class="highlight"><pre><span></span>could not open /dev/net/tun: Permission denied
</pre></div>


<p>I had to remove br-tun bridges at ovs, if not, ovs locks the device and
midolman cannot create the tunnel beetwen compute nodes and gateway
nodes.</p>
<div class="highlight"><pre><span></span>ovs-vsctl del-br br-tun
</pre></div>


<p>This post is my experience integrating Midonet into OpenStack, maybe
some things are not correct, if you find any issue, please advise me to
fix it.<br>
Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/midonet-integration-with-openstack-mitaka.html">posted at 21:48</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/linux-openstack-various.html" rel="tag">Linux, OpenStack, Various</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/cloud.html" class="tags">cloud</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/config.html" class="tags">config</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/install.html" class="tags">install</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/integration.html" class="tags">integration</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/manual.html" class="tags selected">manual</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/midokura.html" class="tags">midokura</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/midonet.html" class="tags">midonet</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/mitaka.html" class="tags">mitaka</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/network.html" class="tags">network</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/packages.html" class="tags">packages</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/packstack.html" class="tags">packstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rdo.html" class="tags">rdo</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/sdn.html" class="tags">sdn</a>
                </div>
		<a href="https://egonzalez90.github.io/midonet-integration-with-openstack-mitaka.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Mar 24, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code.html" rel="bookmark" title="Permanent Link to &quot;Magnum in RDO OpenStack - Liberty Manual Installation from source code&quot;">Magnum in RDO OpenStack - Liberty Manual Installation from source code</a>
                </h2>

                
                

                <p>Want to install Magnum (Containers as a Service) in an OpenStack
environment based on packages from RDO project?<br>
Here are the steps to do it:</p>
<p>Primary steps are the same as official Magnum guide, major differences
come from DevStack or manual installations vs packages from RDO
project.<br>
Also, some of the steps are explained to show how Magnum should work,
as well this guide can help you understand Magnum integration with your
current environment.<br>
I'm not going to use Barbican service for certs management, you will
see how to use Magnum without Barbican too.</p>
<ul>
<li>For now, there is not RDO packages for magnum, so we are going to
    install it from source code.</li>
<li>As i know, currently magnum packages are under development and will
    be added in future OpenStack versions to RDO project packages.
    (Probably Mitaka or Newton)</li>
</ul>
<p>Passwords used at this demo are:</p>
<ul>
<li>temporal (Databases and OpenStack users)</li>
<li>guest (RabbitMQ)</li>
</ul>
<p>IPs used are:</p>
<ul>
<li>192.168.200.208 (Service APIs)</li>
<li>192.168.100.0/24 (External network range)</li>
<li>10.0.0.0/24 (Tenant network range)</li>
<li>8.8.8.8 (Google DNS server)</li>
</ul>
<p>First we need to install some dependencies and packages needed for next
steps.</p>
<div class="highlight"><pre><span></span>sudo yum install -y gcc python-setuptools python-devel git libffi-devel openssl-devel wget
</pre></div>


<p>Install pip</p>
<div class="highlight"><pre><span></span>easy_install pip
</pre></div>


<p>Clone Magnum source code from OpenStack git repository, ensure you use
Liberty branch, if not, Magnum dependencies will break all OpenStack
services dependencies and lost your current environment (Trust me, i'm
talking from my own experience)</p>
<div class="highlight"><pre><span></span>git clone https://git.openstack.org/openstack/magnum -b stable/liberty
</pre></div>


<p>Move to your newly created folder and install Magnum (dependency
requirements and Magnum)</p>
<div class="highlight"><pre><span></span>cd magnum
sudo pip install -e .
</pre></div>


<p>Once Magnum is installed, create Magnum database and Magnum user</p>
<div class="highlight"><pre><span></span>mysql -uroot -p
CREATE DATABASE IF NOT EXISTS magnum DEFAULT CHARACTER SET utf8;
GRANT ALL PRIVILEGES ON magnum.* TO&#39;magnum&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;temporal&#39;;
GRANT ALL PRIVILEGES ON magnum.* TO&#39;magnum&#39;@&#39;%&#39; IDENTIFIED BY &#39;temporal&#39;;
</pre></div>


<p>Create Magnum folder and copy sample configuration files.</p>
<div class="highlight"><pre><span></span>mkdir /etc/magnum
sudo cp etc/magnum/magnum.conf.sample /etc/magnum/magnum.conf
sudo cp etc/magnum/policy.json /etc/magnum/policy.json
</pre></div>


<p>Edit Magnum main configuration file</p>
<div class="highlight"><pre><span></span>vi /etc/magnum/magnum.conf
</pre></div>


<p>Configure messaging backend to RabbitMQ</p>
<div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>

<span class="na">rpc_backend</span> <span class="o">=</span> <span class="s">rabbit</span>
<span class="na">notification_driver</span> <span class="o">=</span> <span class="s">messaging</span>
</pre></div>


<p>Bind Magnum API port to listen on all the interfaces, you can also
especify on which IP Magnum API will be listening if you are concerned
about security risks.</p>
<div class="highlight"><pre><span></span><span class="k">[api]</span>

<span class="na">host</span> <span class="o">=</span> <span class="s">0.0.0.0</span>
</pre></div>


<p>Configure RabbitMQ backend</p>
<div class="highlight"><pre><span></span><span class="k">[oslo_messaging_rabbit]</span>

<span class="na">rabbit_host</span> <span class="o">=</span> <span class="s">192.168.200.208</span>
<span class="na">rabbit_userid</span> <span class="o">=</span> <span class="s">guest</span>
<span class="na">rabbit_password</span> <span class="o">=</span> <span class="s">guest</span>
<span class="na">rabbit_virtual_host</span> <span class="o">=</span> <span class="s">/</span>
</pre></div>


<p>Set database connection</p>
<div class="highlight"><pre><span></span><span class="k">[database]</span>

<span class="na">connection</span><span class="o">=</span><span class="s">mysql://magnum:temporal@192.168.200.208/magnum</span>
</pre></div>


<p>Set cert_manager_type to local, this option will disable Barbican
service, you will need to create a folder (We will do it in next steps)</p>
<div class="highlight"><pre><span></span><span class="k">[certificates]</span>

<span class="na">cert_manager_type</span> <span class="o">=</span> <span class="s">local</span>
</pre></div>


<p>As all OpenStack services, Keystone authentication is required.</p>
<ul>
<li>Check what your service tenant name it is (RDO default name is
    "services" other installations usually use "service" name.</li>
</ul>
<!-- -->

<div class="highlight"><pre><span></span><span class="k">[keystone_authtoken]</span>

<span class="na">auth_uri</span><span class="o">=</span><span class="s">http://192.168.200.208:5000/v2.0</span>
<span class="na">identity_uri</span><span class="o">=</span><span class="s">http://192.168.200.208:35357</span>
<span class="na">auth_strategy</span><span class="o">=</span><span class="s">keystone</span>
<span class="na">admin_user</span><span class="o">=</span><span class="s">magnum</span>
<span class="na">admin_password</span><span class="o">=</span><span class="s">temporal</span>
<span class="na">admin_tenant_name</span><span class="o">=</span><span class="s">services</span>
</pre></div>


<p>As we saw before, create local certificates folder to avoid using
Barbican service. This is the step we previously commented</p>
<div class="highlight"><pre><span></span>mkdir -p /var/lib/magnum/certificates/
</pre></div>


<p>Clone python-magnumclient and install it, this package will provide us
commands to use Magnum</p>
<div class="highlight"><pre><span></span>git clone https://git.openstack.org/openstack/python-magnumclient -b stable/liberty
cd python-magnumclient
sudo pip install -e .
</pre></div>


<p>Create Magnum user at keystone</p>
<div class="highlight"><pre><span></span>openstack user create --password temporal magnum
</pre></div>


<p>Add admin role to Magnum user at tenant services</p>
<div class="highlight"><pre><span></span>openstack role add --project services --user magnum admin
</pre></div>


<p>Create container service</p>
<div class="highlight"><pre><span></span>openstack service create --name magnum --description &quot;Magnum Container Service&quot; container
</pre></div>


<p>Finally create Magnum endpoints</p>
<div class="highlight"><pre><span></span>openstack endpoint create --region RegionOne --publicurl &#39;http://192.168.200.208:9511/v1&#39; --adminurl &#39;http://192.168.200.208:9511/v1&#39; --internalurl &#39;http://192.168.200.208:9511/v1&#39; magnum
</pre></div>


<p>Sync Magnum database, this step will create Magnum tables at the
database</p>
<div class="highlight"><pre><span></span>magnum-db-manage --config-file /etc/magnum/magnum.conf upgrade
</pre></div>


<p>Open two terminal session and execute one command on each terminal to
start both services. If you encounter any issue, logs can be found at
these terminal</p>
<div class="highlight"><pre><span></span>magnum-api --config-file /etc/magnum/magnum.conf
magnum-conductor --config-file /etc/magnum/magnum.conf
</pre></div>


<p>Check if Magnum service is fine</p>
<div class="highlight"><pre><span></span>magnum service-list
+----+------------+------------------+-------+
| id | host       | binary           | state |
+----+------------+------------------+-------+
| 1  | controller | magnum-conductor | up    |
+----+------------+------------------+-------+
</pre></div>


<p>Download fedora atomic image</p>
<div class="highlight"><pre><span></span>wget https://fedorapeople.org/groups/magnum/fedora-21-atomic-5.qcow2
</pre></div>


<p>Create a Glance image with Atomic.qcow2 file</p>
<div class="highlight"><pre><span></span>glance image-create --name fedora-21-atomic-5   
                   --visibility public   
                   --disk-format qcow2   
                   --os-distro fedora-atomic   
                   --container-format bare &lt; fedora-21-atomic-5.qcow2
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | cebefc0c21fb8567e662bf9f2d5b78b0     |
| container_format | bare                                 |
| created_at       | 2016-03-19T15:55:21Z                 |
| disk_format      | qcow2                                |
| id               | 7293891d-cfba-48a9-a4db-72c29c65f681 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | fedora-21-atomic-5                   |
| os_distro        | fedora-atomic                        |
| owner            | e3cca42ed57745148e0c342a000d99e9     |
| protected        | False                                |
| size             | 891355136                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2016-03-19T15:55:28Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+
</pre></div>


<p>Create a ssh key if not exists, this command won't create a new ssh key
if already exists</p>
<div class="highlight"><pre><span></span>test -f ~/.ssh/id_rsa.pub || ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa
</pre></div>


<p>Add the key to nova, mine is called egonzalez</p>
<div class="highlight"><pre><span></span>nova keypair-add --pub-key ~/.ssh/id_rsa.pub egonzalez
</pre></div>


<p>Now we are going to test our new Magnum service, you have various
methods to do it.<br>
I will use Docker Swarm method because is the simplest one for this
demo purposes. Go through Magnum documentation to check other container
methods as Kubernetes is.</p>
<p>Create a baymodel with atomic image and swarm, select a flavor with at
least 10GB of disk</p>
<div class="highlight"><pre><span></span>magnum baymodel-create --name demoswarmbaymodel   
                      --image-id fedora-21-atomic-5   
                      --keypair-id egonzalez   
                      --external-network-id public   
                      --dns-nameserver 8.8.8.8   
                      --flavor-id testflavor   
                      --docker-volume-size 1   
                      --coe swarm
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| http_proxy          | None                                 |
| updated_at          | None                                 |
| master_flavor_id    | None                                 |
| fixed_network       | None                                 |
| uuid                | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| no_proxy            | None                                 |
| https_proxy         | None                                 |
| tls_disabled        | False                                |
| keypair_id          | egonzalez                            |
| public              | False                                |
| labels              | {}                                   |
| docker_volume_size  | 1                                    |
| external_network_id | public                               |
| cluster_distro      | fedora-atomic                        |
| image_id            | fedora-21-atomic-5                   |
| registry_enabled    | False                                |
| apiserver_port      | None                                 |
| name                | demoswarmbaymodel                    |
| created_at          | 2016-03-19T17:22:43+00:00            |
| network_driver      | None                                 |
| ssh_authorized_key  | None                                 |
| coe                 | swarm                                |
| flavor_id           | testflavor                           |
| dns_nameserver      | 8.8.8.8                              |
+---------------------+--------------------------------------+
</pre></div>


<p>Create a bay with the previous bay model, we are going to create one
master node and one worker, specify all that apply to your environment</p>
<div class="highlight"><pre><span></span>magnum bay-create --name demoswarmbay --baymodel demoswarmbaymodel --master-count 1 --node-count 1
+--------------------+--------------------------------------+
| Property           | Value                                |
+--------------------+--------------------------------------+
| status             | None                                 |
| uuid               | a2388916-db30-41bf-84eb-df0b65979eaf |
| status_reason      | None                                 |
| created_at         | 2016-03-19T17:23:00+00:00            |
| updated_at         | None                                 |
| bay_create_timeout | 0                                    |
| api_address        | None                                 |
| baymodel_id        | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| node_count         | 1                                    |
| node_addresses     | None                                 |
| master_count       | 1                                    |
| discovery_url      | None                                 |
| name               | demoswarmbay                         |
+--------------------+--------------------------------------+
</pre></div>


<p>Check bay status, for now it should be in CREATE_IN_PROGRESS state</p>
<div class="highlight"><pre><span></span>magnum bay-show demoswarmbay
+--------------------+--------------------------------------+
| Property           | Value                                |
+--------------------+--------------------------------------+
| status             | CREATE_IN_PROGRESS                   |
| uuid               | a2388916-db30-41bf-84eb-df0b65979eaf |
| status_reason      |                                      |
| created_at         | 2016-03-19T17:23:00+00:00            |
| updated_at         | 2016-03-19T17:23:01+00:00            |
| bay_create_timeout | 0                                    |
| api_address        | None                                 |
| baymodel_id        | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| node_count         | 1                                    |
| node_addresses     | []                                   |
| master_count       | 1                                    |
| discovery_url      | None                                 |
| name               | demoswarmbay                         |
+--------------------+--------------------------------------+
</pre></div>


<p>If all is going fine, nova should have two new instances(in ACTIVE
state), one for the master node and second for the worker.</p>
<div class="highlight"><pre><span></span>nova list
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
| ID                                   | Name                                                  | Status | Task State | Power State | Networks                                                                      |
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
| e38eb88c-bb6b-427d-a2c5-cdfe868796f0 | de-44kx2l4q4wc-0-d6j5svvjxmne-swarm_node-xafkm2jskf5j | ACTIVE | -          | Running     | demoswarmbay-agf6y3qnjoyw-fixed_network-g37bcmc52akv=10.0.0.4, 192.168.100.16 |
| 5acc579d-152a-4656-9eb8-e800b7ab3bcf | demoswarmbay-agf6y3qnjoyw-swarm_master-fllwhrpuabbq   | ACTIVE | -          | Running     | demoswarmbay-agf6y3qnjoyw-fixed_network-g37bcmc52akv=10.0.0.3, 192.168.100.15 |
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
</pre></div>


<p>You can see how heat stack is going</p>
<div class="highlight"><pre><span></span>heat stack-list
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
| id                                   | stack_name                | stack_status       | creation_time       | updated_time |
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
| 3a64fa60-4df8-498f-aceb-a0cb8cfc0b18 | demoswarmbay-agf6y3qnjoyw | CREATE_IN_PROGRESS | 2016-03-19T17:22:59 | None         |
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
</pre></div>


<p>We can see what tasks are executing during stack creation</p>
<div class="highlight"><pre><span></span>heat event-list demoswarmbay-agf6y3qnjoyw
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
| resource_name                       | id                                   | resource_status_reason | resource_status    | event_time          |
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
| demoswarmbay-agf6y3qnjoyw           | 004c9388-b8ab-4541-ada8-99b65203e41d | Stack CREATE started   | CREATE_IN_PROGRESS | 2016-03-19T17:23:01 |
| master_wait_handle                  | d6f0798a-bfde-4bad-9c73-e108bd101009 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:01 |
| secgroup_manager                    | e2e0eb08-aeeb-4290-9ad5-bd20fe243f07 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:02 |
| disable_selinux                     | d7290592-ab81-4d7a-b2fa-902975904a25 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| agent_wait_handle                   | 65ec5553-56a4-4416-9748-bfa0ae35737a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| add_proxy                           | 46bdcff8-4606-406f-8c99-7f48adc4de57 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| write_docker_socket                 | ab5402ea-44af-4433-84aa-a63256817a9a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| make_cert                           | 3b9817a5-606f-41ab-8799-b411c017f05d | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| cfn_signal                          | 0add665a-3fdf-4408-ab15-76332aa326fe | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| remove_docker_key                   | 94f4106e-f139-4d9f-9974-8821d04be103 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| configure_swarm                     | f7e0ebd5-1893-43d1-bd29-81a7e39de0c0 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| extrouter                           | a94a8f68-c237-4dbc-9513-cdbe3de1465e | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| enable_services                     | c250f532-99bd-43d7-9d15-b2d3ae16567a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| write_docker_service                | 2c9d8954-4446-4578-a871-0910e8996571 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| cloud_init_wait_handle              | 6cc51d2d-56e9-458b-a21b-bc553e0c8291 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| fixed_network                       | 3125395f-c689-4481-bf01-94bb2f701993 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:07 |
| agent_wait_handle                   | 2db801e8-c2b5-47b0-ac16-122dba3a22d6 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| remove_docker_key                   | 75e2c7a6-a2ce-4026-aeeb-739c4a522f48 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| secgroup_manager                    | ac51a029-26c1-495a-bc13-232cfb8c1060 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| write_docker_socket                 | 58e08b52-a12a-43e9-b41d-071750294024 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| master_wait_handle                  | 3e741b76-6470-47d4-b13e-3f8f446be53c | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| cfn_signal                          | 96c26b4f-1e99-478e-a8e5-9dcc4486e1b3 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| enable_services                     | beedc358-ee72-4b34-a6b9-1b47ffc15306 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| add_proxy                           | caae3a07-d5f1-4eb0-8a82-02ea634f77ae | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| make_cert                           | 79363643-e5e4-4d1b-ad8a-5a56e1f6a8e7 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| cloud_init_wait_handle              | 0457b008-6da8-44fd-abef-cb99bd4d0518 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| configure_swarm                     | baf1e089-c627-4b24-a571-63b3c9c14e28 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| extrouter                           | 184614d9-2280-4cb4-9253-f538463dbdf4 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| write_docker_service                | 80e66b4e-d40a-4243-bb27-0d2a6b68651f | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| disable_selinux                     | d8a64822-2571-4dcf-9da5-b3ec73e771eb | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| fixed_network                       | 528b0ced-23f6-4c22-8cbc-357ba0ee5bc5 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| write_swarm_manager_failure_service | 9fa100a3-b4a9-465c-8b33-dd000cb4866a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| write_swarm_agent_failure_service   | a7c09833-929e-4711-a3e9-39923d23b2f2 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| fixed_subnet                        | 23d8b0a6-a7a3-4f71-9c18-ba6255cf071a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| write_swarm_master_service          | d24a6099-3cad-41ce-8d4b-a7ad0661aaea | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:11 |
| fixed_subnet                        | 1a2b7397-1d09-4544-bb9f-985c2f64cb09 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_manager_failure_service | 615a2a7a-5266-487b-bbe1-fcaa82f43243 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_agent_failure_service   | 3f8c54b4-6644-49a0-ad98-9bc6b4332a07 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_master_service          | 2f58b3c8-d1cc-4590-a328-0e775e495bcf | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| extrouter_inside                    | f3da7f2f-643e-4f29-a00f-d2595d7faeaf | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:14 |
| swarm_master_eth0                   | 1d6a510d-520c-4796-8990-aa8f7dd59757 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:16 |
| swarm_master_eth0                   | 3fd85913-7399-49be-bb46-5085ff953611 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:19 |
| extrouter_inside                    | 33749e30-cbea-4093-b36a-94967e299002 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:19 |
| write_heat_params                   | 054e0af5-e3e0-4bc0-92b5-b40aeedc39ab | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:19 |
| swarm_nodes                         | df7af58c-8148-4b51-bd65-b0734d9051b5 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:20 |
| write_swarm_agent_service           | ab1e8b1e-2837-4693-b791-e1311f85fa63 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:21 |
| swarm_master_floating               | d99ffe66-cb02-4279-99dc-a1f3e2ca817c | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:22 |
| write_heat_params                   | 33d9999f-6c93-453d-8565-ac99db021f8f | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| write_swarm_agent_service           | 02a1b7f6-2660-4345-ad08-42b66ffaaad5 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| swarm_master_floating               | 8ce6ecd8-c421-4e4a-ab81-cba4b5ccedf4 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| swarm_master_init                   | 3787dcc8-e644-412b-859b-63a434b9ee6c | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:26 |
| swarm_master_init                   | a1dd67bb-49c7-4507-8af0-7758b76b57e1 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:28 |
| swarm_master                        | d12b915e-3087-4e17-9954-8233926b504b | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:29 |
| swarm_master                        | a34ad52a-def7-460b-b5b7-410000207b3e | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:48 |
| master_wait_condition               | 0c9331a4-8ad0-46e0-bf2a-35943021a1a3 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
| cloud_init_wait_condition           | de3707a0-f46a-44a9-b4b8-ff50e12cc77f | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
| agent_wait_condition                | a1a810a4-9c19-4983-aaa8-e03f308c1e39 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
</pre></div>


<p>Once all tasks are completed, we can create containers in the bay we
created in previous steps.</p>
<div class="highlight"><pre><span></span>magnum container-create --name demo-container   
                       --image docker.io/cirros:latest   
                       --bay demoswarmbay   
                       --command &quot;ping -c 4 192.168.100.2&quot;
+------------+----------------------------------------+
| Property   | Value                                  |
+------------+----------------------------------------+
| uuid       | 36595858-8657-d465-3e5a-dfcddad8a238   |
| links      | ...                                    |
| bay_uuid   | a2388916-db30-41bf-84eb-df0b65979eaf   |
| updated_at | None                                   |
| image      | cirros                                 |
| command    | ping -c 4 192.168.100.2                |
| created_at | 2016-03-19T17:30:00+00:00              |
| name       | demo-container                         |
+------------+----------------------------------------+
</pre></div>


<p>Container is created, but not started.<br>
Start the container</p>
<div class="highlight"><pre><span></span>magnum container-start demo-container
</pre></div>


<p>Check container logs, you should see 4 pings succeed to our external
router gateway.</p>
<div class="highlight"><pre><span></span>magnum container-logs demo-container

PING 192.168.100.2 (192.168.100.2) 56(84) bytes of data.
64 bytes from 192.168.100.2: icmp_seq=1 ttl=64 time=0.083 ms
64 bytes from 192.168.100.2: icmp_seq=2 ttl=64 time=0.068 ms
64 bytes from 192.168.100.2: icmp_seq=3 ttl=64 time=0.043 ms
64 bytes from 192.168.100.2: icmp_seq=4 ttl=64 time=0.099 ms
</pre></div>


<p>You can delete the container</p>
<div class="highlight"><pre><span></span>magnum container-delete demo-container
</pre></div>


<p>While doing this demo, i missed adding branch name while cloning Magnum
source code, when i installed Magnum all package dependencies where
installed from master, who was Mitaka instead of Liberty, which broke my
environment.</p>
<p>I suffered the following issues:</p>
<p>Issues with packages</p>
<div class="highlight"><pre><span></span><span class="n">ImportError</span><span class="o">:</span> <span class="n">No</span> <span class="n">module</span> <span class="n">named</span> <span class="n">MySQLdb</span>
</pre></div>


<p>Was solved installing MySQL-python from pip instead of yum</p>
<div class="highlight"><pre><span></span>pip install MySQL-python
</pre></div>


<p>Issues with policies, admin privileges weren't recognized by Magnum api.</p>
<div class="highlight"><pre><span></span><span class="n">PolicyNotAuthorized</span><span class="o">:</span> <span class="n">magnum</span><span class="o">-</span><span class="n">service</span><span class="o">:</span><span class="n">get_all</span><span class="o">{{</span> <span class="n">bunch</span> <span class="n">of</span> <span class="n">stuff</span> <span class="o">}}</span> <span class="n">disallowed</span> <span class="n">by</span> <span class="n">policy</span>
</pre></div>


<p>Was solved removing admin_api rule at Magnum policy.json file</p>
<div class="highlight"><pre><span></span>vi /etc/magnum/policy.json

#    &quot;admin_api&quot;: &quot;rule:context_is_admin&quot;,
</pre></div>


<p>Unfortunately, nova was completely broken and it was not working at all,
so i installed a new environment and added branch while cloning source
code.<br>
Next issue i found was Barbican, who was not installed, i used the
steps mentioned at this post to solve this issue.</p>
<p>Hope this guide helps you integrating Magnum Container as a Service in
OpenStack.</p>
<p>Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code.html">posted at 19:22</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/openstack.html" rel="tag">OpenStack</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/container.html" class="tags">container</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/docker.html" class="tags">docker</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/guide.html" class="tags">guide</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/install.html" class="tags">install</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/liberty.html" class="tags">liberty</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/magnum.html" class="tags">magnum</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/manual.html" class="tags selected">manual</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rdo.html" class="tags">rdo</a>
                </div>
		<a href="https://egonzalez90.github.io/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Dec 14, 2015</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/murano-in-rdo-openstack-manual-installation.html" rel="bookmark" title="Permanent Link to &quot;Murano in RDO OpenStack - Manual Installation&quot;">Murano in RDO OpenStack - Manual Installation</a>
                </h2>

                
                

                <p>Want to install and use Murano in a RDO OpenStack environment? Here are
the steps to do it.</p>
<p>The first thing we need to do, is to know what is Murano:<br>
Murano is an application catalog who gives the users the capacity to
launch pre-configured s/instances/jobs/g with apps in an OpenStack
infrastructure.<br>
As the final user just select an application from a catalog with a
minimal configuration, and Murano will take the role to orchestrate the
background jobs(create instances, configure apps, connect networks,
etc)<br>
For more information about application catalog project refer to this
site:<br>
<a href="https://wiki.openstack.org/wiki/Murano/ApplicationCatalog">https://wiki.openstack.org/wiki/Murano/ApplicationCatalog</a></p>
<p>At this tutorial, i will use the following s/configurations/versions/g:</p>
<ul>
<li>Centos 7.1</li>
<li>RDO Liberty release</li>
<li>Hosts installed with packstack/ML2 network</li>
</ul>
<p>Let's start installing some pre requisites</p>
<div class="highlight"><pre><span></span>sudo yum install -y gcc python-setuptools python-devel git postgresql-devel libffi-devel openssl-devel
</pre></div>


<p>Install pip</p>
<div class="highlight"><pre><span></span>sudo easy_install pip
</pre></div>


<p>Install tox and upgrade six</p>
<div class="highlight"><pre><span></span>sudo pip install tox
sudo pip install --upgrade six
</pre></div>


<p>Create a database for murano</p>
<div class="highlight"><pre><span></span>mysql -u root -p
CREATE DATABASE murano;
</pre></div>


<p>Create murano user at MySQL</p>
<div class="highlight"><pre><span></span>GRANT ALL PRIVILEGES ON murano.* TO &#39;murano&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;MURANODB_PASS&#39;;
GRANT ALL PRIVILEGES ON murano.* TO &#39;murano&#39;@&#39;%&#39; IDENTIFIED BY &#39;MURANODB_PASS&#39;;
</pre></div>


<p>Clone murano from liberty/stable branch</p>
<div class="highlight"><pre><span></span>git clone -b stable/liberty git://git.openstack.org/openstack/murano
</pre></div>


<p>Install all requirements</p>
<div class="highlight"><pre><span></span>cd ~/murano/
sudo  pip install -r requirements.txt
</pre></div>


<p>Install murano</p>
<div class="highlight"><pre><span></span>sudo python setup.py install
</pre></div>


<p>Create sample configuration file</p>
<div class="highlight"><pre><span></span>oslo-config-generator --config-file etc/oslo-config-generator/murano.conf
</pre></div>


<p>Create murano directory and copy the sample content on it</p>
<div class="highlight"><pre><span></span>mkdir /etc/murano
cp ~/murano/etc/murano/* /etc/murano/
</pre></div>


<p>Rename sample configuration to murano.conf</p>
<div class="highlight"><pre><span></span>mv /etc/murano/murano.conf.sample /etc/murano/murano.conf
</pre></div>


<p>Edit the configuration file like this, adjust the configuration as your
environment needs.<br>
<code>vi /etc/murano/murano.conf</code></p>
<div class="highlight"><pre><span></span><span class="k">[oslo_messaging_rabbit]</span>

<span class="na">rabbit_host</span><span class="o">=</span><span class="s">RABBITMQ_IP</span>
<span class="na">rabbit_port</span><span class="o">=</span><span class="s">5672</span>
<span class="na">rabbit_hosts</span><span class="o">=</span><span class="s">RABBITMQ_IP:5672</span>
<span class="na">rabbit_use_ssl</span><span class="o">=</span><span class="s">False</span>
<span class="na">rabbit_userid</span><span class="o">=</span><span class="s">guest</span>
<span class="na">rabbit_password</span><span class="o">=</span><span class="s">guest</span>
<span class="na">rabbit_virtual_host</span><span class="o">=</span><span class="s">/</span>
<span class="na">rabbit_ha_queues</span><span class="o">=</span><span class="s">False</span>
<span class="na">rabbit_notification_exchange</span><span class="o">=</span><span class="s">openstack</span>
<span class="na">rabbit_notification_topic</span><span class="o">=</span><span class="s">notifications</span>

<span class="k">[database]</span>
<span class="na">connection</span> <span class="o">=</span> <span class="s">mysql://murano:MURANODB_PASS@MYSQL_IP/murano</span>

<span class="k">[keystone_authtoken]</span>
<span class="na">auth_uri</span><span class="o">=</span><span class="s">http://KEYSTONE_IP:5000/v2.0</span>
<span class="na">identity_uri</span><span class="o">=</span><span class="s">http://KEYSTONE_IP:35357</span>
<span class="na">admin_user</span><span class="o">=</span><span class="s">murano</span>
<span class="na">admin_password</span><span class="o">=</span><span class="s">MURANO_PASS</span>
<span class="na">admin_tenant_name</span><span class="o">=</span><span class="s">services</span>

<span class="k">[murano]</span>
<span class="na">url</span> <span class="o">=</span> <span class="s">http://MURANO_IP:8082</span>

<span class="k">[rabbitmq]</span>

<span class="na">host</span><span class="o">=</span><span class="s">RABBITMQ_IP</span>
<span class="na">login</span><span class="o">=</span><span class="s">guest</span>
<span class="na">password</span><span class="o">=</span><span class="s">guest</span>
<span class="na">virtual_host</span><span class="o">=</span><span class="s">/</span>
</pre></div>


<p>Create murano user</p>
<div class="highlight"><pre><span></span>openstack user create --password MURANO_PASS murano
</pre></div>


<p>Add murano user to services tenant with admin privileges</p>
<div class="highlight"><pre><span></span>openstack role add --project services --user murano admin
</pre></div>


<p>Create a service for application-catalog</p>
<div class="highlight"><pre><span></span>openstack service create --name muranoapi --description &quot;Murano Project&quot; application-catalog
</pre></div>


<p>Associate an endpoint to application-catalog service</p>
<div class="highlight"><pre><span></span>openstack endpoint create --region RegionOne --publicurl &#39;http://MURANO_IP:8082/&#39; --adminurl &#39;http://MURANO_IP:8082/&#39; --internalurl &#39;http://http://MURANO_IP:8082/&#39; MURANO_SERVICE_ID
</pre></div>


<p>Sync the database</p>
<div class="highlight"><pre><span></span>murano-db-manage --config-file /etc/murano/murano.conf upgrade
</pre></div>


<p>Open a new terminal and start murano-api service</p>
<div class="highlight"><pre><span></span>murano-api --config-file /etc/murano/murano.conf
</pre></div>


<p>Import base murano package</p>
<div class="highlight"><pre><span></span>murano-manage --config-file /etc/murano/murano.conf import-package murano/meta/io.murano
</pre></div>


<p>In a new terminal, start murano-engine service</p>
<div class="highlight"><pre><span></span>murano-engine --config-file /etc/murano/murano.conf
</pre></div>


<p>Clone stable liberty module for horizon</p>
<div class="highlight"><pre><span></span>git clone -b stable/liberty git://git.openstack.org/openstack/murano-dashboard
</pre></div>


<p>Install base requirements</p>
<div class="highlight"><pre><span></span>cd ~/murano-dashboard
pip install -r requirements.txt
</pre></div>


<p>Install murano-dashboard module</p>
<div class="highlight"><pre><span></span>sudo python setup.py install
</pre></div>


<p>Enable murano-dashboard at horizon</p>
<div class="highlight"><pre><span></span>cp muranodashboard/local/_50_murano.py /usr/share/openstack-dashboard/openstack_dashboard/enabled/
</pre></div>


<p>Restart apache to apply changes</p>
<div class="highlight"><pre><span></span>systemctl restart httpd
</pre></div>


<p>Import ApacheHttpServer package</p>
<div class="highlight"><pre><span></span><span class="n">murano</span> <span class="o">--</span><span class="n">murano</span><span class="o">-</span><span class="n">repo</span><span class="o">-</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://storage.apps.openstack.org/&quot;</span> <span class="n">package</span><span class="o">-</span><span class="kn">import</span> <span class="nn">io.murano.apps.apache.ApacheHttpServer</span>
</pre></div>


<p>You can find more packages at:
<a href="http://apps.openstack.org/#tab=murano-apps">http://apps.openstack.org/#tab=murano-apps</a></p>
<p>This will add a Debian image to glance image service, wait until the
image is in active status</p>
<p>Create a file with the following content, modify the variables with your
own needs<br>
<code>vi object_model_patch.json</code></p>
<div class="highlight"><pre><span></span>[
    { &quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/-&quot;, &quot;value&quot;:
        {
            &quot;instance&quot;: {
                &quot;availabilityZone&quot;: &quot;nova&quot;,
                &quot;name&quot;: &quot;APP_NAME&quot;,
                &quot;image&quot;: &quot;GLANCE_IMAGE_ID&quot;,
                &quot;keyname&quot;: &quot;KEY_PAIR&quot;,
                &quot;flavor&quot;: &quot;FLAVOR&quot;,
                &quot;assignFloatingIp&quot;: false,
                &quot;?&quot;: {
                    &quot;type&quot;: &quot;io.murano.resources.LinuxMuranoInstance&quot;,
                    &quot;id&quot;: &quot;===id1===&quot;
                }
            },
            &quot;name&quot;: &quot;ApacheHttpServer&quot;,
            &quot;enablePHP&quot;: true,
            &quot;?&quot;: {
                &quot;type&quot;: &quot;io.murano.apps.apache.ApacheHttpServer&quot;,
                &quot;id&quot;: &quot;===id2===&quot;
            }
        }
    }
]
</pre></div>


<p>Create an environment</p>
<div class="highlight"><pre><span></span>murano environment-create --join-subnet-id SUBNET_ID ENVIRONMENT_NAME

murano environment-create --join-subnet-id e2c5175a-d5bc-4eb7-91ba-67ac9120c64a test
+----------------------------------+------+---------------------+---------------------+
| ID                               | Name | Created             | Updated             |
+----------------------------------+------+---------------------+---------------------+
| 68a19d233d2d42459faf64d375d995e5 | test | 2015-12-11T13:09:57 | 2015-12-11T13:09:57 |
+----------------------------------+------+---------------------+---------------------+
</pre></div>


<p>Create a session for temporal working on the environment</p>
<div class="highlight"><pre><span></span>murano environment-session-create ENVIRONMENT_ID

murano environment-session-create 68a19d233d2d42459faf64d375d995e5
Created new session:
+----------+----------------------------------+
| Property | Value                            |
+----------+----------------------------------+
| id       | b0f5e39a9c4c419c9ee7fdb6c92c37a6 |
+----------+----------------------------------+
</pre></div>


<p>Add the file with the apps configuration</p>
<div class="highlight"><pre><span></span>murano environment-apps-edit --session-id SESSION_ID ENVIRONMENT_ID FILE_NAME

murano environment-apps-edit --session-id b0f5e39a9c4c419c9ee7fdb6c92c37a6 68a19d233d2d42459faf64d375d995e5 object_model_patch.json
</pre></div>


<p>Deploy the environment</p>
<div class="highlight"><pre><span></span>murano environment-deploy ENVIRONMENT_ID --session-id SESSION_ID

murano environment-deploy 68a19d233d2d42459faf64d375d995e5 --session-id b0f5e39a9c4c419c9ee7fdb6c92c37a6
+-----------+-------------------------------------------------------------+
| Property  | Value                                                       |
+-----------+-------------------------------------------------------------+
| created   | 2015-12-11T13:09:57                                         |
| id        | 68a19d233d2d42459faf64d375d995e5                            |
| name      | test                                                        |
| services  | [                                                           |
|           |   {                                                         |
|           |     &quot;instance&quot;: {                                           |
|           |       &quot;availabilityZone&quot;: &quot;nova&quot;,                           |
|           |       &quot;name&quot;: &quot;test&quot;,                                       |
|           |       &quot;assignFloatingIp&quot;: false,                            |
|           |       &quot;keyname&quot;: &quot;&quot;,                                        |
|           |       &quot;flavor&quot;: &quot;twogb&quot;,                                    |
|           |       &quot;image&quot;: &quot;9049eb0c-081e-4d56-9413-72fdc6f8d8bf&quot;,      |
|           |       &quot;?&quot;: {                                                |
|           |         &quot;type&quot;: &quot;io.murano.resources.LinuxMuranoInstance&quot;,  |
|           |         &quot;id&quot;: &quot;30f5a591a58a468fbf4d7ef4755e0512&quot;            |
|           |       }                                                     |
|           |     },                                                      |
|           |     &quot;name&quot;: &quot;ApacheHttpServer&quot;,                             |
|           |     &quot;enablePHP&quot;: true,                                      |
|           |     &quot;?&quot;: {                                                  |
|           |       &quot;status&quot;: &quot;deploying&quot;,                                |
|           |       &quot;type&quot;: &quot;io.murano.apps.apache.ApacheHttpServer&quot;,     |
|           |       &quot;id&quot;: &quot;98b994565c634f7e97d5f365203ce222&quot;              |
|           |     }                                                       |
|           |   }                                                         |
|           | ]                                                           |
| status    | deploying                                                   |
| tenant_id | 3a5d50fac9a3462fa4d76b8b84677c3f                            |
| updated   | 2015-12-11T13:09:57                                         |
| version   | 0                                                           |
+-----------+-------------------------------------------------------------+
</pre></div>


<p>Now, you can check at nova the building status of the instances</p>
<div class="highlight"><pre><span></span>nova list
+--------------------------------------+-----------------------------------------+--------+------------+-------------+----------+
| ID                                   | Name                                    | Status | Task State | Power State | Networks |
+--------------------------------------+-----------------------------------------+--------+------------+-------------+----------+
| a68cedfb-7b4c-47a6-96fb-6b64a85a8ca6 | murano-mmnpdii1ozz7r2-test-5np5cvfeoiyh | BUILD  | scheduling | NOSTATE     |          |
+--------------------------------------+-----------------------------------------+--------+------------+-------------+----------+
</pre></div>


<p>After a while, the instance is up and running</p>
<div class="highlight"><pre><span></span>nova list
+--------------------------------------+-----------------------------------------+--------+------------+-------------+------------------+
| ID                                   | Name                                    | Status | Task State | Power State | Networks         |
+--------------------------------------+-----------------------------------------+--------+------------+-------------+------------------+
| a68cedfb-7b4c-47a6-96fb-6b64a85a8ca6 | murano-mmnpdii1ozz7r2-test-5np5cvfeoiyh | ACTIVE | -          | Running     | private=10.0.0.8 |
+--------------------------------------+-----------------------------------------+--------+------------+-------------+------------------+
</pre></div>


<p>Once the instance is active, murano will configure the application
inside, wait until the status is ready.</p>
<div class="highlight"><pre><span></span>murano environment-show f392de2004e24ff7b2a08f05df0599b8
+-----------+---------------------------------------------------------------+
| Property  | Value                                                         |
+-----------+---------------------------------------------------------------+
| created   | 2015-12-11T13:43:23                                           |
| id        | 68a19d233d2d42459faf64d375d995e5                              |
| name      | test                                                          |
| services  | [                                                             |
|           |   {                                                           |
|           |     &quot;instance&quot;: {                                             |
|           |       &quot;availabilityZone&quot;: &quot;nova&quot;,                             |
|           |       &quot;openstackId&quot;: &quot;91615340-e1d3-428e-848f-38a762004d33&quot;,  |
|           |       &quot;name&quot;: &quot;test&quot;,                                         |
|           |       &quot;securityGroupName&quot;: null,                              |
|           |       &quot;image&quot;: &quot;9049eb0c-081e-4d56-9413-72fdc6f8d8bf&quot;,        |
|           |       &quot;assignFloatingIp&quot;: false,                              |
|           |       &quot;floatingIpAddress&quot;: null,                              |
|           |       &quot;keyname&quot;: &quot;&quot;,                                          |
|           |       &quot;?&quot;: {                                                  |
|           |         &quot;classVersion&quot;: &quot;0.0.0&quot;,                              |
|           |         &quot;name&quot;: null,                                         |
|           |         &quot;package&quot;: &quot;io.murano&quot;,                               |
|           |         &quot;type&quot;: &quot;io.murano.resources.LinuxMuranoInstance&quot;,    |
|           |         &quot;_actions&quot;: {},                                       |
|           |         &quot;id&quot;: &quot;30f5a591a58a468fbf4d7ef4755e0512&quot;              |
|           |       },                                                      |
|           |       &quot;ipAddresses&quot;: [                                        |
|           |         &quot;10.0.0.8&quot;                                            |
|           |       ],                                                      |
|           |       &quot;flavor&quot;: &quot;twogb&quot;,                                      |
|           |       &quot;networks&quot;: {                                           |
|           |         &quot;useFlatNetwork&quot;: false,                              |
|           |         &quot;primaryNetwork&quot;: null,                               |
|           |         &quot;useEnvironmentNetwork&quot;: true,                        |
|           |         &quot;customNetworks&quot;: []                                  |
|           |       },                                                      |
|           |       &quot;sharedIps&quot;: []                                         |
|           |     },                                                        |
|           |     &quot;name&quot;: &quot;ApacheHttpServer&quot;,                               |
|           |     &quot;?&quot;: {                                                    |
|           |       &quot;classVersion&quot;: &quot;0.0.0&quot;,                                |
|           |       &quot;status&quot;: &quot;ready&quot;,                                      |
|           |       &quot;name&quot;: null,                                           |
|           |       &quot;package&quot;: &quot;io.murano.apps.apache.ApacheHttpServer&quot;,    |
|           |       &quot;type&quot;: &quot;io.murano.apps.apache.ApacheHttpServer&quot;,       |
|           |       &quot;_actions&quot;: {},                                         |
|           |       &quot;id&quot;: &quot;98b994565c634f7e97d5f365203ce222&quot;                |
|           |     },                                                        |
|           |     &quot;enablePHP&quot;: true                                         |
|           |   }                                                           |
|           | ]                                                             |
| status    | ready                                                         |
| tenant_id | 3a5d50fac9a3462fa4d76b8b84677c3f                              |
| updated   | 2015-12-11T13:47:35                                           |
| version   | 1                                                             |
+-----------+---------------------------------------------------------------+
</pre></div>


<p>That's all you need to have up and running a Murano application catalog,
for now there is no rpm package to ease the installation, so you need to
install from source like we done.<br>
A thing you can do, is create systemd files to manage murano services
in a easier way.</p>
<p>Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/murano-in-rdo-openstack-manual-installation.html">posted at 20:04</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/openstack.html" rel="tag">OpenStack</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/app.html" class="tags">app</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/application-catalog.html" class="tags">application catalog</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/centos.html" class="tags">centos</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/create.html" class="tags">create</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/deploy.html" class="tags">deploy</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/environment.html" class="tags">environment</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/install.html" class="tags">install</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/liberty.html" class="tags">liberty</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/manual.html" class="tags selected">manual</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/murano.html" class="tags">murano</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/packstack.html" class="tags">packstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rdo.html" class="tags">rdo</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/red-hat.html" class="tags">red hat</a>
                </div>
		<a href="https://egonzalez90.github.io/murano-in-rdo-openstack-manual-installation.html#disqus_thread">Click to read and post comments</a>
            </article>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="https://egonzalez90.github.io/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>