<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenStack Stuff | articles tagged "openstack" | Page 3</title>
    <link rel="shortcut icon" type="image/png" href="https://egonzalez90.github.io/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="https://egonzalez90.github.io/favicon.ico">
    <link href="https://egonzalez90.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="OpenStack Stuff Full Atom Feed" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Eduardo Gonzalez" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li class="ephemeral selected"><a href="https://egonzalez90.github.io/tag/openstack3.html">openstack</a></li>
                <li><a href="https://egonzalez90.github.io/">Home</a></li>
                <li><a href="https://docs.openstack.org">OpenStack</a></li>
                <li><a href="https://www.linkedin.com/in/eduardogonzalezgutierrez">Linkedin</a></li>
                <li><a href="https://github.com/egonzalez90">Github</a></li>
                <li><a href="https://egonzalez90.github.io/archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="https://egonzalez90.github.io/">OpenStack Stuff</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Mar 24, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code.html" rel="bookmark" title="Permanent Link to &quot;Magnum in RDO OpenStack - Liberty Manual Installation from source code&quot;">Magnum in RDO OpenStack - Liberty Manual Installation from source code</a>
                </h2>

                
                

                <p>Want to install Magnum (Containers as a Service) in an OpenStack
environment based on packages from RDO project?<br>
Here are the steps to do it:</p>
<p>Primary steps are the same as official Magnum guide, major differences
come from DevStack or manual installations vs packages from RDO
project.<br>
Also, some of the steps are explained to show how Magnum should work,
as well this guide can help you understand Magnum integration with your
current environment.<br>
I'm not going to use Barbican service for certs management, you will
see how to use Magnum without Barbican too.</p>
<ul>
<li>For now, there is not RDO packages for magnum, so we are going to
    install it from source code.</li>
<li>As i know, currently magnum packages are under development and will
    be added in future OpenStack versions to RDO project packages.
    (Probably Mitaka or Newton)</li>
</ul>
<p>Passwords used at this demo are:</p>
<ul>
<li>temporal (Databases and OpenStack users)</li>
<li>guest (RabbitMQ)</li>
</ul>
<p>IPs used are:</p>
<ul>
<li>192.168.200.208 (Service APIs)</li>
<li>192.168.100.0/24 (External network range)</li>
<li>10.0.0.0/24 (Tenant network range)</li>
<li>8.8.8.8 (Google DNS server)</li>
</ul>
<p>First we need to install some dependencies and packages needed for next
steps.</p>
<div class="highlight"><pre><span></span>sudo yum install -y gcc python-setuptools python-devel git libffi-devel openssl-devel wget
</pre></div>


<p>Install pip</p>
<div class="highlight"><pre><span></span>easy_install pip
</pre></div>


<p>Clone Magnum source code from OpenStack git repository, ensure you use
Liberty branch, if not, Magnum dependencies will break all OpenStack
services dependencies and lost your current environment (Trust me, i'm
talking from my own experience)</p>
<div class="highlight"><pre><span></span>git clone https://git.openstack.org/openstack/magnum -b stable/liberty
</pre></div>


<p>Move to your newly created folder and install Magnum (dependency
requirements and Magnum)</p>
<div class="highlight"><pre><span></span>cd magnum
sudo pip install -e .
</pre></div>


<p>Once Magnum is installed, create Magnum database and Magnum user</p>
<div class="highlight"><pre><span></span>mysql -uroot -p
CREATE DATABASE IF NOT EXISTS magnum DEFAULT CHARACTER SET utf8;
GRANT ALL PRIVILEGES ON magnum.* TO&#39;magnum&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;temporal&#39;;
GRANT ALL PRIVILEGES ON magnum.* TO&#39;magnum&#39;@&#39;%&#39; IDENTIFIED BY &#39;temporal&#39;;
</pre></div>


<p>Create Magnum folder and copy sample configuration files.</p>
<div class="highlight"><pre><span></span>mkdir /etc/magnum
sudo cp etc/magnum/magnum.conf.sample /etc/magnum/magnum.conf
sudo cp etc/magnum/policy.json /etc/magnum/policy.json
</pre></div>


<p>Edit Magnum main configuration file</p>
<div class="highlight"><pre><span></span>vi /etc/magnum/magnum.conf
</pre></div>


<p>Configure messaging backend to RabbitMQ</p>
<div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>

<span class="na">rpc_backend</span> <span class="o">=</span> <span class="s">rabbit</span>
<span class="na">notification_driver</span> <span class="o">=</span> <span class="s">messaging</span>
</pre></div>


<p>Bind Magnum API port to listen on all the interfaces, you can also
especify on which IP Magnum API will be listening if you are concerned
about security risks.</p>
<div class="highlight"><pre><span></span><span class="k">[api]</span>

<span class="na">host</span> <span class="o">=</span> <span class="s">0.0.0.0</span>
</pre></div>


<p>Configure RabbitMQ backend</p>
<div class="highlight"><pre><span></span><span class="k">[oslo_messaging_rabbit]</span>

<span class="na">rabbit_host</span> <span class="o">=</span> <span class="s">192.168.200.208</span>
<span class="na">rabbit_userid</span> <span class="o">=</span> <span class="s">guest</span>
<span class="na">rabbit_password</span> <span class="o">=</span> <span class="s">guest</span>
<span class="na">rabbit_virtual_host</span> <span class="o">=</span> <span class="s">/</span>
</pre></div>


<p>Set database connection</p>
<div class="highlight"><pre><span></span><span class="k">[database]</span>

<span class="na">connection</span><span class="o">=</span><span class="s">mysql://magnum:temporal@192.168.200.208/magnum</span>
</pre></div>


<p>Set cert_manager_type to local, this option will disable Barbican
service, you will need to create a folder (We will do it in next steps)</p>
<div class="highlight"><pre><span></span><span class="k">[certificates]</span>

<span class="na">cert_manager_type</span> <span class="o">=</span> <span class="s">local</span>
</pre></div>


<p>As all OpenStack services, Keystone authentication is required.</p>
<ul>
<li>Check what your service tenant name it is (RDO default name is
    "services" other installations usually use "service" name.</li>
</ul>
<!-- -->

<div class="highlight"><pre><span></span><span class="k">[keystone_authtoken]</span>

<span class="na">auth_uri</span><span class="o">=</span><span class="s">http://192.168.200.208:5000/v2.0</span>
<span class="na">identity_uri</span><span class="o">=</span><span class="s">http://192.168.200.208:35357</span>
<span class="na">auth_strategy</span><span class="o">=</span><span class="s">keystone</span>
<span class="na">admin_user</span><span class="o">=</span><span class="s">magnum</span>
<span class="na">admin_password</span><span class="o">=</span><span class="s">temporal</span>
<span class="na">admin_tenant_name</span><span class="o">=</span><span class="s">services</span>
</pre></div>


<p>As we saw before, create local certificates folder to avoid using
Barbican service. This is the step we previously commented</p>
<div class="highlight"><pre><span></span>mkdir -p /var/lib/magnum/certificates/
</pre></div>


<p>Clone python-magnumclient and install it, this package will provide us
commands to use Magnum</p>
<div class="highlight"><pre><span></span>git clone https://git.openstack.org/openstack/python-magnumclient -b stable/liberty
cd python-magnumclient
sudo pip install -e .
</pre></div>


<p>Create Magnum user at keystone</p>
<div class="highlight"><pre><span></span>openstack user create --password temporal magnum
</pre></div>


<p>Add admin role to Magnum user at tenant services</p>
<div class="highlight"><pre><span></span>openstack role add --project services --user magnum admin
</pre></div>


<p>Create container service</p>
<div class="highlight"><pre><span></span>openstack service create --name magnum --description &quot;Magnum Container Service&quot; container
</pre></div>


<p>Finally create Magnum endpoints</p>
<div class="highlight"><pre><span></span>openstack endpoint create --region RegionOne --publicurl &#39;http://192.168.200.208:9511/v1&#39; --adminurl &#39;http://192.168.200.208:9511/v1&#39; --internalurl &#39;http://192.168.200.208:9511/v1&#39; magnum
</pre></div>


<p>Sync Magnum database, this step will create Magnum tables at the
database</p>
<div class="highlight"><pre><span></span>magnum-db-manage --config-file /etc/magnum/magnum.conf upgrade
</pre></div>


<p>Open two terminal session and execute one command on each terminal to
start both services. If you encounter any issue, logs can be found at
these terminal</p>
<div class="highlight"><pre><span></span>magnum-api --config-file /etc/magnum/magnum.conf
magnum-conductor --config-file /etc/magnum/magnum.conf
</pre></div>


<p>Check if Magnum service is fine</p>
<div class="highlight"><pre><span></span>magnum service-list
+----+------------+------------------+-------+
| id | host       | binary           | state |
+----+------------+------------------+-------+
| 1  | controller | magnum-conductor | up    |
+----+------------+------------------+-------+
</pre></div>


<p>Download fedora atomic image</p>
<div class="highlight"><pre><span></span>wget https://fedorapeople.org/groups/magnum/fedora-21-atomic-5.qcow2
</pre></div>


<p>Create a Glance image with Atomic.qcow2 file</p>
<div class="highlight"><pre><span></span>glance image-create --name fedora-21-atomic-5   
                   --visibility public   
                   --disk-format qcow2   
                   --os-distro fedora-atomic   
                   --container-format bare &lt; fedora-21-atomic-5.qcow2
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | cebefc0c21fb8567e662bf9f2d5b78b0     |
| container_format | bare                                 |
| created_at       | 2016-03-19T15:55:21Z                 |
| disk_format      | qcow2                                |
| id               | 7293891d-cfba-48a9-a4db-72c29c65f681 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | fedora-21-atomic-5                   |
| os_distro        | fedora-atomic                        |
| owner            | e3cca42ed57745148e0c342a000d99e9     |
| protected        | False                                |
| size             | 891355136                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2016-03-19T15:55:28Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+
</pre></div>


<p>Create a ssh key if not exists, this command won't create a new ssh key
if already exists</p>
<div class="highlight"><pre><span></span>test -f ~/.ssh/id_rsa.pub || ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa
</pre></div>


<p>Add the key to nova, mine is called egonzalez</p>
<div class="highlight"><pre><span></span>nova keypair-add --pub-key ~/.ssh/id_rsa.pub egonzalez
</pre></div>


<p>Now we are going to test our new Magnum service, you have various
methods to do it.<br>
I will use Docker Swarm method because is the simplest one for this
demo purposes. Go through Magnum documentation to check other container
methods as Kubernetes is.</p>
<p>Create a baymodel with atomic image and swarm, select a flavor with at
least 10GB of disk</p>
<div class="highlight"><pre><span></span>magnum baymodel-create --name demoswarmbaymodel   
                      --image-id fedora-21-atomic-5   
                      --keypair-id egonzalez   
                      --external-network-id public   
                      --dns-nameserver 8.8.8.8   
                      --flavor-id testflavor   
                      --docker-volume-size 1   
                      --coe swarm
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| http_proxy          | None                                 |
| updated_at          | None                                 |
| master_flavor_id    | None                                 |
| fixed_network       | None                                 |
| uuid                | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| no_proxy            | None                                 |
| https_proxy         | None                                 |
| tls_disabled        | False                                |
| keypair_id          | egonzalez                            |
| public              | False                                |
| labels              | {}                                   |
| docker_volume_size  | 1                                    |
| external_network_id | public                               |
| cluster_distro      | fedora-atomic                        |
| image_id            | fedora-21-atomic-5                   |
| registry_enabled    | False                                |
| apiserver_port      | None                                 |
| name                | demoswarmbaymodel                    |
| created_at          | 2016-03-19T17:22:43+00:00            |
| network_driver      | None                                 |
| ssh_authorized_key  | None                                 |
| coe                 | swarm                                |
| flavor_id           | testflavor                           |
| dns_nameserver      | 8.8.8.8                              |
+---------------------+--------------------------------------+
</pre></div>


<p>Create a bay with the previous bay model, we are going to create one
master node and one worker, specify all that apply to your environment</p>
<div class="highlight"><pre><span></span>magnum bay-create --name demoswarmbay --baymodel demoswarmbaymodel --master-count 1 --node-count 1
+--------------------+--------------------------------------+
| Property           | Value                                |
+--------------------+--------------------------------------+
| status             | None                                 |
| uuid               | a2388916-db30-41bf-84eb-df0b65979eaf |
| status_reason      | None                                 |
| created_at         | 2016-03-19T17:23:00+00:00            |
| updated_at         | None                                 |
| bay_create_timeout | 0                                    |
| api_address        | None                                 |
| baymodel_id        | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| node_count         | 1                                    |
| node_addresses     | None                                 |
| master_count       | 1                                    |
| discovery_url      | None                                 |
| name               | demoswarmbay                         |
+--------------------+--------------------------------------+
</pre></div>


<p>Check bay status, for now it should be in CREATE_IN_PROGRESS state</p>
<div class="highlight"><pre><span></span>magnum bay-show demoswarmbay
+--------------------+--------------------------------------+
| Property           | Value                                |
+--------------------+--------------------------------------+
| status             | CREATE_IN_PROGRESS                   |
| uuid               | a2388916-db30-41bf-84eb-df0b65979eaf |
| status_reason      |                                      |
| created_at         | 2016-03-19T17:23:00+00:00            |
| updated_at         | 2016-03-19T17:23:01+00:00            |
| bay_create_timeout | 0                                    |
| api_address        | None                                 |
| baymodel_id        | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| node_count         | 1                                    |
| node_addresses     | []                                   |
| master_count       | 1                                    |
| discovery_url      | None                                 |
| name               | demoswarmbay                         |
+--------------------+--------------------------------------+
</pre></div>


<p>If all is going fine, nova should have two new instances(in ACTIVE
state), one for the master node and second for the worker.</p>
<div class="highlight"><pre><span></span>nova list
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
| ID                                   | Name                                                  | Status | Task State | Power State | Networks                                                                      |
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
| e38eb88c-bb6b-427d-a2c5-cdfe868796f0 | de-44kx2l4q4wc-0-d6j5svvjxmne-swarm_node-xafkm2jskf5j | ACTIVE | -          | Running     | demoswarmbay-agf6y3qnjoyw-fixed_network-g37bcmc52akv=10.0.0.4, 192.168.100.16 |
| 5acc579d-152a-4656-9eb8-e800b7ab3bcf | demoswarmbay-agf6y3qnjoyw-swarm_master-fllwhrpuabbq   | ACTIVE | -          | Running     | demoswarmbay-agf6y3qnjoyw-fixed_network-g37bcmc52akv=10.0.0.3, 192.168.100.15 |
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
</pre></div>


<p>You can see how heat stack is going</p>
<div class="highlight"><pre><span></span>heat stack-list
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
| id                                   | stack_name                | stack_status       | creation_time       | updated_time |
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
| 3a64fa60-4df8-498f-aceb-a0cb8cfc0b18 | demoswarmbay-agf6y3qnjoyw | CREATE_IN_PROGRESS | 2016-03-19T17:22:59 | None         |
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
</pre></div>


<p>We can see what tasks are executing during stack creation</p>
<div class="highlight"><pre><span></span>heat event-list demoswarmbay-agf6y3qnjoyw
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
| resource_name                       | id                                   | resource_status_reason | resource_status    | event_time          |
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
| demoswarmbay-agf6y3qnjoyw           | 004c9388-b8ab-4541-ada8-99b65203e41d | Stack CREATE started   | CREATE_IN_PROGRESS | 2016-03-19T17:23:01 |
| master_wait_handle                  | d6f0798a-bfde-4bad-9c73-e108bd101009 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:01 |
| secgroup_manager                    | e2e0eb08-aeeb-4290-9ad5-bd20fe243f07 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:02 |
| disable_selinux                     | d7290592-ab81-4d7a-b2fa-902975904a25 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| agent_wait_handle                   | 65ec5553-56a4-4416-9748-bfa0ae35737a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| add_proxy                           | 46bdcff8-4606-406f-8c99-7f48adc4de57 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| write_docker_socket                 | ab5402ea-44af-4433-84aa-a63256817a9a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| make_cert                           | 3b9817a5-606f-41ab-8799-b411c017f05d | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| cfn_signal                          | 0add665a-3fdf-4408-ab15-76332aa326fe | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| remove_docker_key                   | 94f4106e-f139-4d9f-9974-8821d04be103 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| configure_swarm                     | f7e0ebd5-1893-43d1-bd29-81a7e39de0c0 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| extrouter                           | a94a8f68-c237-4dbc-9513-cdbe3de1465e | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| enable_services                     | c250f532-99bd-43d7-9d15-b2d3ae16567a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| write_docker_service                | 2c9d8954-4446-4578-a871-0910e8996571 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| cloud_init_wait_handle              | 6cc51d2d-56e9-458b-a21b-bc553e0c8291 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| fixed_network                       | 3125395f-c689-4481-bf01-94bb2f701993 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:07 |
| agent_wait_handle                   | 2db801e8-c2b5-47b0-ac16-122dba3a22d6 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| remove_docker_key                   | 75e2c7a6-a2ce-4026-aeeb-739c4a522f48 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| secgroup_manager                    | ac51a029-26c1-495a-bc13-232cfb8c1060 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| write_docker_socket                 | 58e08b52-a12a-43e9-b41d-071750294024 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| master_wait_handle                  | 3e741b76-6470-47d4-b13e-3f8f446be53c | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| cfn_signal                          | 96c26b4f-1e99-478e-a8e5-9dcc4486e1b3 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| enable_services                     | beedc358-ee72-4b34-a6b9-1b47ffc15306 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| add_proxy                           | caae3a07-d5f1-4eb0-8a82-02ea634f77ae | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| make_cert                           | 79363643-e5e4-4d1b-ad8a-5a56e1f6a8e7 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| cloud_init_wait_handle              | 0457b008-6da8-44fd-abef-cb99bd4d0518 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| configure_swarm                     | baf1e089-c627-4b24-a571-63b3c9c14e28 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| extrouter                           | 184614d9-2280-4cb4-9253-f538463dbdf4 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| write_docker_service                | 80e66b4e-d40a-4243-bb27-0d2a6b68651f | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| disable_selinux                     | d8a64822-2571-4dcf-9da5-b3ec73e771eb | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| fixed_network                       | 528b0ced-23f6-4c22-8cbc-357ba0ee5bc5 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| write_swarm_manager_failure_service | 9fa100a3-b4a9-465c-8b33-dd000cb4866a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| write_swarm_agent_failure_service   | a7c09833-929e-4711-a3e9-39923d23b2f2 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| fixed_subnet                        | 23d8b0a6-a7a3-4f71-9c18-ba6255cf071a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| write_swarm_master_service          | d24a6099-3cad-41ce-8d4b-a7ad0661aaea | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:11 |
| fixed_subnet                        | 1a2b7397-1d09-4544-bb9f-985c2f64cb09 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_manager_failure_service | 615a2a7a-5266-487b-bbe1-fcaa82f43243 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_agent_failure_service   | 3f8c54b4-6644-49a0-ad98-9bc6b4332a07 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_master_service          | 2f58b3c8-d1cc-4590-a328-0e775e495bcf | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| extrouter_inside                    | f3da7f2f-643e-4f29-a00f-d2595d7faeaf | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:14 |
| swarm_master_eth0                   | 1d6a510d-520c-4796-8990-aa8f7dd59757 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:16 |
| swarm_master_eth0                   | 3fd85913-7399-49be-bb46-5085ff953611 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:19 |
| extrouter_inside                    | 33749e30-cbea-4093-b36a-94967e299002 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:19 |
| write_heat_params                   | 054e0af5-e3e0-4bc0-92b5-b40aeedc39ab | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:19 |
| swarm_nodes                         | df7af58c-8148-4b51-bd65-b0734d9051b5 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:20 |
| write_swarm_agent_service           | ab1e8b1e-2837-4693-b791-e1311f85fa63 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:21 |
| swarm_master_floating               | d99ffe66-cb02-4279-99dc-a1f3e2ca817c | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:22 |
| write_heat_params                   | 33d9999f-6c93-453d-8565-ac99db021f8f | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| write_swarm_agent_service           | 02a1b7f6-2660-4345-ad08-42b66ffaaad5 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| swarm_master_floating               | 8ce6ecd8-c421-4e4a-ab81-cba4b5ccedf4 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| swarm_master_init                   | 3787dcc8-e644-412b-859b-63a434b9ee6c | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:26 |
| swarm_master_init                   | a1dd67bb-49c7-4507-8af0-7758b76b57e1 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:28 |
| swarm_master                        | d12b915e-3087-4e17-9954-8233926b504b | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:29 |
| swarm_master                        | a34ad52a-def7-460b-b5b7-410000207b3e | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:48 |
| master_wait_condition               | 0c9331a4-8ad0-46e0-bf2a-35943021a1a3 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
| cloud_init_wait_condition           | de3707a0-f46a-44a9-b4b8-ff50e12cc77f | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
| agent_wait_condition                | a1a810a4-9c19-4983-aaa8-e03f308c1e39 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
</pre></div>


<p>Once all tasks are completed, we can create containers in the bay we
created in previous steps.</p>
<div class="highlight"><pre><span></span>magnum container-create --name demo-container   
                       --image docker.io/cirros:latest   
                       --bay demoswarmbay   
                       --command &quot;ping -c 4 192.168.100.2&quot;
+------------+----------------------------------------+
| Property   | Value                                  |
+------------+----------------------------------------+
| uuid       | 36595858-8657-d465-3e5a-dfcddad8a238   |
| links      | ...                                    |
| bay_uuid   | a2388916-db30-41bf-84eb-df0b65979eaf   |
| updated_at | None                                   |
| image      | cirros                                 |
| command    | ping -c 4 192.168.100.2                |
| created_at | 2016-03-19T17:30:00+00:00              |
| name       | demo-container                         |
+------------+----------------------------------------+
</pre></div>


<p>Container is created, but not started.<br>
Start the container</p>
<div class="highlight"><pre><span></span>magnum container-start demo-container
</pre></div>


<p>Check container logs, you should see 4 pings succeed to our external
router gateway.</p>
<div class="highlight"><pre><span></span>magnum container-logs demo-container

PING 192.168.100.2 (192.168.100.2) 56(84) bytes of data.
64 bytes from 192.168.100.2: icmp_seq=1 ttl=64 time=0.083 ms
64 bytes from 192.168.100.2: icmp_seq=2 ttl=64 time=0.068 ms
64 bytes from 192.168.100.2: icmp_seq=3 ttl=64 time=0.043 ms
64 bytes from 192.168.100.2: icmp_seq=4 ttl=64 time=0.099 ms
</pre></div>


<p>You can delete the container</p>
<div class="highlight"><pre><span></span>magnum container-delete demo-container
</pre></div>


<p>While doing this demo, i missed adding branch name while cloning Magnum
source code, when i installed Magnum all package dependencies where
installed from master, who was Mitaka instead of Liberty, which broke my
environment.</p>
<p>I suffered the following issues:</p>
<p>Issues with packages</p>
<div class="highlight"><pre><span></span><span class="n">ImportError</span><span class="o">:</span> <span class="n">No</span> <span class="n">module</span> <span class="n">named</span> <span class="n">MySQLdb</span>
</pre></div>


<p>Was solved installing MySQL-python from pip instead of yum</p>
<div class="highlight"><pre><span></span>pip install MySQL-python
</pre></div>


<p>Issues with policies, admin privileges weren't recognized by Magnum api.</p>
<div class="highlight"><pre><span></span><span class="n">PolicyNotAuthorized</span><span class="o">:</span> <span class="n">magnum</span><span class="o">-</span><span class="n">service</span><span class="o">:</span><span class="n">get_all</span><span class="o">{{</span> <span class="n">bunch</span> <span class="n">of</span> <span class="n">stuff</span> <span class="o">}}</span> <span class="n">disallowed</span> <span class="n">by</span> <span class="n">policy</span>
</pre></div>


<p>Was solved removing admin_api rule at Magnum policy.json file</p>
<div class="highlight"><pre><span></span>vi /etc/magnum/policy.json

#    &quot;admin_api&quot;: &quot;rule:context_is_admin&quot;,
</pre></div>


<p>Unfortunately, nova was completely broken and it was not working at all,
so i installed a new environment and added branch while cloning source
code.<br>
Next issue i found was Barbican, who was not installed, i used the
steps mentioned at this post to solve this issue.</p>
<p>Hope this guide helps you integrating Magnum Container as a Service in
OpenStack.</p>
<p>Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code.html">posted at 19:22</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/openstack.html" rel="tag">OpenStack</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/container.html" class="tags">container</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/docker.html" class="tags">docker</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/guide.html" class="tags">guide</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/install.html" class="tags">install</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/liberty.html" class="tags">liberty</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/magnum.html" class="tags">magnum</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/manual.html" class="tags">manual</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags selected">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rdo.html" class="tags">rdo</a>
                </div>
		<a href="https://egonzalez90.github.io/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Mar 02, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/nova-vnc-flows-under-the-hood.html" rel="bookmark" title="Permanent Link to &quot;Nova VNC flows under the hood&quot;">Nova VNC flows under the hood</a>
                </h2>

                
                

                <p>Most OpenStack deployments has a VNC console implemented with
nova-novncproxy. This service gives the final user the ability to log
into their instances in a web based method through a browser.</p>
<p>At this post i'm going to show how a vnc console request works under the
hood while using the following command or lauching a vnc session through
Horizon.</p>
<div class="highlight"><pre><span></span># nova get-vnc-console INSTANCE novnc
</pre></div>


<p>First of all, a user connects to NOVA and issues a VNC console request
for an instance. Nova API needs to validate the user issuing an
authentication request to keystone.<br>
The user receives a token with nova's endpoint URL in the catalog, with
that endpoint and the token, the user makes a request against nova
calling for a VNC session.</p>
<div class="highlight"><pre><span></span>GET http://192.168.200.208:5000/v2.0 -H &quot;Accept: application/json&quot; -H
</pre></div>


<p>"User-Agent: python-keystoneclient"</p>
<div class="highlight"><pre><span></span>GET http://192.168.200.208:8774/v2/ -H &quot;User-Agent: python-novaclient&quot; -H
</pre></div>


<p>"Accept: application/json" -H "X-Auth-Token: {SHA1}3b6262df9eaba5da33c1004805187806322201f1"</p>
<p>If a name instead of an instance ID is used in the request, Nova need to
check his database to match that name with his corresponding ID, as we
can see in the following request.</p>
<div class="highlight"><pre><span></span>GET http://192.168.200.208:8774/v2/ee84411cdb8148d28674b129ef482f31/servers?name=test1
</pre></div>


<p>-H "User-Agent: python-novaclient" -H "Accept: application/json" <br>
   -H "X-Auth-Token: {SHA1}3b6262df9eaba5da33c1004805187806322201f1"</p>
<div class="highlight"><pre><span></span>RESP BODY: {&quot;servers&quot;: [{&quot;id&quot;: &quot;9165dbda-f54e-4186-b2cb-e6ca05ac53ee&quot;,
</pre></div>


<p>"links": [{"href": "http://192.168.200.208:8774/v2/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee", "rel": "self"},<br>
    {"href": "http://192.168.200.208:8774/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee", <br>
   "rel": "bookmark"}], "name": "test1"}]}</p>
<p>Once the ID is matched with the name, Nova check information about the
instance (I thought it was to validate if is in ACTIVE status, but i
realized that even when is in STOPPED status the request is made it
anyway).</p>
<div class="highlight"><pre><span></span>GET http://192.168.200.208:8774/v2/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee  
-H &quot;User-Agent: python-novaclient&quot; -H &quot;Accept: application/json&quot;   
-H &quot;X-Auth-Token: {SHA1}3b6262df9eaba5da33c1004805187806322201f1&quot;

RESP BODY: {&quot;server&quot;: {&quot;status&quot;: &quot;ACTIVE&quot;, &quot;updated&quot;: &quot;2016-03-02T17:28:45Z&quot;, &quot;hostId&quot;: &quot;ca3a874dcad9079fcc6a0b10b0e2efaa394bc66b5335197fdd9c2498&quot;, &quot;OS-EXT-SRV-ATTR:host&quot;: &quot;liberty&quot;, &quot;addresses&quot;: {&quot;private&quot;: [{&quot;OS-EXT-IPS-MAC:mac_addr&quot;: &quot;fa:16:3e:aa:1c:32&quot;, &quot;version&quot;: 4, &quot;addr&quot;: &quot;10.0.0.6&quot;, &quot;OS-EXT-IPS:type&quot;: &quot;fixed&quot;}]}, &quot;links&quot;: [{&quot;href&quot;: &quot;http://192.168.200.208:8774/v2/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee&quot;, &quot;rel&quot;: &quot;self&quot;}, {&quot;href&quot;: &quot;http://192.168.200.208:8774/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee&quot;, &quot;rel&quot;: &quot;bookmark&quot;}], &quot;key_name&quot;: null, &quot;image&quot;: {&quot;id&quot;: &quot;bf31eadd-c5f4-40f8-9ddb-30f688ca5e5f&quot;, &quot;links&quot;: [{&quot;href&quot;: &quot;http://192.168.200.208:8774/ee84411cdb8148d28674b129ef482f31/images/bf31eadd-c5f4-40f8-9ddb-30f688ca5e5f&quot;, &quot;rel&quot;: &quot;bookmark&quot;}]}, &quot;OS-EXT-STS:task_state&quot;: null, &quot;OS-EXT-STS:vm_state&quot;: &quot;active&quot;, &quot;OS-EXT-SRV-ATTR:instance_name&quot;: &quot;instance-0000000a&quot;, &quot;OS-SRV-USG:launched_at&quot;: &quot;2016-03-02T17:28:45.000000&quot;, &quot;OS-EXT-SRV-ATTR:hypervisor_hostname&quot;: &quot;liberty&quot;, &quot;flavor&quot;: {&quot;id&quot;: &quot;1&quot;, &quot;links&quot;: [{&quot;href&quot;: &quot;http://192.168.200.208:8774/ee84411cdb8148d28674b129ef482f31/flavors/1&quot;, &quot;rel&quot;: &quot;bookmark&quot;}]}, &quot;id&quot;: &quot;9165dbda-f54e-4186-b2cb-e6ca05ac53ee&quot;, &quot;security_groups&quot;: [{&quot;name&quot;: &quot;default&quot;}], &quot;OS-SRV-USG:terminated_at&quot;: null, &quot;OS-EXT-AZ:availability_zone&quot;: &quot;nova&quot;, &quot;user_id&quot;: &quot;d9164a323be649c0a8c5c80fdd5bd585&quot;, &quot;name&quot;: &quot;test1&quot;, &quot;created&quot;: &quot;2016-03-02T17:28:34Z&quot;, &quot;tenant_id&quot;: &quot;ee84411cdb8148d28674b129ef482f31&quot;, &quot;OS-DCF:diskConfig&quot;: &quot;MANUAL&quot;, &quot;os-extended-volumes:volumes_attached&quot;: [], &quot;accessIPv4&quot;: &quot;&quot;, &quot;accessIPv6&quot;: &quot;&quot;, &quot;progress&quot;: 0, &quot;OS-EXT-STS:power_state&quot;: 1, &quot;config_drive&quot;: &quot;&quot;, &quot;metadata&quot;: {}}}
</pre></div>


<p>When we get the information, nova-api POST a request to nova-consoleauth
for a VNC console.</p>
<div class="highlight"><pre><span></span>POST http://192.168.200.208:8774/v2/ee84411cdb8148d28674b129ef482f31/servers/9165dbda-f54e-4186-b2cb-e6ca05ac53ee/action
</pre></div>


<p>-H "User-Agent: python-novaclient" -H "Content-Type: application/json" <br>
   -H "Accept: application/json" -H "X-Auth-Token: {SHA1}3b6262df9eaba5da33c1004805187806322201f1"<br>
   -d '{"os-getVNCConsole": {"type": "novnc"}}'</p>
<div class="highlight"><pre><span></span>DEBUG nova.api.openstack.wsgi [req-2201b9d6-5711-46d3-ac4d-669094f07527
</pre></div>


<p>d9164a323be649c0a8c5c80fdd5bd585 ee84411cdb8148d28674b129ef482f31 - - -] <br>
   Action: 'action', calling method: , body: {"os-getVNCConsole": {"type": "novnc"}} <br>
   _process_stack /usr/lib/python2.7/site-packages/nova/api/openstack/wsgi.py:789</p>
<p>Nova-consoleauth receives the console request and create an access URL
while generates a temporary token for the vnc console.</p>
<div class="highlight"><pre><span></span>INFO nova.consoleauth.manager [req-d4def6f9-1ab9-4626-b6a8-d81643ea5eb4 d9164a323be649c0a8c5c80fdd5bd585 ee84411cdb8148d28674b129ef482f31 - - -]
</pre></div>


<p>Received Token: 3dfcd011-28f1-4cf3-8f5c-8cd18de4560e, <br>
   {'instance_uuid': u'9165dbda-f54e-4186-b2cb-e6ca05ac53ee', <br>
   'access_url': u'http://192.168.200.208:6080/vnc_auto.html?token=3dfcd011-28f1-4cf3-8f5c-8cd18de4560e',<br>
    'token': u'3dfcd011-28f1-4cf3-8f5c-8cd18de4560e', 'last_activity_at': 1456940028.356214, <br>
   'internal_access_path': None, 'console_type': u'novnc', 'host': u'liberty', 'port': u'5900'}</p>
<p>Nova-consoleauth answer to nova-api who also answers to the user with an
access URL.<br>
This URL got the following content on it:</p>
<ul>
<li>HTTP or HTTPS connection to nova-novncproxy IP</li>
<li>Nova-novncproxy port</li>
<li>A token to validate the VNC connection</li>
</ul>
<!-- -->

<div class="highlight"><pre><span></span>RESP BODY: {&quot;console&quot;: {&quot;url&quot;: &quot;http://192.168.200.208:6080/vnc_auto.html?token=3dfcd011-28f1-4cf3-8f5c-8cd18de4560e&quot;, &quot;type&quot;: &quot;novnc&quot;}}

+-------+--------------------------------------------------------------------------------------+
| Type  | Url                                                                                  |
+-------+--------------------------------------------------------------------------------------+
| novnc | http://192.168.200.208:6080/vnc_auto.html?token=3dfcd011-28f1-4cf3-8f5c-8cd18de4560e |
+-------+--------------------------------------------------------------------------------------+
</pre></div>


<p>Until now, nova-novncproxy service can be stopped or isn't used at all,
is at this point the when proxy server enter into the game.<br>
The user connects through a web browser to the nova-novncproxy's URL
provided by nova before.</p>
<div class="highlight"><pre><span></span>DEBUG nova.console.websocketproxy [-] 192.168.200.1:
</pre></div>


<p>new handler Process vmsg /usr/lib/python2.7/site-packages/websockify/websocket.py:828</p>
<p>Nova-vncproxy validate the issued token with the URL against
nova-consoleauth.</p>
<div class="highlight"><pre><span></span>nova.consoleauth.manager [req-399c7b58-700a-4779-b215-b12d10056813 - - - - -]
</pre></div>


<p>Checking Token: 3dfcd011-28f1-4cf3-8f5c-8cd18de4560e, True</p>
<p>When the token is validated, nova-novncproxy maps compute's node private
IP (at this case port 5900) with the nova-novncproxy public IP(6080
port).</p>
<div class="highlight"><pre><span></span>INFO nova.console.websocketproxy [req-399c7b58-700a-4779-b215-b12d10056813 - - - - -]  
  7: connect info: {u&#39;instance_uuid&#39;: u&#39;9165dbda-f54e-4186-b2cb-e6ca05ac53ee&#39;, u&#39;
</pre></div>


<p>internal_access_path': None, u'last_activity_at': 1456940028.356214, <br>
   u'console_type': u'novnc', u'host': u'liberty', u'token': u'3dfcd011-28f1-4cf3-8f5c-8cd18de4560e', <br>
   u'access_url': u'http://192.168.200.208:6080/vnc_auto.html?token=3dfcd011-28f1-4cf3-8f5c-8cd18de4560e'<br>
   , u'port': u'5900'}</p>
<p>We can see how the python novncproxy process binds both IPs/port.</p>
<div class="highlight"><pre><span></span># ps aux | grep vnc
nova     14840  1.2  0.7 362096 41000 ?        S    18:53   0:14 /usr/bin/python2 /usr/bin/nova-novncproxy --web /usr/share/novnc/

# netstat -putona | grep 14840
tcp        0      0 192.168.200.208:6080    192.168.200.1:59918     ESTABLISHED 14840/python2        keepalive (3,13/0/0)
tcp        0      0 192.168.122.73:57764    192.168.122.73:5900     ESTABLISHED 14840/python2        keepalive (3,13/0/0)
</pre></div>


<p>Nova-novncproxy starts the connection between the instance and user's
browser session.</p>
<div class="highlight"><pre><span></span>INFO nova.console.websocketproxy [req-399c7b58-700a-4779-b215-b12d10056813 - - - - -]  
  7: connecting to: liberty:5900
</pre></div>


<p>Libvirt connects a vnc console into the instance, as we can see at the
xml provided by virsh command.<br>
Also, port 5900 now is binded at qemu-kvm process.</p>
<div class="highlight"><pre><span></span># virsh dumpxml 2
...
<span class="nt">&lt;graphics</span> <span class="na">type=</span><span class="s">&#39;vnc&#39;</span> <span class="na">port=</span><span class="s">&#39;5900&#39;</span> <span class="na">autoport=</span><span class="s">&#39;yes&#39;</span> <span class="na">listen=</span><span class="s">&#39;0.0.0.0&#39;</span> <span class="na">keymap=</span><span class="s">&#39;en-us&#39;</span><span class="nt">&gt;</span>
     <span class="nt">&lt;listen</span> <span class="na">type=</span><span class="s">&#39;address&#39;</span> <span class="na">address=</span><span class="s">&#39;0.0.0.0&#39;</span><span class="nt">/&gt;</span>
   <span class="nt">&lt;/graphics&gt;</span>
...

# netstat -putona | grep 5900
tcp        0      0 0.0.0.0:5900            0.0.0.0:*               LISTEN      5910/qemu-kvm        off (0.00/0/0)
tcp        0      0 192.168.122.73:5900     192.168.122.73:57702    ESTABLISHED 5910/qemu-kvm        off (0.00/0/0)
tcp        0      0 192.168.122.73:57702    192.168.122.73:5900     ESTABLISHED 11118/python2        keepalive (1,92/0/0)
</pre></div>


<p>Nova-novncproxy keeps the connection alive until browser session ends.</p>
<div class="highlight"><pre><span></span>DEBUG nova.console.websocketproxy [-]
</pre></div>


<p>Reaing zombies, active child count is 1 vmsg /usr/lib/python2.7/site-packages/websockify/websocket.py:828</p>
<p>When a token is not valid while authenticating against nova-consoleauth,
we can see a message like the following.</p>
<div class="highlight"><pre><span></span>INFO nova.console.websocketproxy [req-9164b32d-3ce1-441b-82c7-6c23c9a354d0 - - - - -]
</pre></div>


<p>handler exception: The token '3dfcd011-28f1-4cf3-8f5c-8cd18de4560e' is invalid or has expired</p>
<p>Regards.<br>
Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/nova-vnc-flows-under-the-hood.html">posted at 22:26</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/openstack.html" rel="tag">OpenStack</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/auth.html" class="tags">auth</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/console.html" class="tags">console</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/consoleauth.html" class="tags">consoleauth</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/flow.html" class="tags">flow</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/flows.html" class="tags">flows</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/how.html" class="tags">how</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/internal.html" class="tags">internal</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/liberty.html" class="tags">liberty</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/nova.html" class="tags">nova</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/novnc.html" class="tags">novnc</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/novncproxy.html" class="tags">novncproxy</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags selected">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/traffic.html" class="tags">traffic</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/vnc.html" class="tags">vnc</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/works.html" class="tags">works</a>
                </div>
		<a href="https://egonzalez90.github.io/nova-vnc-flows-under-the-hood.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Feb 24, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/ansible-ini_file-module-simplifying-your-devops-life.html" rel="bookmark" title="Permanent Link to &quot;Ansible ini_file module, simplifying your DevOps life&quot;">Ansible ini_file module, simplifying your DevOps life</a>
                </h2>

                
                

                <p>If you don't read docs, one day you'll realize that your an idiot as i
am|was.</p>
<p>A few days back, I've realized that i was using wrong all Ansible
modules power since i started with it. What happened?</p>
<p>Most of the time i use Ansible is related to OpenStack configuration
jobs. Almost, all OpenStack projects use INI formatted files for their
configuration files.<br>
When i started using Ansible, I searched on Google how to configure any
kind of file with Ansible modules. Almost all blogs/forums that i saw,
talked about lineinfile module. So i used these guidelines on my next
few months, now i realize that i was using in the wrong way Ansible
modules.</p>
<p>Ansible have a module called ini_file, you change values inside INI
formatted files in a easy way , you don't need to use complicated
regular expressions to change a value in a file.</p>
<p>Here you have ini_file module usage docs:
<a href="http://docs.ansible.com/ansible/ini_file_module.html">http://docs.ansible.com/ansible/ini_file_module.html</a></p>
<p>We are going to change Neutron user password in his dump config file, so
we create a simple task on which we can see how ini_file module can be
used.</p>
<div class="highlight"><pre><span></span>- hosts: localhost
  tasks:
  - name: Change neutron user password
    ini_file:
      dest: ~/neutron.conf
      section: keystone_authtoken
      option: password
      value: 12345
</pre></div>


<p>Once the task has been applied, we can see how the values are applied in
a proper ini style.</p>
<div class="highlight"><pre><span></span>cat neutron.conf
[keystone_authtoken]
password = 12345
</pre></div>


<p>How many times you need to make a change in an INI formatted
configuration file with Ansible and used lineinfile module?<br>
If the answer is many times, it's OK, you are a dump like me.</p>
<p>Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/ansible-ini_file-module-simplifying-your-devops-life.html">posted at 20:00</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/linux-various.html" rel="tag">Linux, Various</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/-configure.html" class="tags">--configure</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/ansible.html" class="tags">ansible</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/config.html" class="tags">config</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/file.html" class="tags">file</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/idiot.html" class="tags">idiot</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/ini.html" class="tags">ini</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/ini_file.html" class="tags">ini_file</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags selected">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/style.html" class="tags">style</a>
                </div>
		<a href="https://egonzalez90.github.io/ansible-ini_file-module-simplifying-your-devops-life.html#disqus_thread">Click to read and post comments</a>
            </article>

                <div class="clear"></div>
                <div class="pages">

                    <a href="https://egonzalez90.github.io/page/2" class="prev_page">&larr;&nbsp;Previous</a>
                    <a href="https://egonzalez90.github.io/page/4" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 3 of 15</span>
                </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="https://egonzalez90.github.io/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>