<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenStack Stuff | articles tagged "guide"</title>
    <link rel="shortcut icon" type="image/png" href="https://egonzalez90.github.io/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="https://egonzalez90.github.io/favicon.ico">
    <link href="https://egonzalez90.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="OpenStack Stuff Full Atom Feed" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Eduardo Gonzalez" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li class="ephemeral selected"><a href="https://egonzalez90.github.io/tag/guide.html">guide</a></li>
                <li><a href="https://egonzalez90.github.io/">Home</a></li>
                <li><a href="https://docs.openstack.org">OpenStack</a></li>
                <li><a href="https://www.linkedin.com/in/eduardogonzalezgutierrez">Linkedin</a></li>
                <li><a href="https://github.com/egonzalez90">Github</a></li>
                <li><a href="https://egonzalez90.github.io/archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="https://egonzalez90.github.io/">OpenStack Stuff</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Mar 24, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code.html" rel="bookmark" title="Permanent Link to &quot;Magnum in RDO OpenStack - Liberty Manual Installation from source code&quot;">Magnum in RDO OpenStack - Liberty Manual Installation from source code</a>
                </h2>

                
                

                <p>Want to install Magnum (Containers as a Service) in an OpenStack
environment based on packages from RDO project?<br>
Here are the steps to do it:</p>
<p>Primary steps are the same as official Magnum guide, major differences
come from DevStack or manual installations vs packages from RDO
project.<br>
Also, some of the steps are explained to show how Magnum should work,
as well this guide can help you understand Magnum integration with your
current environment.<br>
I'm not going to use Barbican service for certs management, you will
see how to use Magnum without Barbican too.</p>
<ul>
<li>For now, there is not RDO packages for magnum, so we are going to
    install it from source code.</li>
<li>As i know, currently magnum packages are under development and will
    be added in future OpenStack versions to RDO project packages.
    (Probably Mitaka or Newton)</li>
</ul>
<p>Passwords used at this demo are:</p>
<ul>
<li>temporal (Databases and OpenStack users)</li>
<li>guest (RabbitMQ)</li>
</ul>
<p>IPs used are:</p>
<ul>
<li>192.168.200.208 (Service APIs)</li>
<li>192.168.100.0/24 (External network range)</li>
<li>10.0.0.0/24 (Tenant network range)</li>
<li>8.8.8.8 (Google DNS server)</li>
</ul>
<p>First we need to install some dependencies and packages needed for next
steps.</p>
<div class="highlight"><pre><span></span>sudo yum install -y gcc python-setuptools python-devel git libffi-devel openssl-devel wget
</pre></div>


<p>Install pip</p>
<div class="highlight"><pre><span></span>easy_install pip
</pre></div>


<p>Clone Magnum source code from OpenStack git repository, ensure you use
Liberty branch, if not, Magnum dependencies will break all OpenStack
services dependencies and lost your current environment (Trust me, i'm
talking from my own experience)</p>
<div class="highlight"><pre><span></span>git clone https://git.openstack.org/openstack/magnum -b stable/liberty
</pre></div>


<p>Move to your newly created folder and install Magnum (dependency
requirements and Magnum)</p>
<div class="highlight"><pre><span></span>cd magnum
sudo pip install -e .
</pre></div>


<p>Once Magnum is installed, create Magnum database and Magnum user</p>
<div class="highlight"><pre><span></span>mysql -uroot -p
CREATE DATABASE IF NOT EXISTS magnum DEFAULT CHARACTER SET utf8;
GRANT ALL PRIVILEGES ON magnum.* TO&#39;magnum&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;temporal&#39;;
GRANT ALL PRIVILEGES ON magnum.* TO&#39;magnum&#39;@&#39;%&#39; IDENTIFIED BY &#39;temporal&#39;;
</pre></div>


<p>Create Magnum folder and copy sample configuration files.</p>
<div class="highlight"><pre><span></span>mkdir /etc/magnum
sudo cp etc/magnum/magnum.conf.sample /etc/magnum/magnum.conf
sudo cp etc/magnum/policy.json /etc/magnum/policy.json
</pre></div>


<p>Edit Magnum main configuration file</p>
<div class="highlight"><pre><span></span>vi /etc/magnum/magnum.conf
</pre></div>


<p>Configure messaging backend to RabbitMQ</p>
<div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>

<span class="na">rpc_backend</span> <span class="o">=</span> <span class="s">rabbit</span>
<span class="na">notification_driver</span> <span class="o">=</span> <span class="s">messaging</span>
</pre></div>


<p>Bind Magnum API port to listen on all the interfaces, you can also
especify on which IP Magnum API will be listening if you are concerned
about security risks.</p>
<div class="highlight"><pre><span></span><span class="k">[api]</span>

<span class="na">host</span> <span class="o">=</span> <span class="s">0.0.0.0</span>
</pre></div>


<p>Configure RabbitMQ backend</p>
<div class="highlight"><pre><span></span><span class="k">[oslo_messaging_rabbit]</span>

<span class="na">rabbit_host</span> <span class="o">=</span> <span class="s">192.168.200.208</span>
<span class="na">rabbit_userid</span> <span class="o">=</span> <span class="s">guest</span>
<span class="na">rabbit_password</span> <span class="o">=</span> <span class="s">guest</span>
<span class="na">rabbit_virtual_host</span> <span class="o">=</span> <span class="s">/</span>
</pre></div>


<p>Set database connection</p>
<div class="highlight"><pre><span></span><span class="k">[database]</span>

<span class="na">connection</span><span class="o">=</span><span class="s">mysql://magnum:temporal@192.168.200.208/magnum</span>
</pre></div>


<p>Set cert_manager_type to local, this option will disable Barbican
service, you will need to create a folder (We will do it in next steps)</p>
<div class="highlight"><pre><span></span><span class="k">[certificates]</span>

<span class="na">cert_manager_type</span> <span class="o">=</span> <span class="s">local</span>
</pre></div>


<p>As all OpenStack services, Keystone authentication is required.</p>
<ul>
<li>Check what your service tenant name it is (RDO default name is
    "services" other installations usually use "service" name.</li>
</ul>
<!-- -->

<div class="highlight"><pre><span></span><span class="k">[keystone_authtoken]</span>

<span class="na">auth_uri</span><span class="o">=</span><span class="s">http://192.168.200.208:5000/v2.0</span>
<span class="na">identity_uri</span><span class="o">=</span><span class="s">http://192.168.200.208:35357</span>
<span class="na">auth_strategy</span><span class="o">=</span><span class="s">keystone</span>
<span class="na">admin_user</span><span class="o">=</span><span class="s">magnum</span>
<span class="na">admin_password</span><span class="o">=</span><span class="s">temporal</span>
<span class="na">admin_tenant_name</span><span class="o">=</span><span class="s">services</span>
</pre></div>


<p>As we saw before, create local certificates folder to avoid using
Barbican service. This is the step we previously commented</p>
<div class="highlight"><pre><span></span>mkdir -p /var/lib/magnum/certificates/
</pre></div>


<p>Clone python-magnumclient and install it, this package will provide us
commands to use Magnum</p>
<div class="highlight"><pre><span></span>git clone https://git.openstack.org/openstack/python-magnumclient -b stable/liberty
cd python-magnumclient
sudo pip install -e .
</pre></div>


<p>Create Magnum user at keystone</p>
<div class="highlight"><pre><span></span>openstack user create --password temporal magnum
</pre></div>


<p>Add admin role to Magnum user at tenant services</p>
<div class="highlight"><pre><span></span>openstack role add --project services --user magnum admin
</pre></div>


<p>Create container service</p>
<div class="highlight"><pre><span></span>openstack service create --name magnum --description &quot;Magnum Container Service&quot; container
</pre></div>


<p>Finally create Magnum endpoints</p>
<div class="highlight"><pre><span></span>openstack endpoint create --region RegionOne --publicurl &#39;http://192.168.200.208:9511/v1&#39; --adminurl &#39;http://192.168.200.208:9511/v1&#39; --internalurl &#39;http://192.168.200.208:9511/v1&#39; magnum
</pre></div>


<p>Sync Magnum database, this step will create Magnum tables at the
database</p>
<div class="highlight"><pre><span></span>magnum-db-manage --config-file /etc/magnum/magnum.conf upgrade
</pre></div>


<p>Open two terminal session and execute one command on each terminal to
start both services. If you encounter any issue, logs can be found at
these terminal</p>
<div class="highlight"><pre><span></span>magnum-api --config-file /etc/magnum/magnum.conf
magnum-conductor --config-file /etc/magnum/magnum.conf
</pre></div>


<p>Check if Magnum service is fine</p>
<div class="highlight"><pre><span></span>magnum service-list
+----+------------+------------------+-------+
| id | host       | binary           | state |
+----+------------+------------------+-------+
| 1  | controller | magnum-conductor | up    |
+----+------------+------------------+-------+
</pre></div>


<p>Download fedora atomic image</p>
<div class="highlight"><pre><span></span>wget https://fedorapeople.org/groups/magnum/fedora-21-atomic-5.qcow2
</pre></div>


<p>Create a Glance image with Atomic.qcow2 file</p>
<div class="highlight"><pre><span></span>glance image-create --name fedora-21-atomic-5   
                   --visibility public   
                   --disk-format qcow2   
                   --os-distro fedora-atomic   
                   --container-format bare &lt; fedora-21-atomic-5.qcow2
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | cebefc0c21fb8567e662bf9f2d5b78b0     |
| container_format | bare                                 |
| created_at       | 2016-03-19T15:55:21Z                 |
| disk_format      | qcow2                                |
| id               | 7293891d-cfba-48a9-a4db-72c29c65f681 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | fedora-21-atomic-5                   |
| os_distro        | fedora-atomic                        |
| owner            | e3cca42ed57745148e0c342a000d99e9     |
| protected        | False                                |
| size             | 891355136                            |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2016-03-19T15:55:28Z                 |
| virtual_size     | None                                 |
| visibility       | public                               |
+------------------+--------------------------------------+
</pre></div>


<p>Create a ssh key if not exists, this command won't create a new ssh key
if already exists</p>
<div class="highlight"><pre><span></span>test -f ~/.ssh/id_rsa.pub || ssh-keygen -t rsa -N &quot;&quot; -f ~/.ssh/id_rsa
</pre></div>


<p>Add the key to nova, mine is called egonzalez</p>
<div class="highlight"><pre><span></span>nova keypair-add --pub-key ~/.ssh/id_rsa.pub egonzalez
</pre></div>


<p>Now we are going to test our new Magnum service, you have various
methods to do it.<br>
I will use Docker Swarm method because is the simplest one for this
demo purposes. Go through Magnum documentation to check other container
methods as Kubernetes is.</p>
<p>Create a baymodel with atomic image and swarm, select a flavor with at
least 10GB of disk</p>
<div class="highlight"><pre><span></span>magnum baymodel-create --name demoswarmbaymodel   
                      --image-id fedora-21-atomic-5   
                      --keypair-id egonzalez   
                      --external-network-id public   
                      --dns-nameserver 8.8.8.8   
                      --flavor-id testflavor   
                      --docker-volume-size 1   
                      --coe swarm
+---------------------+--------------------------------------+
| Property            | Value                                |
+---------------------+--------------------------------------+
| http_proxy          | None                                 |
| updated_at          | None                                 |
| master_flavor_id    | None                                 |
| fixed_network       | None                                 |
| uuid                | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| no_proxy            | None                                 |
| https_proxy         | None                                 |
| tls_disabled        | False                                |
| keypair_id          | egonzalez                            |
| public              | False                                |
| labels              | {}                                   |
| docker_volume_size  | 1                                    |
| external_network_id | public                               |
| cluster_distro      | fedora-atomic                        |
| image_id            | fedora-21-atomic-5                   |
| registry_enabled    | False                                |
| apiserver_port      | None                                 |
| name                | demoswarmbaymodel                    |
| created_at          | 2016-03-19T17:22:43+00:00            |
| network_driver      | None                                 |
| ssh_authorized_key  | None                                 |
| coe                 | swarm                                |
| flavor_id           | testflavor                           |
| dns_nameserver      | 8.8.8.8                              |
+---------------------+--------------------------------------+
</pre></div>


<p>Create a bay with the previous bay model, we are going to create one
master node and one worker, specify all that apply to your environment</p>
<div class="highlight"><pre><span></span>magnum bay-create --name demoswarmbay --baymodel demoswarmbaymodel --master-count 1 --node-count 1
+--------------------+--------------------------------------+
| Property           | Value                                |
+--------------------+--------------------------------------+
| status             | None                                 |
| uuid               | a2388916-db30-41bf-84eb-df0b65979eaf |
| status_reason      | None                                 |
| created_at         | 2016-03-19T17:23:00+00:00            |
| updated_at         | None                                 |
| bay_create_timeout | 0                                    |
| api_address        | None                                 |
| baymodel_id        | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| node_count         | 1                                    |
| node_addresses     | None                                 |
| master_count       | 1                                    |
| discovery_url      | None                                 |
| name               | demoswarmbay                         |
+--------------------+--------------------------------------+
</pre></div>


<p>Check bay status, for now it should be in CREATE_IN_PROGRESS state</p>
<div class="highlight"><pre><span></span>magnum bay-show demoswarmbay
+--------------------+--------------------------------------+
| Property           | Value                                |
+--------------------+--------------------------------------+
| status             | CREATE_IN_PROGRESS                   |
| uuid               | a2388916-db30-41bf-84eb-df0b65979eaf |
| status_reason      |                                      |
| created_at         | 2016-03-19T17:23:00+00:00            |
| updated_at         | 2016-03-19T17:23:01+00:00            |
| bay_create_timeout | 0                                    |
| api_address        | None                                 |
| baymodel_id        | 887edbc7-0805-4796-be78-dfcddad8eb03 |
| node_count         | 1                                    |
| node_addresses     | []                                   |
| master_count       | 1                                    |
| discovery_url      | None                                 |
| name               | demoswarmbay                         |
+--------------------+--------------------------------------+
</pre></div>


<p>If all is going fine, nova should have two new instances(in ACTIVE
state), one for the master node and second for the worker.</p>
<div class="highlight"><pre><span></span>nova list
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
| ID                                   | Name                                                  | Status | Task State | Power State | Networks                                                                      |
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
| e38eb88c-bb6b-427d-a2c5-cdfe868796f0 | de-44kx2l4q4wc-0-d6j5svvjxmne-swarm_node-xafkm2jskf5j | ACTIVE | -          | Running     | demoswarmbay-agf6y3qnjoyw-fixed_network-g37bcmc52akv=10.0.0.4, 192.168.100.16 |
| 5acc579d-152a-4656-9eb8-e800b7ab3bcf | demoswarmbay-agf6y3qnjoyw-swarm_master-fllwhrpuabbq   | ACTIVE | -          | Running     | demoswarmbay-agf6y3qnjoyw-fixed_network-g37bcmc52akv=10.0.0.3, 192.168.100.15 |
+--------------------------------------+-------------------------------------------------------+--------+------------+-------------+-------------------------------------------------------------------------------+
</pre></div>


<p>You can see how heat stack is going</p>
<div class="highlight"><pre><span></span>heat stack-list
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
| id                                   | stack_name                | stack_status       | creation_time       | updated_time |
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
| 3a64fa60-4df8-498f-aceb-a0cb8cfc0b18 | demoswarmbay-agf6y3qnjoyw | CREATE_IN_PROGRESS | 2016-03-19T17:22:59 | None         |
+--------------------------------------+---------------------------+--------------------+---------------------+--------------+
</pre></div>


<p>We can see what tasks are executing during stack creation</p>
<div class="highlight"><pre><span></span>heat event-list demoswarmbay-agf6y3qnjoyw
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
| resource_name                       | id                                   | resource_status_reason | resource_status    | event_time          |
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
| demoswarmbay-agf6y3qnjoyw           | 004c9388-b8ab-4541-ada8-99b65203e41d | Stack CREATE started   | CREATE_IN_PROGRESS | 2016-03-19T17:23:01 |
| master_wait_handle                  | d6f0798a-bfde-4bad-9c73-e108bd101009 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:01 |
| secgroup_manager                    | e2e0eb08-aeeb-4290-9ad5-bd20fe243f07 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:02 |
| disable_selinux                     | d7290592-ab81-4d7a-b2fa-902975904a25 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| agent_wait_handle                   | 65ec5553-56a4-4416-9748-bfa0ae35737a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| add_proxy                           | 46bdcff8-4606-406f-8c99-7f48adc4de57 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:03 |
| write_docker_socket                 | ab5402ea-44af-4433-84aa-a63256817a9a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| make_cert                           | 3b9817a5-606f-41ab-8799-b411c017f05d | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| cfn_signal                          | 0add665a-3fdf-4408-ab15-76332aa326fe | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:04 |
| remove_docker_key                   | 94f4106e-f139-4d9f-9974-8821d04be103 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| configure_swarm                     | f7e0ebd5-1893-43d1-bd29-81a7e39de0c0 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| extrouter                           | a94a8f68-c237-4dbc-9513-cdbe3de1465e | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:05 |
| enable_services                     | c250f532-99bd-43d7-9d15-b2d3ae16567a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| write_docker_service                | 2c9d8954-4446-4578-a871-0910e8996571 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| cloud_init_wait_handle              | 6cc51d2d-56e9-458b-a21b-bc553e0c8291 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:06 |
| fixed_network                       | 3125395f-c689-4481-bf01-94bb2f701993 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:07 |
| agent_wait_handle                   | 2db801e8-c2b5-47b0-ac16-122dba3a22d6 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| remove_docker_key                   | 75e2c7a6-a2ce-4026-aeeb-739c4a522f48 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| secgroup_manager                    | ac51a029-26c1-495a-bc13-232cfb8c1060 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| write_docker_socket                 | 58e08b52-a12a-43e9-b41d-071750294024 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| master_wait_handle                  | 3e741b76-6470-47d4-b13e-3f8f446be53c | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| cfn_signal                          | 96c26b4f-1e99-478e-a8e5-9dcc4486e1b3 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| enable_services                     | beedc358-ee72-4b34-a6b9-1b47ffc15306 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| add_proxy                           | caae3a07-d5f1-4eb0-8a82-02ea634f77ae | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| make_cert                           | 79363643-e5e4-4d1b-ad8a-5a56e1f6a8e7 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:08 |
| cloud_init_wait_handle              | 0457b008-6da8-44fd-abef-cb99bd4d0518 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| configure_swarm                     | baf1e089-c627-4b24-a571-63b3c9c14e28 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| extrouter                           | 184614d9-2280-4cb4-9253-f538463dbdf4 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| write_docker_service                | 80e66b4e-d40a-4243-bb27-0d2a6b68651f | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| disable_selinux                     | d8a64822-2571-4dcf-9da5-b3ec73e771eb | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| fixed_network                       | 528b0ced-23f6-4c22-8cbc-357ba0ee5bc5 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:09 |
| write_swarm_manager_failure_service | 9fa100a3-b4a9-465c-8b33-dd000cb4866a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| write_swarm_agent_failure_service   | a7c09833-929e-4711-a3e9-39923d23b2f2 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| fixed_subnet                        | 23d8b0a6-a7a3-4f71-9c18-ba6255cf071a | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:10 |
| write_swarm_master_service          | d24a6099-3cad-41ce-8d4b-a7ad0661aaea | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:11 |
| fixed_subnet                        | 1a2b7397-1d09-4544-bb9f-985c2f64cb09 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_manager_failure_service | 615a2a7a-5266-487b-bbe1-fcaa82f43243 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_agent_failure_service   | 3f8c54b4-6644-49a0-ad98-9bc6b4332a07 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| write_swarm_master_service          | 2f58b3c8-d1cc-4590-a328-0e775e495bcf | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:13 |
| extrouter_inside                    | f3da7f2f-643e-4f29-a00f-d2595d7faeaf | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:14 |
| swarm_master_eth0                   | 1d6a510d-520c-4796-8990-aa8f7dd59757 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:16 |
| swarm_master_eth0                   | 3fd85913-7399-49be-bb46-5085ff953611 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:19 |
| extrouter_inside                    | 33749e30-cbea-4093-b36a-94967e299002 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:19 |
| write_heat_params                   | 054e0af5-e3e0-4bc0-92b5-b40aeedc39ab | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:19 |
| swarm_nodes                         | df7af58c-8148-4b51-bd65-b0734d9051b5 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:20 |
| write_swarm_agent_service           | ab1e8b1e-2837-4693-b791-e1311f85fa63 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:21 |
| swarm_master_floating               | d99ffe66-cb02-4279-99dc-a1f3e2ca817c | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:22 |
| write_heat_params                   | 33d9999f-6c93-453d-8565-ac99db021f8f | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| write_swarm_agent_service           | 02a1b7f6-2660-4345-ad08-42b66ffaaad5 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| swarm_master_floating               | 8ce6ecd8-c421-4e4a-ab81-cba4b5ccedf4 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:25 |
| swarm_master_init                   | 3787dcc8-e644-412b-859b-63a434b9ee6c | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:26 |
| swarm_master_init                   | a1dd67bb-49c7-4507-8af0-7758b76b57e1 | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:28 |
| swarm_master                        | d12b915e-3087-4e17-9954-8233926b504b | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:29 |
| swarm_master                        | a34ad52a-def7-460b-b5b7-410000207b3e | state changed          | CREATE_COMPLETE    | 2016-03-19T17:23:48 |
| master_wait_condition               | 0c9331a4-8ad0-46e0-bf2a-35943021a1a3 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
| cloud_init_wait_condition           | de3707a0-f46a-44a9-b4b8-ff50e12cc77f | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
| agent_wait_condition                | a1a810a4-9c19-4983-aaa8-e03f308c1e39 | state changed          | CREATE_IN_PROGRESS | 2016-03-19T17:23:49 |
+-------------------------------------+--------------------------------------+------------------------+--------------------+---------------------+
</pre></div>


<p>Once all tasks are completed, we can create containers in the bay we
created in previous steps.</p>
<div class="highlight"><pre><span></span>magnum container-create --name demo-container   
                       --image docker.io/cirros:latest   
                       --bay demoswarmbay   
                       --command &quot;ping -c 4 192.168.100.2&quot;
+------------+----------------------------------------+
| Property   | Value                                  |
+------------+----------------------------------------+
| uuid       | 36595858-8657-d465-3e5a-dfcddad8a238   |
| links      | ...                                    |
| bay_uuid   | a2388916-db30-41bf-84eb-df0b65979eaf   |
| updated_at | None                                   |
| image      | cirros                                 |
| command    | ping -c 4 192.168.100.2                |
| created_at | 2016-03-19T17:30:00+00:00              |
| name       | demo-container                         |
+------------+----------------------------------------+
</pre></div>


<p>Container is created, but not started.<br>
Start the container</p>
<div class="highlight"><pre><span></span>magnum container-start demo-container
</pre></div>


<p>Check container logs, you should see 4 pings succeed to our external
router gateway.</p>
<div class="highlight"><pre><span></span>magnum container-logs demo-container

PING 192.168.100.2 (192.168.100.2) 56(84) bytes of data.
64 bytes from 192.168.100.2: icmp_seq=1 ttl=64 time=0.083 ms
64 bytes from 192.168.100.2: icmp_seq=2 ttl=64 time=0.068 ms
64 bytes from 192.168.100.2: icmp_seq=3 ttl=64 time=0.043 ms
64 bytes from 192.168.100.2: icmp_seq=4 ttl=64 time=0.099 ms
</pre></div>


<p>You can delete the container</p>
<div class="highlight"><pre><span></span>magnum container-delete demo-container
</pre></div>


<p>While doing this demo, i missed adding branch name while cloning Magnum
source code, when i installed Magnum all package dependencies where
installed from master, who was Mitaka instead of Liberty, which broke my
environment.</p>
<p>I suffered the following issues:</p>
<p>Issues with packages</p>
<div class="highlight"><pre><span></span><span class="n">ImportError</span><span class="o">:</span> <span class="n">No</span> <span class="n">module</span> <span class="n">named</span> <span class="n">MySQLdb</span>
</pre></div>


<p>Was solved installing MySQL-python from pip instead of yum</p>
<div class="highlight"><pre><span></span>pip install MySQL-python
</pre></div>


<p>Issues with policies, admin privileges weren't recognized by Magnum api.</p>
<div class="highlight"><pre><span></span><span class="n">PolicyNotAuthorized</span><span class="o">:</span> <span class="n">magnum</span><span class="o">-</span><span class="n">service</span><span class="o">:</span><span class="n">get_all</span><span class="o">{{</span> <span class="n">bunch</span> <span class="n">of</span> <span class="n">stuff</span> <span class="o">}}</span> <span class="n">disallowed</span> <span class="n">by</span> <span class="n">policy</span>
</pre></div>


<p>Was solved removing admin_api rule at Magnum policy.json file</p>
<div class="highlight"><pre><span></span>vi /etc/magnum/policy.json

#    &quot;admin_api&quot;: &quot;rule:context_is_admin&quot;,
</pre></div>


<p>Unfortunately, nova was completely broken and it was not working at all,
so i installed a new environment and added branch while cloning source
code.<br>
Next issue i found was Barbican, who was not installed, i used the
steps mentioned at this post to solve this issue.</p>
<p>Hope this guide helps you integrating Magnum Container as a Service in
OpenStack.</p>
<p>Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code.html">posted at 19:22</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/openstack.html" rel="tag">OpenStack</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/container.html" class="tags">container</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/docker.html" class="tags">docker</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/guide.html" class="tags selected">guide</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/install.html" class="tags">install</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/liberty.html" class="tags">liberty</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/magnum.html" class="tags">magnum</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/manual.html" class="tags">manual</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rdo.html" class="tags">rdo</a>
                </div>
		<a href="https://egonzalez90.github.io/magnum-in-rdo-openstack-liberty-manual-installation-from-source-code.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Feb 02, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/migrate-from-keystone-v2-0-to-keystone-v3-openstack-liberty.html" rel="bookmark" title="Permanent Link to &quot;Migrate from keystone v2.0 to keystone v3 OpenStack Liberty&quot;">Migrate from keystone v2.0 to keystone v3 OpenStack Liberty</a>
                </h2>

                
                

                <p>Migrate from keystone v2.0 to v3 isn't as easy like just changing the
endpoints at the database, every service must be configured to
authenticate against keystone v3.</p>
<p>I've been working on that the past few days looking for a method, with
the purpose of facilitate operators life's who need this kind of
migration.<br>
I have to thank Adam Young work, i followed his blog to make a first
configuration idea, after that, i configured all core services to make
use of keystone v3.<br>
If you want to check Adam's blog, follow this link:
<a href="http://adam.younglogic.com/2015/05/rdo-v3-only/">http://adam.younglogic.com/2015/05/rdo-v3-only/</a></p>
<p>I used OpenStack Liberty installed with RDO packstack over CentOS 7
servers.<br>
The example IP used is <code>192.168.200.168</code>, use your own according your
needs.<br>
Password used for all services is <code>PASSWD1234</code>, use your own password,
you can locate your passwords at the packstack answer file.</p>
<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Horizon</strong></ins></p>
<p>First we configure Horizon with keystone v3 as below:</p>
<div class="highlight"><pre><span></span>vi /etc/openstack-dashboard/local_settings

OPENSTACK_API_VERSIONS = {
    &quot;identity&quot;: 3
}

OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = &#39;Default&#39;
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>keystone</strong></ins></p>
<p>Check your current identity endpoints</p>
<div class="highlight"><pre><span></span>mysql  --user keystone_admin --password=PASSWD1234  keystone -e &quot;select interface, url from endpoint where service_id =  (select id from service where service.type = &#39;identity&#39;);&quot;
</pre></div>


<p>Change your public, admin and internal endpoints with v3 at the end,
instead of v2.0</p>
<div class="highlight"><pre><span></span><span class="nt">mysql</span>  <span class="nt">--user</span> <span class="nt">keystone_admin</span> <span class="nt">--password</span><span class="o">=</span><span class="nt">PASSWD1234</span>   <span class="nt">keystone</span> <span class="nt">-e</span> <span class="s2">&quot;update endpoint set   url  = &#39;http://192.168.200.178:5000/v3&#39; where  interface =&#39;internal&#39; and  service_id =  (select id from service where service.type = &#39;identity&#39;);&quot;</span>

<span class="nt">mysql</span>  <span class="nt">--user</span> <span class="nt">keystone_admin</span> <span class="nt">--password</span><span class="o">=</span><span class="nt">PASSWD1234</span>   <span class="nt">keystone</span> <span class="nt">-e</span> <span class="s2">&quot;update endpoint set   url  = &#39;http://192.168.200.178:5000/v3&#39; where  interface =&#39;public&#39; and  service_id =  (select id from service where service.type = &#39;identity&#39;);&quot;</span>

<span class="nt">mysql</span>  <span class="nt">--user</span> <span class="nt">keystone_admin</span> <span class="nt">--password</span><span class="o">=</span><span class="nt">PASSWD1234</span>   <span class="nt">keystone</span> <span class="nt">-e</span> <span class="s2">&quot;update endpoint set   url  = &#39;http://192.168.200.178:35357/v3&#39; where  interface =&#39;admin&#39; and  service_id =  (select id from service where service.type = &#39;identity&#39;);&quot;</span>
</pre></div>


<p>Ensure the endpoints are properly created</p>
<div class="highlight"><pre><span></span>mysql  --user keystone_admin --password=KEYSTONE_DB_PW   keystone -e &quot;select interface, url from endpoint where service_id =  (select id from service where service.type = &#39;identity&#39;);&quot;
</pre></div>


<p>Create a source file or edit keystonerc_admin with the following data</p>
<div class="highlight"><pre><span></span>vi v3_keystone

unset OS_SERVICE_TOKEN
export OS_USERNAME=admin
export OS_PASSWORD=PASSWD1234
export OS_AUTH_URL=http://192.168.200.178:5000/v3
export OS_PROJECT_NAME=admin
export OS_PROJECT_DOMAIN_NAME=Default
export OS_USER_DOMAIN_NAME=Default
export OS_REGION_NAME=RegionOne
export PS1=&#39;[\u@\h \W(keystone_admin)]\$ &#39;
export OS_IDENTITY_API_VERSION=3
</pre></div>


<p>Comment both pipelines, in public_api and admin_api</p>
<div class="highlight"><pre><span></span>vi /usr/share/keystone/keystone-dist-paste.ini

[pipeline:public_api]
# The last item in this pipeline must be public_service or an equivalent
# application. It cannot be a filter.
#pipeline = sizelimit url_normalize request_id build_auth_context token_auth admin_token_auth json_body ec2_extension user_crud_extension public_service

[pipeline:admin_api]
# The last item in this pipeline must be admin_service or an equivalent
# application. It cannot be a filter.
#pipeline = sizelimit url_normalize request_id build_auth_context token_auth admin_token_auth json_body ec2_extension s3_extension crud_extension admin_service
</pre></div>


<p>Comment v2.0 entries in composite:main and admin sections.</p>
<div class="highlight"><pre><span></span><span class="k">[composite:main]</span>
<span class="na">use</span> <span class="o">=</span> <span class="s">egg:Paste#urlmap</span>
<span class="c1">#/v2.0 = public_api</span>
<span class="na">/v3</span> <span class="o">=</span> <span class="s">api_v3</span>
<span class="na">/</span> <span class="o">=</span> <span class="s">public_version_api</span>

<span class="k">[composite:admin]</span>
<span class="na">use</span> <span class="o">=</span> <span class="s">egg:Paste#urlmap</span>
<span class="c1">#/v2.0 = admin_api</span>
<span class="na">/v3</span> <span class="o">=</span> <span class="s">api_v3</span>
<span class="na">/</span> <span class="o">=</span> <span class="s">admin_version_api</span>
</pre></div>


<p>Restart httpd to apply changes</p>
<div class="highlight"><pre><span></span>systemctl restart httpd
</pre></div>


<p>Check whether keystone and horizon are properly working<br>
The command below should prompt an user list, if not, check
configuration in previous steps</p>
<div class="highlight"><pre><span></span>openstack user list
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Glance</strong></ins></p>
<p>Edit the following files, with the content below:</p>
<div class="highlight"><pre><span></span>vi /etc/glance/glance-api.conf 
vi /etc/glance/glance-registry.conf 
vi /etc/glance/glance-cache.conf

[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = glance
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre></div>


<p>Comment the following lines:</p>
<div class="highlight"><pre><span></span>#auth_host=127.0.0.1
#auth_port=35357
#auth_protocol=http
#identity_uri=http://192.168.200.178:35357
#admin_user=glance
#admin_password=PASSWD1234
#admin_tenant_name=services
</pre></div>


<p>Those lines, should be commented in all the other OpenStack core
services at keystone_authtoken section</p>
<p>Edit the files below and comment the lines inside keystone_authtoken
section.</p>
<div class="highlight"><pre><span></span>vi /usr/share/glance/glance-api-dist.conf 
vi /usr/share/glance/glance-registry-dist.conf

[keystone_authtoken]
#admin_tenant_name = %SERVICE_TENANT_NAME%
#admin_user = %SERVICE_USER%
#admin_password = %SERVICE_PASSWORD%
#auth_host = 127.0.0.1
#auth_port = 35357
#auth_protocol = http
</pre></div>


<p>Restart glance services</p>
<div class="highlight"><pre><span></span>openstack-service restart glance
</pre></div>


<p>Ensure glance service is working</p>
<div class="highlight"><pre><span></span>openstack image list
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Nova</strong></ins></p>
<p>Edit the file below and comment the lines inside keystone_authtoken</p>
<div class="highlight"><pre><span></span>vi /usr/share/nova/nova-dist.conf

[keystone_authtoken]
#auth_host = 127.0.0.1
#auth_port = 35357
#auth_protocol = http
</pre></div>


<p>Edit nova.conf and add the auth content inside keystone_authtoken,
don't forget to comment the lines related to the last auth method, which
were commented in glance section.</p>
<div class="highlight"><pre><span></span>vi /etc/nova/nova.conf

[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = nova
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre></div>


<p>Configure nova authentication against neutron</p>
<div class="highlight"><pre><span></span><span class="k">[neutron]</span>

<span class="na">auth_plugin</span> <span class="o">=</span> <span class="s">password</span>
<span class="na">auth_url</span> <span class="o">=</span> <span class="s">http://192.168.200.178:35357</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">neutron</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">PASSWD1234</span>
<span class="na">project_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">user_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">project_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">auth_uri</span><span class="o">=</span><span class="s">http://192.168.200.178:5000</span>
</pre></div>


<p>Restart nova services to apply changes</p>
<div class="highlight"><pre><span></span>openstack-service restart nova
</pre></div>


<p>Check if nova works</p>
<div class="highlight"><pre><span></span>openstack hypervisor list
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Neutron</strong></ins></p>
<p>Comment or remove the following entries at api-paste.ini and add the new
version auth lines</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/api-paste.ini

[filter:authtoken]
#identity_uri=http://192.168.200.178:35357
#admin_user=neutron
#admin_password=PASSWD1234
#auth_uri=http://192.168.200.178:5000/v2.0
#admin_tenant_name=services

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = neutron
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre></div>


<p>Configure v3 authentication for metadata service, remember comment the
old auth lines</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/metadata_agent.ini

[DEFAULT]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = neutron
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
</pre></div>


<p>Configure neutron server with v3 auth</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/neutron.conf

nova_admin_auth_url = http://192.168.200.178:5000
# nova_admin_tenant_id =1fb93c84c6474c5ea92c0ed5f7d4a6a7
nova_admin_tenant_name = services


[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = neutron
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000

#auth_uri = http://192.168.200.178:5000/v2.0
#identity_uri = http://192.168.200.178:35357
#admin_tenant_name = services
#admin_user = neutron
#admin_password = PASSWD1234
</pre></div>


<p>Configure neutron auth against nova services</p>
<div class="highlight"><pre><span></span><span class="k">[nova]</span>

<span class="na">auth_plugin</span> <span class="o">=</span> <span class="s">password</span>
<span class="na">auth_url</span> <span class="o">=</span> <span class="s">http://192.168.200.178:35357</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">nova</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">PASSWD1234</span>
<span class="na">project_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">user_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">project_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">auth_uri</span><span class="o">=</span><span class="s">http://192.168.200.178:5000</span>
</pre></div>


<p>Restart neutron services to apply changes</p>
<div class="highlight"><pre><span></span>openstack-service restart neutron
</pre></div>


<p>Test correct neutron funtionality</p>
<div class="highlight"><pre><span></span>openstack network list
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Cinder</strong></ins></p>
<p>Edit api-paste.ini with the following content</p>
<div class="highlight"><pre><span></span>vi /etc/cinder/api-paste.ini

[filter:authtoken]
paste.filter_factory = keystonemiddleware.auth_token:filter_factory
auth_plugin = password
auth_url = http://192.168.200.178:35357
username = cinder
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000
#admin_tenant_name=services
#auth_uri=http://192.168.200.178:5000/v2.0
#admin_user=cinder
#identity_uri=http://192.168.200.178:35357
#admin_password=PASSWD1234
</pre></div>


<p>Restart cinder services to apply changes</p>
<div class="highlight"><pre><span></span>openstack-service restart cinder
</pre></div>


<p>Ensure cinder is properly running</p>
<div class="highlight"><pre><span></span>openstack volume create --size 1 testvolume
openstack volume list
</pre></div>


<p>Now, you can check if nova is working fine, create an instance and
ensure it is in ACTIVE state.</p>
<div class="highlight"><pre><span></span>openstack server create --flavor m1.tiny --image cirros --nic net-id=a1aa6336-9ae2-4ffb-99f5-1b6d1130989c testinstance
openstack server list
</pre></div>


<p>If any error occurs, review configuration files</p>
<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Swift</strong></ins></p>
<p>Configure proxy server auth agains keystone v3</p>
<div class="highlight"><pre><span></span>vi /etc/swift/proxy-server.conf

[filter:authtoken]
log_name = swift
signing_dir = /var/cache/swift
paste.filter_factory = keystonemiddleware.auth_token:filter_factory
auth_plugin = password
auth_url = http://192.168.200.178:35357
username = swift
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000

#auth_uri = http://192.168.200.178:5000/v2.0
#identity_uri = http://192.168.200.178:35357
#admin_tenant_name = services
#admin_user = swift
#admin_password = PASSWD1234
delay_auth_decision = 1
cache = swift.cache
include_service_catalog = False
</pre></div>


<p>Restart swift services to apply changes</p>
<div class="highlight"><pre><span></span>openstack-service restart swift
</pre></div>


<p>Swift commands must be issued with python-openstackclient instead of
swiftclient<br>
If done with swiftclient a -V 3 option must be used in order to avoid
issues</p>
<p>Check if swift works fine</p>
<div class="highlight"><pre><span></span>openstack container create testcontainer
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Ceilometer</strong></ins></p>
<p>Configure ceilometer service in order to authenticate agains keystone v3</p>
<div class="highlight"><pre><span></span><span class="k">[keystone_authtoken]</span>

<span class="na">auth_plugin</span> <span class="o">=</span> <span class="s">password</span>
<span class="na">auth_url</span> <span class="o">=</span> <span class="s">http://192.168.200.178:35357</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">ceilometer</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">PASSWD1234</span>
<span class="na">project_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">user_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">project_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
<span class="na">auth_uri</span><span class="o">=</span><span class="s">http://192.168.200.178:5000</span>

<span class="k">[service_credentials]</span>

<span class="na">os_auth_url</span> <span class="o">=</span> <span class="s">http://controller:5000/v3</span>
<span class="na">os_username</span> <span class="o">=</span> <span class="s">ceilometer</span>
<span class="na">os_tenant_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">os_password</span> <span class="o">=</span> <span class="s">PASSWD1234</span>
<span class="na">os_endpoint_type</span> <span class="o">=</span> <span class="s">internalURL</span>
<span class="na">os_region_name</span> <span class="o">=</span> <span class="s">RegionOne</span>
</pre></div>


<p>Restart ceilometer services</p>
<div class="highlight"><pre><span></span>openstack-service restart ceilometer
</pre></div>


<p>Check ceilometer funtionality</p>
<div class="highlight"><pre><span></span>ceilometer statistics -m memory
</pre></div>


<p><ins datetime="2016-02-02T18:25:21+00:00"><strong>Heat</strong></ins></p>
<p>Configure Heat authentication, since trusts are not stable use password
auth method</p>
<div class="highlight"><pre><span></span>vi /etc/heat/heat.conf

# Allowed values: password, trusts
#deferred_auth_method = trusts
deferred_auth_method = password
</pre></div>


<p>Configure auth_uri and keystone_authtoken section</p>
<div class="highlight"><pre><span></span># From heat.common.config
#
# Unversioned keystone url in format like http://0.0.0.0:5000. (string value)
#auth_uri =
auth_uri = http://192.168.200.178:5000

[keystone_authtoken]

auth_plugin = password
auth_url = http://192.168.200.178:35357
username = heat
password = PASSWD1234
project_name = services
user_domain_name = Default
project_domain_name = Default
auth_uri=http://192.168.200.178:5000

#admin_user=heat
#admin_password=PASSWD1234
#admin_tenant_name=services
#identity_uri=http://192.168.200.178:35357
#auth_uri=http://192.168.200.178:5000/v2.0
</pre></div>


<p>Comment or remove heat-dist auth entries in order to avoid conflicts
with your config files</p>
<div class="highlight"><pre><span></span>vi /usr/share/heat/heat-dist.conf

[keystone_authtoken]
#auth_host = 127.0.0.1
#auth_port = 35357
#auth_protocol = http
#auth_uri = http://127.0.0.1:5000/v2.0
#signing_dir = /tmp/keystone-signing-heat
</pre></div>


<p>Restart heat services to apply changes</p>
<div class="highlight"><pre><span></span>openstack-service restart heat
</pre></div>


<p>Ensure heat authentication is properly configured with a simple heat
template</p>
<div class="highlight"><pre><span></span>heat stack-create --template-file sample.yaml teststack
</pre></div>


<p>Most issues occurs in the authentication between nova and neutron
services, if instances does not launch as expected, review [nova] and
[neutron] sections.</p>
<p>Best regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/migrate-from-keystone-v2-0-to-keystone-v3-openstack-liberty.html">posted at 19:43</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/openstack.html" rel="tag">OpenStack</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/auth.html" class="tags">auth</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/auth_url.html" class="tags">auth_url</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/core.html" class="tags">core</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/guide.html" class="tags selected">guide</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/how-to.html" class="tags">how to</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/keystone.html" class="tags">keystone</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/migrate.html" class="tags">migrate</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/packstack.html" class="tags">packstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rdo.html" class="tags">rdo</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/services.html" class="tags">services</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/tutorial.html" class="tags">Tutorial</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/v20.html" class="tags">v2.0</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/v3.html" class="tags">v3</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/version.html" class="tags">version</a>
                </div>
		<a href="https://egonzalez90.github.io/migrate-from-keystone-v2-0-to-keystone-v3-openstack-liberty.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Jan 14, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/openstack-segregation-with-availability-zones-and-host-aggregates.html" rel="bookmark" title="Permanent Link to &quot;OpenStack segregation with Availability Zones and Host Aggregates&quot;">OpenStack segregation with Availability Zones and Host Aggregates</a>
                </h2>

                
                

                <p>When a new OpenStack cloud born, usually all servers run over the same
hardware and specifications, often, all servers are in the same
building, room, rack, even a chassis when the cloud is in the first
growth paces.</p>
<p>After a while, workloads increase and the current hardware is not enough
to process that workloads. At this point, your hardware is old and new
hardware is bought. This hardware has different storage disks, CPU, RAM
and so on. You passed from 10's of servers to 100's. DataCenter racks,
rooms and buildings are too small and the growing cloud needs redundancy
between cities or countries.</p>
<p>OpenStack offers a few solutions for that purpose, called Regions,
Cells, Availability Zones and Host Aggregates.<br>
Now, we are going to focus on Availability Zones and Host Aggregates,
which are the way to segregate computational workloads.</p>
<ul>
<li>Host Aggregates:<ul>
<li>Host Aggregates represent a logical set of
    properties/characteristics a group of hosts owns in the form of
    metadata. Imagine some of your servers have SSD disks and the
    other ones SATA, you can map those properties SSD/SATA to a
    group of hosts, when a image or flavor with the metaparameter
    associated is launched, Nova Scheduler will filter the available
    hosts with the meta parameter value and boot the instance on
    hosts with the desired property. Host Aggregates are managed by
    OpenStack admins.</li>
</ul>
</li>
<li>Availability Zones<ul>
<li>Availability Zones represent a logical partition of the
    infrastructure(not necessary but is the common use case) in the
    form of racks, rooms, buildings, etc. Customers can launch
    instances in the desired Availability Zone.</li>
</ul>
</li>
</ul>
<p>Usually, Host Aggregates are mapped to Availability Zones allowing
customers to use the desired set of hardware or characteristics to boot
instances.</p>
<p>At the end of this guide you will know how to:</p>
<ol>
<li>Create Availability Zones and Host Aggregates.</li>
<li>Adding hosts to Host Aggregates and Availability Zones.</li>
<li>Launch instances directly to Availability Zones.</li>
<li>Configure nova scheduler for Host Aggregates usage.</li>
<li>Configure Images and Flavors for scheduling to Host Aggregates.</li>
<li>Launch instances based on flavors and image parameters.</li>
</ol>
<p>Let's start: \00/</p>
<p>Create two Host Aggregate called <code>"az1-ag"/"az2-ag"</code>, this command also,
will create two Availability Zones called <code>"az1"/"az2"</code>.<br>
By default, when a Host Aggregate is created with an Availability Zone,
a metadata key called <code>"availability_zone=NAME_OF_AZ</code>" will be created.</p>
<div class="highlight"><pre><span></span># nova aggregate-create az1-ag az1
+----+--------+-------------------+-------+-------------------------+
| Id | Name   | Availability Zone | Hosts | Metadata                |
+----+--------+-------------------+-------+-------------------------+
| 2  | az1-ag | az1               |       | &#39;availability_zone=az1&#39; |
+----+--------+-------------------+-------+-------------------------+
# nova aggregate-create az2-ag az2
+----+--------+-------------------+-------+-------------------------+
| Id | Name   | Availability Zone | Hosts | Metadata                |
+----+--------+-------------------+-------+-------------------------+
| 3  | az2-ag | az2               |       | &#39;availability_zone=az2&#39; |
+----+--------+-------------------+-------+-------------------------+
</pre></div>


<p>Add one or more compute nodes to Host Aggregates.</p>
<div class="highlight"><pre><span></span># nova aggregate-add-host 2 compute1az
Host compute1az has been successfully added for aggregate 2 
+----+--------+-------------------+--------------+-------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                |
+----+--------+-------------------+--------------+-------------------------+
| 2  | az1-ag | az1               | &#39;compute1az&#39; | &#39;availability_zone=az1&#39; |
+----+--------+-------------------+--------------+-------------------------+
# nova aggregate-add-host 3 compute2az
Host compute2az has been successfully added for aggregate 3 
+----+--------+-------------------+--------------+-------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                |
+----+--------+-------------------+--------------+-------------------------+
| 3  | az2-ag | az2               | &#39;compute2az&#39; | &#39;availability_zone=az2&#39; |
+----+--------+-------------------+--------------+-------------------------+
</pre></div>


<p>Details about a Host Aggregate can be reviewed with:</p>
<div class="highlight"><pre><span></span># nova aggregate-details az1-ag
+----+--------+-------------------+--------------+-------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                |
+----+--------+-------------------+--------------+-------------------------+
| 2  | az1-ag | az1               | &#39;compute1az&#39; | &#39;availability_zone=az1&#39; |
+----+--------+-------------------+--------------+-------------------------+
# nova aggregate-details az2-ag
+----+--------+-------------------+--------------+-------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                |
+----+--------+-------------------+--------------+-------------------------+
| 3  | az2-ag | az2               | &#39;compute2az&#39; | &#39;availability_zone=az2&#39; |
+----+--------+-------------------+--------------+-------------------------+
</pre></div>


<p>List Availability Zones and check status.</p>
<div class="highlight"><pre><span></span><span class="c1"># nova availability-zone-list</span>
<span class="s s-Atom">+-----------------------+----------------------------------------+</span>
<span class="p">|</span> <span class="nv">Name</span>                  <span class="p">|</span> <span class="nv">Status</span>                                 <span class="p">|</span>
<span class="s s-Atom">+-----------------------+----------------------------------------+</span>
<span class="p">|</span> <span class="s s-Atom">internal</span>              <span class="p">|</span> <span class="s s-Atom">available</span>                              <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">controlleraz</span>       <span class="p">|</span>                                        <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">conductor</span>   <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">16.000000</span> <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">consoleauth</span> <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">16.000000</span> <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">scheduler</span>   <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">16.000000</span> <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">cert</span>        <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">13.000000</span> <span class="p">|</span>
<span class="p">|</span> <span class="s s-Atom">az2</span>                   <span class="p">|</span> <span class="s s-Atom">available</span>                              <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">compute2az</span>         <span class="p">|</span>                                        <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">compute</span>     <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">12.000000</span> <span class="p">|</span>
<span class="p">|</span> <span class="s s-Atom">az1</span>                   <span class="p">|</span> <span class="s s-Atom">available</span>                              <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">compute1az</span>         <span class="p">|</span>                                        <span class="p">|</span>
<span class="p">|</span> <span class="p">|</span> <span class="p">|</span><span class="o">-</span> <span class="s s-Atom">nova</span><span class="o">-</span><span class="s s-Atom">compute</span>     <span class="p">|</span> <span class="nf">enabled</span> <span class="o">:-</span><span class="p">)</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">14</span><span class="nv">T19</span><span class="s s-Atom">:</span><span class="mi">08</span><span class="s s-Atom">:</span><span class="mf">12.000000</span> <span class="p">|</span>
<span class="s s-Atom">+-----------------------+----------------------------------------+</span>
</pre></div>


<p>Other method you can use:</p>
<div class="highlight"><pre><span></span># nova service-list
+----+------------------+--------------+----------+---------+-------+----------------------------+-----------------+
| Id | Binary           | Host         | Zone     | Status  | State | Updated_at                 | Disabled Reason |
+----+------------------+--------------+----------+---------+-------+----------------------------+-----------------+
| 1  | nova-consoleauth | controlleraz | internal | enabled | up    | 2016-01-14T19:54:36.000000 | -               |
| 2  | nova-scheduler   | controlleraz | internal | enabled | up    | 2016-01-14T19:54:36.000000 | -               |
| 3  | nova-conductor   | controlleraz | internal | enabled | up    | 2016-01-14T19:54:35.000000 | -               |
| 4  | nova-cert        | controlleraz | internal | enabled | up    | 2016-01-14T19:54:33.000000 | -               |
| 5  | nova-compute     | compute2az   | az2      | enabled | up    | 2016-01-14T19:54:32.000000 | -               |
| 6  | nova-compute     | compute1az   | az1      | enabled | up    | 2016-01-14T19:54:32.000000 | -               |
+----+------------------+--------------+----------+---------+-------+----------------------------+-----------------+
</pre></div>


<p>Launch two instances using <code>"--availability-zone AZ</code>" option, you can
even select the compute node to use, just use
<code>"--availability-zone AZ:COMPUTE_NODE</code>".</p>
<div class="highlight"><pre><span></span># nova boot --flavor m1.tiny --image cirros --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 --availability-zone az1 instanceaz1
# nova boot --flavor m1.tiny --image cirros --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 --availability-zone az2 instanceaz2
</pre></div>


<p>Ensure the instances are running in the desired Availability Zone.</p>
<div class="highlight"><pre><span></span># nova show instanceaz1 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az1
# nova show instanceaz2 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az2
</pre></div>


<p>List Glance images.</p>
<div class="highlight"><pre><span></span># glance image-list
+--------------------------------------+----------+
| ID                                   | Name     |
+--------------------------------------+----------+
| a6d7a606-f725-480a-9b1b-7b3ae39b93d4 | cirros   |
| a6540d72-dff7-4fb1-bc64-a8ea69e65178 | imageaz1 |
| 9c7e2d55-0b96-43e2-9231-88e426edb350 | imageaz2 |
+--------------------------------------+----------+
</pre></div>


<p>Update the images with custom properties, i use <code>"availability_zone"</code>
because is the default meta parameter a Host Aggregate owns when is
inside Availability Zones.</p>
<div class="highlight"><pre><span></span># glance image-update --property availability_zone=az1 a6540d72-dff7-4fb1-bc64-a8ea69e65178
+-------------------+--------------------------------------+
| Property          | Value                                |
+-------------------+--------------------------------------+
| availability_zone | az1                                  |
| checksum          | 133eae9fb1c98f45894a4e60d8736619     |
| container_format  | bare                                 |
| created_at        | 2016-01-14T19:59:04Z                 |
| disk_format       | qcow2                                |
| id                | a6540d72-dff7-4fb1-bc64-a8ea69e65178 |
| min_disk          | 0                                    |
| min_ram           | 0                                    |
| name              | imageaz1                             |
| owner             | 0571c6769c3f46acb195eeb01b87ae38     |
| protected         | False                                |
| size              | 13200896                             |
| status            | active                               |
| tags              | []                                   |
| updated_at        | 2016-01-14T20:13:05Z                 |
| virtual_size      | None                                 |
| visibility        | private                              |
+-------------------+--------------------------------------+
# glance image-update --property availability_zone=az2 9c7e2d55-0b96-43e2-9231-88e426edb350
+-------------------+--------------------------------------+
| Property          | Value                                |
+-------------------+--------------------------------------+
| availability_zone | az2                                  |
| checksum          | 133eae9fb1c98f45894a4e60d8736619     |
| container_format  | bare                                 |
| created_at        | 2016-01-14T19:59:10Z                 |
| disk_format       | qcow2                                |
| id                | 9c7e2d55-0b96-43e2-9231-88e426edb350 |
| min_disk          | 0                                    |
| min_ram           | 0                                    |
| name              | imageaz2                             |
| owner             | 0571c6769c3f46acb195eeb01b87ae38     |
| protected         | False                                |
| size              | 13200896                             |
| status            | active                               |
| tags              | []                                   |
| updated_at        | 2016-01-14T20:13:27Z                 |
| virtual_size      | None                                 |
| visibility        | private                              |
+-------------------+--------------------------------------+
</pre></div>


<p>Boot two instances, now we use images with custom properties, those
properties will map to Availability Zones(you can use other type of
parameters mapping to Host Aggregates characteristics).</p>
<div class="highlight"><pre><span></span># nova boot --flavor m1.tiny --image imageaz1 --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 instanceimageaz1
# nova boot --flavor m1.tiny --image imageaz2 --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 instanceimageaz2
</pre></div>


<p>Ensure the instances booted in the desired Availability Zone.</p>
<div class="highlight"><pre><span></span># nova show instanceimageaz1 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az1
# nova show instanceimageaz2 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az2
</pre></div>


<p>Other method to launch instances is with parameters in flavors.<br>
Create two flavors.</p>
<div class="highlight"><pre><span></span># nova flavor-create --is-public true flavoraz1 6 512 1 1
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| ID | Name      | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| 7  | flavoraz1 | 512       | 1    | 0         |      | 1     | 1.0         | True      |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
# nova flavor-create --is-public true flavoraz2 7 512 1 1
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| ID | Name      | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| 8  | flavoraz2 | 512       | 1    | 0         |      | 1     | 1.0         | True      |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
</pre></div>


<p>Add metadata to a Host Aggregate with some characteristic property as
can be fast HD or cheap HW.</p>
<div class="highlight"><pre><span></span># nova aggregate-set-metadata az1-ag fast=true
Metadata has been successfully updated for aggregate 2.
+----+--------+-------------------+--------------+--------------------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                             |
+----+--------+-------------------+--------------+--------------------------------------+
| 2  | az1-ag | az1               | &#39;compute1az&#39; | &#39;availability_zone=az1&#39;, &#39;fast=true&#39; |
+----+--------+-------------------+--------------+--------------------------------------+
# nova aggregate-set-metadata az2-ag cheap=true
Metadata has been successfully updated for aggregate 3.
+----+--------+-------------------+--------------+---------------------------------------+
| Id | Name   | Availability Zone | Hosts        | Metadata                              |
+----+--------+-------------------+--------------+---------------------------------------+
| 3  | az2-ag | az2               | &#39;compute2az&#39; | &#39;availability_zone=az2&#39;, &#39;cheap=true&#39; |
+----+--------+-------------------+--------------+---------------------------------------+
</pre></div>


<p>Update the previous created flavors with the associated metadata key
with the Host Aggregate.</p>
<div class="highlight"><pre><span></span># nova flavor-key flavoraz1 set  aggregate_instance_extra_specs:fast=true
# nova flavor-key flavoraz2 set  aggregate_instance_extra_specs:cheap=true
</pre></div>


<p>Ensure, the properties are properly created.</p>
<div class="highlight"><pre><span></span># nova flavor-show 7 | grep fast | awk &#39;{print$4$5}&#39;
{&quot;aggregate_instance_extra_specs:fast&quot;:&quot;true&quot;}
# nova flavor-show 8 | grep cheap | awk &#39;{print$4$5}&#39;
{&quot;aggregate_instance_extra_specs:cheap&quot;:&quot;true&quot;}
</pre></div>


<p>By default, Nova Scheduler don't allow filtering by extra Specs inserted
in flavors or images.<br>
First, ensure the following scheduler filters are allowed in Control
nodes.</p>
<div class="highlight"><pre><span></span># egrep ^scheduler_default_filters /etc/nova/nova.conf 
scheduler_default_filters=AggregateInstanceExtraSpecsFilter,RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter
</pre></div>


<p>If a change has been done in nova.conf file, restart nova services</p>
<div class="highlight"><pre><span></span># openstack-service restart nova
</pre></div>


<p>Boot another two instaces, now using custom flavors</p>
<div class="highlight"><pre><span></span># nova boot --flavor flavoraz1 --image cirros --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 instanceflavoraz1
# nova boot --flavor flavoraz2 --image cirros --nic net-id=6d62149e-74d3-4e52-9813-53ad207309f4 instanceflavoraz2
</pre></div>


<p>Check where the instances are running.</p>
<div class="highlight"><pre><span></span># nova show instanceflavoraz1 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az1
# nova show instanceflavoraz2 | grep OS-EXT-AZ | awk &#39;{print$2&quot;:&quot;$4}&#39;
OS-EXT-AZ:availability_zone:az2
</pre></div>


<p>That's all for now<br>
Hope this guide helps.<br>
Regards</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/openstack-segregation-with-availability-zones-and-host-aggregates.html">posted at 21:34</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/openstack.html" rel="tag">OpenStack</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/availability-zones.html" class="tags">availability zones</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/flavor.html" class="tags">flavor</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/guide.html" class="tags selected">guide</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/host-aggregates.html" class="tags">host aggregates</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/image.html" class="tags">image</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/meta.html" class="tags">meta</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/nova.html" class="tags">nova</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/scheduler.html" class="tags">scheduler</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/segregation.html" class="tags">segregation</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/usage.html" class="tags">usage</a>
                </div>
		<a href="https://egonzalez90.github.io/openstack-segregation-with-availability-zones-and-host-aggregates.html#disqus_thread">Click to read and post comments</a>
            </article>

                <div class="clear"></div>
                <div class="pages">

                    <a href="https://egonzalez90.github.io/page/2" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 1 of 2</span>
                </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="https://egonzalez90.github.io/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>