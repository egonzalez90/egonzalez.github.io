<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>OpenStack Stuff | Articles by egongu90</title>
    <link rel="shortcut icon" type="image/png" href="https://egonzalez90.github.io/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="https://egonzalez90.github.io/favicon.ico">
    <link href="https://egonzalez90.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="OpenStack Stuff Full Atom Feed" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="https://egonzalez90.github.io/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Eduardo Gonzalez" />
</head>
<body>
    <header>
        <nav>
            <ul>

                <li class="ephemeral selected"><a href="https://egonzalez90.github.io/author/egongu902.html">egongu90</a></li>
                <li><a href="https://egonzalez90.github.io/">Home</a></li>
                <li><a href="https://docs.openstack.org">OpenStack</a></li>
                <li><a href="https://www.linkedin.com/in/eduardogonzalezgutierrez">Linkedin</a></li>
                <li><a href="https://github.com/egonzalez90">Github</a></li>
                <li><a href="https://egonzalez90.github.io/archives">Archives</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="https://egonzalez90.github.io/">OpenStack Stuff</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Oct 31, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/to-conditional-or-to-skip-that-is-the-ansible-question.html" rel="bookmark" title="Permanent Link to &quot;To Conditional or To Skip, that is the Ansible question&quot;">To Conditional or To Skip, that is the Ansible question</a>
                </h2>

                
                

                <p>Have you ever think about if an Ansible task should be skipped with a
conditional or without (hidden skip)?.<br>
Well, this post will analyse both methods.</p>
<p>Let's use a example to create the same result and analyse the both
methods:</p>
<p>In OpenStack Kolla we found that sometimes operators need to customise
policy.json files. That's fine, but the problem is that policy.json
files are installed with the services by default and we don't need/want
to maintain policy.json files in our repository because for sure will
cause bugs from outdated policy files in the future.</p>
<p>What's the proposed solution to this? Allow operators use their own
policy files only when their files exists in a custom configuration
folder. If custom files are not present, default policy.json files are
already present as part of the software installation. ( Actually this
change is under review )</p>
<h2>To Conditional method</h2>
<p>Code snippet:</p>
<div class="highlight"><pre><span></span><span class="x">  - name: Check if file exists</span>
<span class="x">    stat:</span>
<span class="x">      path: &quot;/tmp/custom_file.json&quot;</span>
<span class="x">    register: check_custom_file_exist</span>

<span class="x">  - name: Copy custom policy when exist</span>
<span class="x">    template:</span>
<span class="x">      src: &quot;/tmp/custom_file.json&quot;</span>
<span class="x">      dest: &quot;/tmp/destination_file.json&quot;</span>
<span class="x">    when: &quot;</span><span class="cp">{{</span> <span class="nv">check_custom_file_exist.stat.exists</span> <span class="cp">}}</span><span class="x">&quot;</span>
</pre></div>


<p>The first task checks if the file is present and register the stat
result.<br>
The second task, copy the file only when the registered result of the
previous task is True. (exists == True)</p>
<p>Outputs the following when the file is not present:</p>
<div class="highlight"><pre><span></span>PLAY [localhost] ***************************************************************

TASK [Check if file exists] ****************************************************
ok: [localhost]

TASK [Copy custom policy when exist] *******************************************
skipping: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=1    changed=0    unreachable=0    failed=0
</pre></div>


<p>We can see the copy file task is skipped with a skipping message.</p>
<h2>To Skip method</h2>
<p>Code snippet:</p>
<div class="highlight"><pre><span></span><span class="x">  - name: Copy custom policy when exist</span>
<span class="x">    template:</span>
<span class="x">      src: &quot;</span><span class="cp">{{</span> <span class="nv">item</span> <span class="cp">}}</span><span class="x">&quot;</span>
<span class="x">      dest: &quot;/tmp/destination_file.json&quot;</span>
<span class="x">    with_first_found:</span>
<span class="x">    - files:</span>
<span class="x">      - custom_file.json</span>
<span class="x">      skip: True</span>
</pre></div>


<p>This playbook contains a single task, this task will use the first found
file in a list of files. If no file is present will skip the task.</p>
<p>Output from this execution:</p>
<div class="highlight"><pre><span></span>PLAY [localhost] ***************************************************************

TASK [Copy custom policy when exist] *******************************************

PLAY RECAP *********************************************************************
localhost                  : ok=0    changed=0    unreachable=0    failed=0
</pre></div>


<p>We can see that no task is executed when custom files are not present,
no output from the task (hidden skip).</p>
<h2>Analysis and own opinion</h2>
<p>Both methods do the same, both copy custom files when are present and
both skip copy task when are not present.<br>
What are the differences between both methods?</p>
<p>To_skip method is simpler to read and unified in a single task,
to_conditional is created within two tasks.<br>
To_conditional method takes longer to be executed as it has to check
the existence of a file and then evaluate a conditional.</p>
<p>You may think that to_skip method is better than to_conditional
method, that's right in terms of code syntax and execution times.
But...<br>
As both, operator and infrastructure developer, I always use
to_conditional method because when I'm deploying something new, I want
to know what is executed and what not. In to_skip method you don't know
because there is no output provided from the task(not really true) but
in to_conditional method it clearly says Skipping.</p>
<p>Execution times are not a problem in most use cases, as is not commonly
used this kind of tasks in CM systems, only a few tasks will need this
type of logic.</p>
<p>Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/to-conditional-or-to-skip-that-is-the-ansible-question.html">posted at 20:49</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/various.html" rel="tag">Various</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/analysis.html" class="tags">analysis</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/ansible.html" class="tags">ansible</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/code.html" class="tags">code</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/conditional.html" class="tags">conditional</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/logic.html" class="tags">logic</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/opinion.html" class="tags">opinion</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/skip.html" class="tags">skip</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/syntax.html" class="tags">syntax</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/true.html" class="tags">True</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/when.html" class="tags">when</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/with_first_found.html" class="tags">with_first_found</a>
                </div>
		<a href="https://egonzalez90.github.io/to-conditional-or-to-skip-that-is-the-ansible-question.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Jul 12, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/spacewalk-red-hat-satellite-v5-in-a-docker-container-poc.html" rel="bookmark" title="Permanent Link to &quot;Spacewalk (Red Hat Satellite v5) in a Docker container PoC&quot;">Spacewalk (Red Hat Satellite v5) in a Docker container PoC</a>
                </h2>

                
                

                <p>Spacewalk was the upstream project to provide a Linux systems management
layer on which Red Hat Satellite was based, was based at least until RH
Satellite version 5. Newer versions are not anymore based on Spacewalk,
instead Satellite is a federation of several upstream open source
projects, including Katello, Foreman, Pulp, and Candlepin.</p>
<p>Some weeks ago, a friend asked me if I knew a Docker container image for
Satellite.<br>
I have not found any image. What I found was some Spacewalk images, but
sadly none of them worked for me.<br>
I decided to create an image for this purpose.</p>
<p>While developing the image, I found serious troubles to make it run with
systemd (I'm a fan of systemd, but not inside containers yet).<br>
The result was a semi functional working image. I said semi functional
because some Spacewalk features are not working (probably an issue with
systemd again).<br>
The main problem was that spacewalk-setup script starts and uses
systemd to configure the database and the other needed services, that's
OK in a VM but not in a container.<br>
So i needed to hack into postgres setup and start the services with the
typical <code>command --config-file file.conf</code> executed from supervisord as
Docker entrypoint.<br>
Currently there is an issue with <code>osa-dispatcher</code>, on which I can't
find a fix to make it run.</p>
<p>This image is primarily created just for test Spacewalk interface and be
more comfortable with it aka testing/development purposes, or just to
have fun hacking with Docker containers.</p>
<p>Now, I'm going to make a short description of what the Dockerfile makes
and then start the container.<br>
Have fun.</p>
<p>I used centos as image base for this PoC</p>
<div class="highlight"><pre><span></span>FROM centos:7
</pre></div>


<p>Typical Maintainer line</p>
<div class="highlight"><pre><span></span>MAINTAINER Eduardo Gonzalez Gutierrez
</pre></div>


<p>Add jpackage repo which provides Java packages for Linux</p>
<div class="highlight"><pre><span></span>COPY jpackage-generic.repo /etc/yum.repos.d/jpackage-generic.repo
</pre></div>


<p>Install EPEL and Spacewalk repositories, after install, clean all stored
cache to minimize image size</p>
<div class="highlight"><pre><span></span>RUN yum install -y http://yum.spacewalkproject.org/2.5/RHEL/7/x86_64/spacewalk-repo-2.5-3.el7.noarch.rpm   
       epel-release &amp;&amp;   
       yum clean all
</pre></div>


<p>Import Keys to allow installation from these repositories</p>
<div class="highlight"><pre><span></span><span class="n">RUN</span> <span class="n">rpm</span> <span class="o">--</span><span class="kn">import</span> <span class="nn">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">jpackage</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">jpackage</span><span class="o">.</span><span class="n">asc</span> <span class="o">&amp;&amp;</span>   
   <span class="n">rpm</span> <span class="o">--</span><span class="kn">import</span> <span class="nn">https</span><span class="p">:</span><span class="o">//</span><span class="n">dl</span><span class="o">.</span><span class="n">fedoraproject</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">pub</span><span class="o">/</span><span class="n">epel</span><span class="o">/</span><span class="n">RPM</span><span class="o">-</span><span class="n">GPG</span><span class="o">-</span><span class="n">KEY</span><span class="o">-</span><span class="n">EPEL</span><span class="o">-</span><span class="mi">7</span> <span class="o">&amp;&amp;</span>   
   <span class="n">rpm</span> <span class="o">--</span><span class="kn">import</span> <span class="nn">http</span><span class="p">:</span><span class="o">//</span><span class="n">yum</span><span class="o">.</span><span class="n">spacewalkproject</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">RPM</span><span class="o">-</span><span class="n">GPG</span><span class="o">-</span><span class="n">KEY</span><span class="o">-</span><span class="n">spacewalk</span><span class="o">-</span><span class="mi">2015</span> <span class="o">&amp;&amp;</span>   
   <span class="n">yum</span> <span class="n">clean</span> <span class="nb">all</span>
</pre></div>


<p>Install spacewalk and supervisord packages</p>
<div class="highlight"><pre><span></span>RUN yum -y install   
       spacewalk-setup-postgresql   
       spacewalk-postgresql   
       supervisor    
       yum clean all
</pre></div>


<p>Copy the example file used to sync spacewalk database in a later step</p>
<div class="highlight"><pre><span></span>COPY answerfile.txt /tmp/answerfile.txt
</pre></div>


<p>Open necessary ports</p>
<div class="highlight"><pre><span></span>EXPOSE 80 443 5222 68 69
</pre></div>


<p>Change to postgres user</p>
<div class="highlight"><pre><span></span>USER postgres
</pre></div>


<p>Initialize the database</p>
<div class="highlight"><pre><span></span>RUN /usr/bin/pg_ctl initdb  -D /var/lib/pgsql/data/
</pre></div>


<p>Create spacewalk database, user, role and create pltclu language</p>
<div class="highlight"><pre><span></span>RUN /usr/bin/pg_ctl start -D /var/lib/pgsql/data/  -w -t 300 &amp;&amp;   
    psql -c &#39;CREATE DATABASE spaceschema&#39; &amp;&amp;   
    psql -c &quot;CREATE USER spaceuser WITH PASSWORD &#39;spacepw&#39;&quot; &amp;&amp;   
    psql -c &#39;ALTER ROLE spaceuser SUPERUSER&#39; &amp;&amp;   
    createlang pltclu spaceschema
</pre></div>


<p>Change to root user</p>
<div class="highlight"><pre><span></span>USER root
</pre></div>


<p>Start the database and execute spacewalk configuration script</p>
<div class="highlight"><pre><span></span>RUN su -c &quot;/usr/bin/pg_ctl start -D /var/lib/pgsql/data/  -w -t 300&quot; postgres &amp;&amp;   
   su -c &quot;spacewalk-setup --answer-file=/tmp/answerfile.txt --skip-db-diskspace-check --skip-db-install&quot; root ; exit 0
</pre></div>


<p>Copy supervisord configuration</p>
<div class="highlight"><pre><span></span>ADD supervisord.conf /etc/supervisord.d/supervisord.conf
</pre></div>


<p>Use supervisord command to start all services at container launch time</p>
<div class="highlight"><pre><span></span>ENTRYPOINT supervisord -c /etc/supervisord.d/supervisord.conf
</pre></div>


<p>You can check or download the source code at GitHub
<a href="https://github.com/egonzalez90/docker-spacewalk">https://github.com/egonzalez90/docker-spacewalk</a></p>
<p>I uploaded the image to DockerHub, which is auto-build from my GitHub
repository, you can find it with the following command.</p>
<div class="highlight"><pre><span></span>[egonzalez@localhost ~]$ docker search spacewalk
INDEX       NAME                                       DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED
docker.io   docker.io/ruo91/spacewalk                  Spacewalk is an open source Linux systems ...   3                    [OK]
docker.io   docker.io/jamesnetherton/spacewalk         Spacewalk running under Docker                  1                    
docker.io   docker.io/coffmant/spacewalk-docker        Spacewalk                                       0                    [OK]
docker.io   docker.io/csabakollar/spacewalk            Spacewalk 2.4 in a CentOS6 container            0                    
docker.io   docker.io/egonzalez90/spacewalk            Spacewalk docker image                          0                    [OK]
docker.io   docker.io/jdostal/spacewalk-clients        Repository containing spacewalk-clients         0                    
docker.io   docker.io/jhutar/spacewalk-client                                                          0                    
docker.io   docker.io/norus/spacewalk-reposync                                                         0                    
docker.io   docker.io/pajinek/spacewalk-client                                                         0                    [OK]
docker.io   docker.io/perfectweb/spacewalk             spacewalk                                       0                    [OK]
docker.io   docker.io/researchiteng/docker-spacewalk   spacewalk is the open source version of Re...   0                    [OK]
docker.io   docker.io/varhoo/spacewalk-proxy                                                           0                    [OK]
</pre></div>


<p>To start the container use the following command. If you don't have the
image locally, it will download the image from DockerHub</p>
<div class="highlight"><pre><span></span>[egonzalez@localhost ~]$ docker run -d --privileged=True egonzalez90/spacewalk
Unable to find image &#39;egonzalez90/spacewalk:latest&#39; locally
Trying to pull repository docker.io/egonzalez90/spacewalk ... 
latest: Pulling from docker.io/egonzalez90/spacewalk
a3ed95caeb02: Already exists 
da71393503ec: Already exists 
519093688e2c: Pull complete 
97bbffaa9fc9: Pull complete 
63bfb115f62d: Pull complete 
929bbb68aff9: Pull complete 
532bc4af8e1a: Pull complete 
3eb667dda9ee: Pull complete 
275894897aa4: Pull complete 
93bcddf9cedb: Pull complete 
266c3b70754f: Pull complete 
Digest: sha256:a4dd98548f9dbb405fb4c6bb4a2a07b83d5f2bf730f29f71913b72876b1a61ab
Status: Downloaded newer image for docker.io/egonzalez90/spacewalk:latest
ded4a8b7eb1ee61fecc8ddc2eb1b092917a361bc36f7f752b32d76e79501d70a
</pre></div>


<p>Now you have the container running, check if all the ports are properly
exposed</p>
<div class="highlight"><pre><span></span><span class="x">[egonzalez@localhost ~]$ docker ps --latest --format &#39;table </span><span class="cp">{{</span><span class="nv">.ID</span><span class="cp">}}</span><span class="x">\t</span><span class="cp">{{</span><span class="nv">.Image</span><span class="cp">}}</span><span class="x">\t</span><span class="cp">{{</span><span class="nv">.Ports</span><span class="cp">}}</span><span class="x">&#39;</span>
<span class="x">CONTAINER ID        IMAGE                   PORTS</span>
<span class="x">ded4a8b7eb1e        egonzalez90/spacewalk   68-69/tcp, 80/tcp, 443/tcp, 5222/tcp</span>
</pre></div>


<p>Get the container IP address in order to enter from a Web Browser</p>
<div class="highlight"><pre><span></span>[egonzalez@localhost ~]$ docker inspect ded4a8b7eb1e | egrep IPAddress
            &quot;SecondaryIPAddresses&quot;: null,
            &quot;IPAddress&quot;: &quot;172.17.0.3&quot;,
                    &quot;IPAddress&quot;: &quot;172.17.0.3&quot;,
</pre></div>


<p>Open A browser and go to the container IP address, if you use HTTP, by
default it will redirect you to HTTPS.<br>
The container uses an auto-signed SSL certificate, you have to add an
exception in the Browser you use to allow connections to Spacewalk.<br>
Once in the Welcome page, create an Organization.</p>
<p>Now you are in Spacewalk and can play/test some features.  </p>
<p><img alt="Selection_003" src="http://egonzalez.org/wp-content/uploads/2016/07/Selection_003-1024x483.png"></p>
<p>There is an issue I was not able to fix, so osa-dispatcher and some
other features will not work with this image.<br>
If someone can give me an input to fix the issue it will appreciated.</p>
<div class="highlight"><pre><span></span>[egonzalez@localhost ~]$ docker logs ded4a8b7eb1e | egrep FATAL
2016-07-12 18:13:32,220 INFO gave up: osa-dispatcher entered FATAL state, too many start retries too quickly
</pre></div>


<p>Thanks for your time and hopes this image at least serves you to learn
and play with the interface.</p>
<p>Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/spacewalk-red-hat-satellite-v5-in-a-docker-container-poc.html">posted at 20:57</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/linux-various.html" rel="tag">Linux, Various</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/container.html" class="tags">container</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/docker.html" class="tags">docker</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/dockerfile.html" class="tags">dockerfile</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/github.html" class="tags">github</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/image.html" class="tags">image</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/poc.html" class="tags">poc</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/proofofconcept.html" class="tags">proofofconcept</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/redhat.html" class="tags">redhat</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/satellite.html" class="tags">satellite</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/spacewalk.html" class="tags">spacewalk</a>
                </div>
		<a href="https://egonzalez90.github.io/spacewalk-red-hat-satellite-v5-in-a-docker-container-poc.html#disqus_thread">Click to read and post comments</a>
            </article>            <h4 class="date">Jul 04, 2016</h4>

            <article class="post">
                <h2 class="title">
                    <a href="https://egonzalez90.github.io/midonet-integration-with-openstack-mitaka.html" rel="bookmark" title="Permanent Link to &quot;MidoNet Integration with OpenStack Mitaka&quot;">MidoNet Integration with OpenStack Mitaka</a>
                </h2>

                
                

                <p>MidoNet is an Open Source network virtualization software for IaaS
infrastructure.<br>
It decouples your IaaS cloud from your network hardware, creating an
intelligent software abstraction layer between your end hosts and your
physical network.<br>
This network abstraction layer allows the cloud operator to move what
has traditionally been hardware-based network appliances into a
software-based multi-tenant virtual domain.</p>
<p>This definition from MidoNet documentation explains what MidoNet is and
what MidoNet does.</p>
<p>At this I will post cover my experiences integrating MidoNet with
OpenStack.<br>
I used the following configurations:</p>
<p>All servers have CentOS 7.2 installed<br>
OpenStack has been previously installed from RDO packages with
multinode Packstack</p>
<ul>
<li>
<p>x3 NSDB nodes (Casandra and Zookeeper services)</p>
</li>
<li>
<p>x2 Gateway Nodes (Midolman Agent)</p>
</li>
<li>
<p>x1 OpenStack Controller (MidoNet Cluster)</p>
</li>
<li>
<p>x1 OpenStack compute node (Midolman Agent)</p>
</li>
</ul>
<p><strong>NSDB NODES</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Cassandra repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/datastax.repo
[datastax]
name = DataStax Repo for Apache Cassandra
baseurl = http://rpm.datastax.com/community
enabled = 1
gpgcheck = 1
gpgkey = https://rpm.datastax.com/rpm/repo_key
EOF
</pre></div>


<p>Add Midonet repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repo cache and update packages</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p><strong>Zookeeper Configuration</strong><br>
Install Zookeeper, java and dependencies</p>
<div class="highlight"><pre><span></span>yum install -y java-1.7.0-openjdk-headless zookeeper zkdump nmap-ncat
</pre></div>


<p>Edit zookeeper configuration file</p>
<div class="highlight"><pre><span></span>vi /etc/zookeeper/zoo.cfg
</pre></div>


<p>Add all NSDB nodes at the configuration file</p>
<div class="highlight"><pre><span></span>server.1=nsdb1:2888:3888
server.2=nsdb2:2888:3888
server.3=nsdb3:2888:3888
autopurge.snapRetainCount=10
autopurge.purgeInterval =12
</pre></div>


<p>Create zookeeper folder on which zookeeper will store data, change the
owner to zookeeper user</p>
<div class="highlight"><pre><span></span>mkdir /var/lib/zookeeper/data
chown zookeeper:zookeeper /var/lib/zookeeper/data
</pre></div>


<p>Create myid file at zookeeper data folder, the ID should match with the
NSDB node number, insert that number as follows:</p>
<div class="highlight"><pre><span></span>#NSDB1
echo 1 &gt; /var/lib/zookeeper/data/myid
#NSDB2
echo 2 &gt; /var/lib/zookeeper/data/myid
#NSDB3
echo 3 &gt; /var/lib/zookeeper/data/myid
</pre></div>


<p>Create java folder and create a softlink to it</p>
<div class="highlight"><pre><span></span>mkdir -p /usr/java/default/bin/
ln -s /usr/lib/jvm/jre-1.7.0-openjdk/bin/java /usr/java/default/bin/java
</pre></div>


<p>Start and enable Zookeeper service</p>
<div class="highlight"><pre><span></span>systemctl enable zookeeper.service
systemctl start zookeeper.service
</pre></div>


<p>Test if zookeeper is working locally</p>
<div class="highlight"><pre><span></span>echo ruok | nc 127.0.0.1 2181
imok
</pre></div>


<p>Test if zookeeper is working at NSDB remote nodes</p>
<div class="highlight"><pre><span></span>echo stat | nc nsdb3 2181

Zookeeper version: 3.4.5--1, built on 02/08/2013 12:25 GMT
Clients:
 /192.168.100.172:35306[0](queued=0,recved=1,sent=0)

Latency min/avg/max: 0/0/0
Received: 1
Sent: 0
Connections: 1
Outstanding: 0
Zxid: 0x100000000
Mode: follower
Node count: 4
</pre></div>


<p><strong>Cassandra configuration</strong><br>
Install Java and Cassandra dependencies</p>
<div class="highlight"><pre><span></span>yum install -y java-1.8.0-openjdk-headless dsc22
</pre></div>


<p>Edit cassandra yaml file</p>
<div class="highlight"><pre><span></span>vi /etc/cassandra/conf/cassandra.yaml
</pre></div>


<p>Change cluster_name to midonet<br>
Configure seed_provider seeds to match all NSDB nodes<br>
Configure listen_address and rpc_address to match the hostname of the
self node</p>
<div class="highlight"><pre><span></span>cluster_name: &#39;midonet&#39;
....
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          - seeds: &quot;nsdb1,nsdb2,nsdb3&quot;

listen_address: nsdb1
rpc_address: nsdb1
</pre></div>


<p>Edit cassandra's init script in order to fix a bug in the init script</p>
<div class="highlight"><pre><span></span>vi /etc/init.d/cassandra
</pre></div>


<p>Add the next two lines after #Casandra startup</p>
<div class="highlight"><pre><span></span>case &quot;$1&quot; in
    start)
        # Cassandra startup
        echo -n &quot;Starting Cassandra: &quot;
        mkdir -p /var/run/cassandra #Add this line
        chown cassandra:cassandra /var/run/cassandra #Add this line
        su $CASSANDRA_OWNR -c &quot;$CASSANDRA_PROG -p $pid_file&quot; &gt; $log_file 2&gt;&amp;1
        retval=$?
        [ $retval -eq 0 ] &amp;&amp; touch $lock_file
        echo &quot;OK&quot;
        ;;
</pre></div>


<p>Start and enable Cassandra service</p>
<div class="highlight"><pre><span></span>systemctl enable cassandra.service
systemctl start cassandra.service
</pre></div>


<p>Check if all NSDB nodes join the cluster</p>
<div class="highlight"><pre><span></span>nodetool --host 127.0.0.1 status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address          Load       Tokens       Owns (effective)  Host ID                               Rack
UN  192.168.100.172  89.1 KB    256          70.8%             3f1ecedd-8caf-4938-84ad-8614d2134557  rack1
UN  192.168.100.224  67.64 KB   256          60.7%             cb36c999-a6e1-4d98-a4dd-d4230b41df08  rack1
UN  192.168.100.195  25.78 KB   256          68.6%             4758bae8-9300-4e57-9a61-5b1107082964  rack1
</pre></div>


<p><strong>Configure OpenStack Controller Nodes (On which Neutron Server is
running)</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Midonet Repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repos cache and update the system</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p>Create an OpenStack user for MidoNet, change the password to match your
own</p>
<div class="highlight"><pre><span></span># openstack user create --password temporal midonet
+----------+----------------------------------+
| Field    | Value                            |
+----------+----------------------------------+
| email    | None                             |
| enabled  | True                             |
| id       | ac25c5a77e7c4e4598ccadea89e09969 |
| name     | midonet                          |
| username | midonet                          |
+----------+----------------------------------+
</pre></div>


<p>Add admin role at tenant services to Midonet user</p>
<div class="highlight"><pre><span></span># openstack role add --project services --user midonet admin
+-----------+----------------------------------+
| Field     | Value                            |
+-----------+----------------------------------+
| domain_id | None                             |
| id        | bca2c6e1f3da42b0ba82aee401398a8a |
| name      | admin                            |
+-----------+----------------------------------+
</pre></div>


<p>Create MidoNet service at Keystone</p>
<div class="highlight"><pre><span></span># openstack service create --name midonet --description &quot;MidoNet API Service&quot; midonet
+-------------+----------------------------------+
| Field       | Value                            |
+-------------+----------------------------------+
| description | MidoNet API Service              |
| enabled     | True                             |
| id          | 499059c4a3a040cfb632411408a2be4c |
| name        | midonet                          |
| type        | midonet                          |
+-------------+----------------------------------+
</pre></div>


<p><strong>Clean up neutron server</strong><br>
Stop neutron services</p>
<div class="highlight"><pre><span></span>openstack-service stop neutron
</pre></div>


<p>Remove neutron database and recreate it again</p>
<div class="highlight"><pre><span></span>mysql -u root -p
DROP DATABASE neutron;
Query OK, 157 rows affected (11.50 sec)

MariaDB [(none)]&gt; CREATE DATABASE neutron;
Query OK, 1 row affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO &#39;neutron&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;ab4f81b1040a495e&#39;;
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON neutron.* TO &#39;neutron&#39;@&#39;%&#39; IDENTIFIED BY &#39;ab4f81b1040a495e&#39;;
Query OK, 0 rows affected (0.00 sec)
MariaDB [(none)]&gt; exit
Bye
</pre></div>


<p>Remove plugin.ini symbolic link to ml2_conf.ini</p>
<div class="highlight"><pre><span></span>#rm /etc/neutron/plugin.ini 
rm: remove symbolic link ‘/etc/neutron/plugin.ini’? y
</pre></div>


<p>Remove br-tun tunnel used by neutron in all the nodes</p>
<div class="highlight"><pre><span></span>ovs-vsctl del-br br-tun
</pre></div>


<p>Install MidoNet packages and remove ml2 package</p>
<div class="highlight"><pre><span></span>yum install -y openstack-neutron python-networking-midonet python-neutronclient
yum remove openstack-neutron-ml2
</pre></div>


<p>Make a backup of neutron configuration file</p>
<div class="highlight"><pre><span></span>cp /etc/neutron/neutron.conf neutron.conf.bak
</pre></div>


<p>Edit neutron configuration file</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/neutron.conf
</pre></div>


<p>Most of the options are already configured by our older neutron
configuration, change the ones who apply to match this configuration</p>
<div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">core_plugin</span> <span class="o">=</span> <span class="s">midonet.neutron.plugin_v2.MidonetPluginV2</span>
<span class="na">service_plugins</span> <span class="o">=</span> <span class="s">midonet.neutron.services.l3.l3_midonet.MidonetL3ServicePlugin</span>
<span class="na">dhcp_agent_notification</span> <span class="o">=</span> <span class="s">False</span>
<span class="na">allow_overlapping_ips</span> <span class="o">=</span> <span class="s">True</span>
<span class="na">rpc_backend</span> <span class="o">=</span> <span class="s">rabbit</span>
<span class="na">auth_strategy</span> <span class="o">=</span> <span class="s">keystone</span>
<span class="na">notify_nova_on_port_status_changes</span> <span class="o">=</span> <span class="s">true</span>
<span class="na">notify_nova_on_port_data_changes</span> <span class="o">=</span> <span class="s">true</span>
<span class="na">nova_url</span> <span class="o">=</span> <span class="s">http://controller:8774/v2</span>

<span class="k">[database]</span>
<span class="na">connection</span> <span class="o">=</span> <span class="s">mysql+pymysql://neutron:ab4f81b1040a495e@controller/neutron</span>

<span class="k">[oslo_messaging_rabbit]</span>
<span class="na">rabbit_host</span> <span class="o">=</span> <span class="s">controller</span>
<span class="na">rabbit_userid</span> <span class="o">=</span> <span class="s">guest</span>
<span class="na">rabbit_password</span> <span class="o">=</span> <span class="s">guest</span>

<span class="k">[keystone_authtoken]</span>
<span class="na">auth_uri</span> <span class="o">=</span> <span class="s">http://controller:5000/v2.0</span>
<span class="na">admin_user</span><span class="o">=</span><span class="s">neutron</span>
<span class="na">admin_tenant_name</span><span class="o">=</span><span class="s">services</span>
<span class="na">identity_uri</span><span class="o">=</span><span class="s">http://controller:35357</span>
<span class="na">admin_password</span><span class="o">=</span><span class="s">d88f0bd060d64c33</span>

<span class="k">[nova]</span>
<span class="na">region_name</span> <span class="o">=</span> <span class="s">RegionOne</span>
<span class="na">auth_url</span> <span class="o">=</span> <span class="s">http://controller:35357</span>
<span class="na">auth_type</span> <span class="o">=</span> <span class="s">password</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">9ca36d15e4824d93</span>
<span class="na">project_domain_id</span> <span class="o">=</span> <span class="s">default</span>
<span class="na">project_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">tenant_name</span> <span class="o">=</span> <span class="s">services</span>
<span class="na">user_domain_id</span> <span class="o">=</span> <span class="s">default</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">nova</span>

<span class="k">[oslo_concurrency]</span>
<span class="na">lock_path</span> <span class="o">=</span> <span class="s">/var/lib/neutron/tmp</span>
</pre></div>


<p>At my deployment these are the options I had to change to configure
midonet</p>
<div class="highlight"><pre><span></span><span class="gh">diff /etc/neutron/neutron.conf neutron.conf.bak </span>
33c33
&lt; core_plugin = midonet.neutron.plugin_v2.MidonetPluginV2
<span class="gd">---</span>
&gt; core_plugin = neutron.plugins.ml2.plugin.Ml2Plugin
37c37
&lt; service_plugins = midonet.neutron.services.l3.l3_midonet.MidonetL3ServicePlugin
<span class="gd">---</span>
&gt; service_plugins =router
120c120
&lt; dhcp_agent_notification = False
<span class="gd">---</span>
&gt; #dhcp_agent_notification = true
1087c1087,1088
&lt; lock_path = /var/lib/neutron/tmp
<span class="gd">---</span>
&gt; lock_path = $state_path/lock
&gt;
</pre></div>


<p>Create midonet plugins folder</p>
<div class="highlight"><pre><span></span>mkdir /etc/neutron/plugins/midonet
</pre></div>


<p>Create a file called midonet.ini</p>
<div class="highlight"><pre><span></span>vi /etc/neutron/plugins/midonet/midonet.ini
</pre></div>


<p>Configure midonet.ini file to match your own configuration options</p>
<div class="highlight"><pre><span></span><span class="k">[MIDONET]</span>
<span class="c1"># MidoNet API URL</span>
<span class="na">midonet_uri</span> <span class="o">=</span> <span class="s">http://controller:8181/midonet-api</span>
<span class="c1"># MidoNet administrative user in Keystone</span>
<span class="na">username</span> <span class="o">=</span> <span class="s">midonet</span>
<span class="na">password</span> <span class="o">=</span> <span class="s">temporal</span>
<span class="c1"># MidoNet administrative user&#39;s tenant</span>
<span class="na">project_id</span> <span class="o">=</span> <span class="s">services</span>
</pre></div>


<p>Create a symbolic link from midonet.ini to plugin.ini</p>
<div class="highlight"><pre><span></span>ln -s /etc/neutron/plugins/midonet/midonet.ini /etc/neutron/plugin.ini
</pre></div>


<p>Sync and populate database tables with Midonet plugin</p>
<div class="highlight"><pre><span></span>su -s /bin/sh -c &quot;neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/midonet/midonet.ini upgrade head&quot; neutron
su -s /bin/sh -c &quot;neutron-db-manage --subproject networking-midonet upgrade head&quot; neutron
</pre></div>


<p>Restart nova api and neutron server services</p>
<div class="highlight"><pre><span></span>systemctl restart openstack-nova-api.service
systemctl restart neutron-server
</pre></div>


<p>Install midonet cluster package</p>
<div class="highlight"><pre><span></span>yum install -y midonet-cluster
</pre></div>


<p>Configure midonet.conf file</p>
<div class="highlight"><pre><span></span>vi /etc/midonet/midonet.conf
</pre></div>


<p>Add all NSDB nodes at zookeeper_hosts</p>
<div class="highlight"><pre><span></span><span class="k">[zookeeper]</span>
<span class="na">zookeeper_hosts</span> <span class="o">=</span> <span class="s">nsdb1:2181,nsdb2:2181,nsdb3:2181</span>
</pre></div>


<p>Configure midonet to make use of NSDB nodes as Zookeeper and cassandra
hosts</p>
<div class="highlight"><pre><span></span>cat &lt;&lt; EOF | mn-conf set -t default
zookeeper {
    zookeeper_hosts = &quot;nsdb1:2181,nsdb2:2181,nsdb3:2181&quot;
}

cassandra {
    servers = &quot;nsdb1,nsdb2,nsdb3&quot;
}
EOF
</pre></div>


<p>Set cassandra replication factor to 3</p>
<div class="highlight"><pre><span></span>echo &quot;cassandra.replication_factor : 3&quot; | mn-conf set -t default
</pre></div>


<p>Grab your admin token</p>
<div class="highlight"><pre><span></span>#egrep ^admin_token /etc/keystone/keystone.conf 
admin_token = 7b84d89b32c34b71a697eb1a270807ab
</pre></div>


<p>Configure Midonet to auth with keystone</p>
<div class="highlight"><pre><span></span>cat &lt;&lt; EOF | mn-conf set -t default
cluster.auth {
    provider_class = &quot;org.midonet.cluster.auth.keystone.KeystoneService&quot;
    admin_role = &quot;admin&quot;
    keystone.tenant_name = &quot;admin&quot;
    keystone.admin_token = &quot;7b84d89b32c34b71a697eb1a270807ab&quot;
    keystone.host = controller
    keystone.port = 35357
}
EOF
</pre></div>


<p>Start and enable midonet cluster service</p>
<div class="highlight"><pre><span></span>systemctl enable midonet-cluster.service
systemctl start midonet-cluster.service
</pre></div>


<p>Install midonet CLI</p>
<div class="highlight"><pre><span></span>yum install -y python-midonetclient
</pre></div>


<p>Create a file at you home directory with midonet auth info</p>
<div class="highlight"><pre><span></span>#vi ~/.midonetrc

[cli]
api_url = http://controller:8181/midonet-api
username = admin
password = temporal
project_id = admin
</pre></div>


<p><strong>Configure Compute nodes</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Midonet repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repos cache and update the system</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p>Edit qemu.conf</p>
<div class="highlight"><pre><span></span>vi /etc/libvirt/qemu.conf
</pre></div>


<p>Configure with the following options, by default all these options are
commented, you can paste it all wherever you want</p>
<div class="highlight"><pre><span></span>user = &quot;root&quot;
group = &quot;root&quot;

cgroup_device_acl = [
    &quot;/dev/null&quot;, &quot;/dev/full&quot;, &quot;/dev/zero&quot;,
    &quot;/dev/random&quot;, &quot;/dev/urandom&quot;,
    &quot;/dev/ptmx&quot;, &quot;/dev/kvm&quot;, &quot;/dev/kqemu&quot;,
    &quot;/dev/rtc&quot;,&quot;/dev/hpet&quot;, &quot;/dev/vfio/vfio&quot;,
    &quot;/dev/net/tun&quot;
]
</pre></div>


<p>Restart libvirtd service</p>
<div class="highlight"><pre><span></span>systemctl restart libvirtd.service
</pre></div>


<p>Install nova-network package</p>
<div class="highlight"><pre><span></span>yum install -y openstack-nova-network
</pre></div>


<p>Disable Nova Network service and restart Nova compute service</p>
<div class="highlight"><pre><span></span>systemctl disable openstack-nova-network.service
systemctl restart openstack-nova-compute.service
</pre></div>


<p>Install Midolman agent and java packages</p>
<div class="highlight"><pre><span></span>yum install -y java-1.8.0-openjdk-headless midolman
</pre></div>


<p>Configure midolman.conf</p>
<div class="highlight"><pre><span></span>vi /etc/midolman/midolman.conf
</pre></div>


<p>Add all nsdb nodes as zookeeper hosts</p>
<div class="highlight"><pre><span></span><span class="k">[zookeeper]</span>
<span class="na">zookeeper_hosts</span> <span class="o">=</span> <span class="s">nsdb1:2181,nsdb2:2181,nsdb3:2181</span>
</pre></div>


<p>Configure each compute node with an appropiate flavor located at
/etc/midolman/ folder, the have different hardware resources configured,
use the one that better match your compute host capabilities</p>
<div class="highlight"><pre><span></span>mn-conf template-set -h local -t agent-compute-medium
cp /etc/midolman/midolman-env.sh.compute.medium /etc/midolman/midolman-env.sh
</pre></div>


<p>Configure metadata, issue the following commands only once, it will
automatically populate the configuration to all midonet agents</p>
<div class="highlight"><pre><span></span>echo &quot;agent.openstack.metadata.nova_metadata_url : \&quot;http://controller:8775\&quot;&quot; | mn-conf set -t default
echo &quot;agent.openstack.metadata.shared_secret : 2bfeb930a90d435d&quot; | mn-conf set -t default
echo &quot;agent.openstack.metadata.enabled : true&quot; | mn-conf set -t default
</pre></div>


<p>Allow metadata trafic at iptables</p>
<div class="highlight"><pre><span></span>iptables -I INPUT 1 -i metadata -j ACCEPT
</pre></div>


<p>Remove br-tun bridge</p>
<div class="highlight"><pre><span></span>ovs-vsctl del-br br-tun
</pre></div>


<p>Start and enable midolman agent service</p>
<div class="highlight"><pre><span></span>systemctl enable midolman.service
systemctl start midolman.service
</pre></div>


<p><strong>Gateway nodes configuration</strong></p>
<p>Disable SElinux</p>
<div class="highlight"><pre><span></span>setenforce 0
sed -i &#39;s/SELINUX=enforcing/SELINUX=permissive/g&#39; /etc/sysconfig/selinux
</pre></div>


<p>Install OpenStack Mitaka release repository</p>
<div class="highlight"><pre><span></span>sudo yum install -y centos-release-openstack-mitaka
</pre></div>


<p>Add Midonet repository</p>
<div class="highlight"><pre><span></span>cat &lt;&lt;EOF&gt;/etc/yum.repos.d/midonet.repo
[midonet]
name=MidoNet
baseurl=http://builds.midonet.org/midonet-5.2/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-openstack-integration]
name=MidoNet OpenStack Integration
baseurl=http://builds.midonet.org/openstack-mitaka/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key

[midonet-misc]
name=MidoNet 3rd Party Tools and Libraries
baseurl=http://builds.midonet.org/misc/stable/el7/
enabled=1
gpgcheck=1
gpgkey=https://builds.midonet.org/midorepo.key
EOF
</pre></div>


<p>Clean repos cache and update the system</p>
<div class="highlight"><pre><span></span>yum clean all
yum update
</pre></div>


<p>Install Midolman agent and java packages</p>
<div class="highlight"><pre><span></span>yum install -y java-1.8.0-openjdk-headless midolman
</pre></div>


<p>Configure midolman.conf</p>
<div class="highlight"><pre><span></span>vi /etc/midolman/midolman.conf
</pre></div>


<p>Add all nsdb nodes as zookeeper hosts</p>
<div class="highlight"><pre><span></span><span class="k">[zookeeper]</span>
<span class="na">zookeeper_hosts</span> <span class="o">=</span> <span class="s">nsdb1:2181,nsdb2:2181,nsdb3:2181</span>
</pre></div>


<p>Configure each gateway node with an appropiate flavor located at
/etc/midolman/ folder, the have different hardware resources configured,
use the one that better match your gateway host capabilities</p>
<div class="highlight"><pre><span></span>mn-conf template-set -h local -t agent-gateway-medium
cp /etc/midolman/midolman-env.sh.gateway.medium /etc/midolman/midolman-env.sh
</pre></div>


<p>Grab the metadata shared secret located at nova.conf at any of your nova
nodes</p>
<div class="highlight"><pre><span></span># egrep ^metadata_proxy_shared_secret /etc/nova/nova.conf 
metadata_proxy_shared_secret =2bfeb930a90d435d
</pre></div>


<p>Allow metadata trafic at iptables</p>
<div class="highlight"><pre><span></span>iptables -I INPUT 1 -i metadata -j ACCEPT
</pre></div>


<p>Start and enable midolman agent service</p>
<div class="highlight"><pre><span></span>systemctl enable midolman.service
systemctl start midolman.service
</pre></div>


<p><strong>Configure encapsulation and register nodes</strong><br>
Enter to midonet CLI from a controller node</p>
<div class="highlight"><pre><span></span>midonet-cli
</pre></div>


<p>Create the tunnel zone with VXLAN encapsulation</p>
<div class="highlight"><pre><span></span>midonet&gt; tunnel-zone create name tz type vxlan
tzone0
midonet&gt; list tunnel-zone
tzone tzone0 name tz type vxlan
</pre></div>


<p>List hosts discovered by midonet, should be all the nodes where you
configured midonet agents(midolman)</p>
<div class="highlight"><pre><span></span>midonet&gt; list host
host host0 name gateway2 alive true addresses fe80:0:0:0:0:11ff:fe00:1102,169.254.123.1,fe80:0:0:0:0:11ff:fe00:1101,127.0.0.1,0:0:0:0:0:0:0:1,192.168.200.176,fe80:0:0:0:5054:ff:fef9:b2a0,169.254.169.254,fe80:0:0:0:7874:d6ff:fe5b:dea8,192.168.100.227,fe80:0:0:0:5054:ff:fed9:9cc0,fe80:0:0:0:5054:ff:fe4a:e39b,192.168.1.86 flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
host host1 name gateway1 alive true addresses 169.254.169.254,fe80:0:0:0:3cd1:23ff:feac:a3c2,192.168.1.87,fe80:0:0:0:5054:ff:fea8:da91,127.0.0.1,0:0:0:0:0:0:0:1,fe80:0:0:0:5054:ff:feec:92c1,192.168.200.232,fe80:0:0:0:0:11ff:fe00:1102,169.254.123.1,fe80:0:0:0:0:11ff:fe00:1101,192.168.100.141,fe80:0:0:0:5054:ff:fe20:30fb flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
host host2 name compute1 alive true addresses fe80:0:0:0:0:11ff:fe00:1101,169.254.123.1,127.0.0.1,0:0:0:0:0:0:0:1,fe80:0:0:0:0:11ff:fe00:1102,192.168.100.173,fe80:0:0:0:5054:ff:fe06:161,fe80:0:0:0:5054:ff:fee3:eb48,192.168.200.251,fe80:0:0:0:5054:ff:fe8d:d22,192.168.1.93,169.254.169.254,fe80:0:0:0:48cb:adff:fe69:f07b flooding-proxy-weight 1 container-weight 1 container-limit no-limit enforce-container-limit false
</pre></div>


<p>Register each of the nodes at the VXLAN zone we created before</p>
<div class="highlight"><pre><span></span>midonet&gt; tunnel-zone tzone0 add member host host0 address 192.168.100.227
zone tzone0 host host0 address 192.168.100.227
midonet&gt; tunnel-zone tzone0 add member host host1 address 192.168.100.141
zone tzone0 host host1 address 192.168.100.141
midonet&gt; tunnel-zone tzone0 add member host host2 address 192.168.100.173
zone tzone0 host host2 address 192.168.100.173
</pre></div>


<p><strong>Create Networks at Neutron</strong><br>
Create an external network</p>
<div class="highlight"><pre><span></span># neutron net-create ext-net --router:external
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:47:30                  |
| description           |                                      |
| id                    | dc15245e-4391-4514-b489-8976373046a3 |
| is_default            | False                                |
| name                  | ext-net                              |
| port_security_enabled | True                                 |
| provider:network_type | midonet                              |
| router:external       | True                                 |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
| updated_at            | 2016-07-03T14:47:30                  |
+-----------------------+--------------------------------------+
</pre></div>


<p>Create an external subnet in the network we created before, use you own
IP ranges to match your environment</p>
<div class="highlight"><pre><span></span># neutron subnet-create ext-net 192.168.200.0/24 --name ext-subnet   
 --allocation-pool start=192.168.200.225,end=192.168.200.240   
 --disable-dhcp --gateway 192.168.200.1
Created a new subnet:
+-------------------+--------------------------------------------------------+
| Field             | Value                                                  |
+-------------------+--------------------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;192.168.200.225&quot;, &quot;end&quot;: &quot;192.168.200.240&quot;} |
| cidr              | 192.168.200.0/24                                       |
| created_at        | 2016-07-03T14:50:46                                    |
| description       |                                                        |
| dns_nameservers   |                                                        |
| enable_dhcp       | False                                                  |
| gateway_ip        | 192.168.200.1                                          |
| host_routes       |                                                        |
| id                | 234dcc9a-2878-4799-b564-bf3a1bd52cad                   |
| ip_version        | 4                                                      |
| ipv6_address_mode |                                                        |
| ipv6_ra_mode      |                                                        |
| name              | ext-subnet                                             |
| network_id        | dc15245e-4391-4514-b489-8976373046a3                   |
| subnetpool_id     |                                                        |
| tenant_id         | 2f7ee2716b3b4140be57b4a5b26401e3                       |
| updated_at        | 2016-07-03T14:50:46                                    |
+-------------------+--------------------------------------------------------+
</pre></div>


<p>Create a tenant network and a subnet on it</p>
<div class="highlight"><pre><span></span># neutron net-create demo-net
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:51:39                  |
| description           |                                      |
| id                    | 075ba699-dc4c-4625-8e0d-0a258a9aeb7d |
| name                  | demo-net                             |
| port_security_enabled | True                                 |
| provider:network_type | midonet                              |
| router:external       | False                                |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
| updated_at            | 2016-07-03T14:51:39                  |
+-----------------------+--------------------------------------+
# neutron subnet-create demo-net 10.0.20.0/24 --name demo-subnet 
Created a new subnet:
+-------------------+----------------------------------------------+
| Field             | Value                                        |
+-------------------+----------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;10.0.20.2&quot;, &quot;end&quot;: &quot;10.0.20.254&quot;} |
| cidr              | 10.0.20.0/24                                 |
| created_at        | 2016-07-03T14:52:32                          |
| description       |                                              |
| dns_nameservers   |                                              |
| enable_dhcp       | True                                         |
| gateway_ip        | 10.0.20.1                                    |
| host_routes       |                                              |
| id                | b299d899-33a3-4bfa-aff4-fda071545bdf         |
| ip_version        | 4                                            |
| ipv6_address_mode |                                              |
| ipv6_ra_mode      |                                              |
| name              | demo-subnet                                  |
| network_id        | 075ba699-dc4c-4625-8e0d-0a258a9aeb7d         |
| subnetpool_id     |                                              |
| tenant_id         | 2f7ee2716b3b4140be57b4a5b26401e3             |
| updated_at        | 2016-07-03T14:52:32                          |
+-------------------+----------------------------------------------+
</pre></div>


<p>Create a tenant router</p>
<div class="highlight"><pre><span></span># neutron router-create router1
Created a new router:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| description           |                                      |
| external_gateway_info |                                      |
| id                    | 258942d8-9d82-4ebd-b829-c7bdfcc973f5 |
| name                  | router1                              |
| routes                |                                      |
| status                | ACTIVE                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
+-----------------------+--------------------------------------+
</pre></div>


<p>Attach the tenant subnet interface we created before to the router</p>
<div class="highlight"><pre><span></span># neutron router-interface-add router1 demo-subnet
Added interface 06c85a56-368c-4d79-bbf0-4bb077f163e5 to router router1.
</pre></div>


<p>Set the external network as router gateway</p>
<div class="highlight"><pre><span></span># neutron router-gateway-set router1 ext-net
Set gateway for router router1
</pre></div>


<p>Now, you can create an instance at tenant network</p>
<div class="highlight"><pre><span></span># nova boot --flavor m1.tiny --image 80871834-29dd-4100-b038-f5f83f126204 --nic net-id=075ba699-dc4c-4625-8e0d-0a258a9aeb7d test1
+--------------------------------------+-----------------------------------------------------+
| Property                             | Value                                               |
+--------------------------------------+-----------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                              |
| OS-EXT-AZ:availability_zone          |                                                     |
| OS-EXT-SRV-ATTR:host                 | -                                                   |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                                   |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000000a                                   |
| OS-EXT-STS:power_state               | 0                                                   |
| OS-EXT-STS:task_state                | scheduling                                          |
| OS-EXT-STS:vm_state                  | building                                            |
| OS-SRV-USG:launched_at               | -                                                   |
| OS-SRV-USG:terminated_at             | -                                                   |
| accessIPv4                           |                                                     |
| accessIPv6                           |                                                     |
| adminPass                            | q2Cq4kxePSLL                                        |
| config_drive                         |                                                     |
| created                              | 2016-07-03T15:46:19Z                                |
| flavor                               | m1.tiny (1)                                         |
| hostId                               |                                                     |
| id                                   | b8aa46f9-186c-4594-8428-f8dbb16a5e16                |
| image                                | cirros image (80871834-29dd-4100-b038-f5f83f126204) |
| key_name                             | -                                                   |
| metadata                             | {}                                                  |
| name                                 | test1                                               |
| os-extended-volumes:volumes_attached | []                                                  |
| progress                             | 0                                                   |
| security_groups                      | default                                             |
| status                               | BUILD                                               |
| tenant_id                            | 2f7ee2716b3b4140be57b4a5b26401e3                    |
| updated                              | 2016-07-03T15:46:20Z                                |
| user_id                              | a2482a91a1f14750b372445d28b07c75                    |
+--------------------------------------+-----------------------------------------------------+
# nova list
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| ID                                   | Name  | Status | Task State | Power State | Networks            |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| b8aa46f9-186c-4594-8428-f8dbb16a5e16 | test1 | ACTIVE | -          | Running     | demo-net=10.0.20.11 |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
</pre></div>


<p>Ensure the instance gets IP and the metadata service is properly running</p>
<div class="highlight"><pre><span></span># nova console-log test1
...#Snipp from the output
Sending discover...
Sending select for 10.0.20.11...
Lease of 10.0.20.11 obtained, lease time 86400
cirros-ds &#39;net&#39; up at 7.92
checking http://169.254.169.254/2009-04-04/instance-id
successful after 1/20 tries: up 8.22. iid=i-0000000a
...
</pre></div>


<p>If you login to the instance through VNC you should be able to ping
another instances</p>
<p><strong>Edge router configuration</strong><br>
Create a new router</p>
<div class="highlight"><pre><span></span># neutron router-create edge-router
Created a new router:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| description           |                                      |
| external_gateway_info |                                      |
| id                    | 5ecadb64-cb0d-4f95-a00e-aa1dd20a2012 |
| name                  | edge-router                          |
| routes                |                                      |
| status                | ACTIVE                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3     |
+-----------------------+--------------------------------------+
</pre></div>


<p>Attach the external subnet interface to the router</p>
<div class="highlight"><pre><span></span># neutron router-interface-add edge-router ext-subnet
Added interface e37f1986-c6b1-47f4-8268-02b837ceac17 to router edge-router.
</pre></div>


<p>Create an uplink network</p>
<div class="highlight"><pre><span></span># neutron net-create uplink-network --tenant_id admin --provider:network_type uplink
Created a new network:
+-----------------------+--------------------------------------+
| Field                 | Value                                |
+-----------------------+--------------------------------------+
| admin_state_up        | True                                 |
| created_at            | 2016-07-03T14:57:15                  |
| description           |                                      |
| id                    | 77173ed4-6106-4515-af1c-3683897955f9 |
| name                  | uplink-network                       |
| port_security_enabled | True                                 |
| provider:network_type | uplink                               |
| router:external       | False                                |
| shared                | False                                |
| status                | ACTIVE                               |
| subnets               |                                      |
| tags                  |                                      |
| tenant_id             | admin                                |
| updated_at            | 2016-07-03T14:57:15                  |
+-----------------------+--------------------------------------+
</pre></div>


<p>Create a subnet in the uplink network</p>
<div class="highlight"><pre><span></span># neutron subnet-create --tenant_id admin --disable-dhcp --name uplink-subnet uplink-network 192.168.1.0/24
Created a new subnet:
+-------------------+--------------------------------------------------+
| Field             | Value                                            |
+-------------------+--------------------------------------------------+
| allocation_pools  | {&quot;start&quot;: &quot;192.168.1.2&quot;, &quot;end&quot;: &quot;192.168.1.254&quot;} |
| cidr              | 192.168.1.0/24                                   |
| created_at        | 2016-07-03T15:06:28                              |
| description       |                                                  |
| dns_nameservers   |                                                  |
| enable_dhcp       | False                                            |
| gateway_ip        | 192.168.1.1                                      |
| host_routes       |                                                  |
| id                | 4e98e789-20d3-45fd-a3b5-9bcf02d8a832             |
| ip_version        | 4                                                |
| ipv6_address_mode |                                                  |
| ipv6_ra_mode      |                                                  |
| name              | uplink-subnet                                    |
| network_id        | 77173ed4-6106-4515-af1c-3683897955f9             |
| subnetpool_id     |                                                  |
| tenant_id         | admin                                            |
| updated_at        | 2016-07-03T15:06:28                              |
+-------------------+--------------------------------------------------+
</pre></div>


<p>Create a port for each of the gateway nodes, interface should match with
the NIC you want to use for binding the gateway nodes and a IP address
for the same purposes</p>
<div class="highlight"><pre><span></span># neutron port-create uplink-network --binding:host_id gateway1 --binding:profile type=dict interface_name=eth1 --fixed-ip ip_address=192.168.1.199
Created a new port:
+-----------------------+--------------------------------------------------------------------------------------+
| Field                 | Value                                                                                |
+-----------------------+--------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                 |
| allowed_address_pairs |                                                                                      |
| binding:host_id       | compute1                                                                             |
| binding:profile       | {&quot;interface_name&quot;: &quot;eth1&quot;}                                                           |
| binding:vif_details   | {&quot;port_filter&quot;: true}                                                                |
| binding:vif_type      | midonet                                                                              |
| binding:vnic_type     | normal                                                                               |
| created_at            | 2016-07-03T15:10:06                                                                  |
| description           |                                                                                      |
| device_id             |                                                                                      |
| device_owner          |                                                                                      |
| extra_dhcp_opts       |                                                                                      |
| fixed_ips             | {&quot;subnet_id&quot;: &quot;4e98e789-20d3-45fd-a3b5-9bcf02d8a832&quot;, &quot;ip_address&quot;: &quot;192.168.1.199&quot;} |
| id                    | 7b4f54dd-2b41-42ba-9c5c-cda4640dc550                                                 |
| mac_address           | fa:16:3e:44:a8:c9                                                                    |
| name                  |                                                                                      |
| network_id            | 77173ed4-6106-4515-af1c-3683897955f9                                                 |
| port_security_enabled | True                                                                                 |
| security_groups       | 0cf3e33e-dbd6-4b42-a0bd-6679b5eed4e1                                                 |
| status                | ACTIVE                                                                               |
| tenant_id             | 2f7ee2716b3b4140be57b4a5b26401e3                                                     |
| updated_at            | 2016-07-03T15:10:06                                                                  |
+-----------------------+--------------------------------------------------------------------------------------+
</pre></div>


<p>Attach each of the ports to the edge router</p>
<div class="highlight"><pre><span></span># neutron router-interface-add edge-router port=7b4f54dd-2b41-42ba-9c5c-cda4640dc550
Added interface 7b4f54dd-2b41-42ba-9c5c-cda4640dc550 to router edge-router.
</pre></div>


<p>At this point you have to decide if use border routers with BGP enabled
or static routes.<br>
Use one of the following links to configure your use case:  </p>
<p><a href="https://docs.midonet.org/docs/latest/operations-guide/content/bgp_uplink_configuration.html">https://docs.midonet.org/docs/latest/operations-guide/content/bgp_uplink_configuration.html</a>  </p>
<p><a href="https://docs.midonet.org/docs/latest/operations-guide/content/static_setup.html">https://docs.midonet.org/docs/latest/operations-guide/content/static_setup.html</a></p>
<p><strong>Issues I faced during configuration of Midonet</strong></p>
<p>Midolman agent don't start:<br>
It was caused because midolman-env.sh file has more RAM configured as
the one of my server.<br>
Edit the file to match your server resources</p>
<div class="highlight"><pre><span></span># egrep ^MAX_HEAP_SIZE /etc/midolman/midolman-env.sh
MAX_HEAP_SIZE=&quot;2048M&quot;
</pre></div>


<p>Instances doesn't boot with the following error:</p>
<div class="highlight"><pre><span></span>could not open /dev/net/tun: Permission denied
</pre></div>


<p>I had to remove br-tun bridges at ovs, if not, ovs locks the device and
midolman cannot create the tunnel beetwen compute nodes and gateway
nodes.</p>
<div class="highlight"><pre><span></span>ovs-vsctl del-br br-tun
</pre></div>


<p>This post is my experience integrating Midonet into OpenStack, maybe
some things are not correct, if you find any issue, please advise me to
fix it.<br>
Regards, Eduardo Gonzalez</p>
                <div class="clear"></div>

                <div class="info">
                    <a href="https://egonzalez90.github.io/midonet-integration-with-openstack-mitaka.html">posted at 21:48</a>
                    &nbsp;&middot;&nbsp;<a href="https://egonzalez90.github.io/category/linux-openstack-various.html" rel="tag">Linux, OpenStack, Various</a>
                    &nbsp;&middot;
                    &nbsp;<a href="https://egonzalez90.github.io/tag/cloud.html" class="tags">cloud</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/config.html" class="tags">config</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/install.html" class="tags">install</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/integration.html" class="tags">integration</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/manual.html" class="tags">manual</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/midokura.html" class="tags">midokura</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/midonet.html" class="tags">midonet</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/mitaka.html" class="tags">mitaka</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/network.html" class="tags">network</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/openstack.html" class="tags">openstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/packages.html" class="tags">packages</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/packstack.html" class="tags">packstack</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/rdo.html" class="tags">rdo</a>
                    &nbsp;<a href="https://egonzalez90.github.io/tag/sdn.html" class="tags">sdn</a>
                </div>
		<a href="https://egonzalez90.github.io/midonet-integration-with-openstack-mitaka.html#disqus_thread">Click to read and post comments</a>
            </article>

                <div class="clear"></div>
                <div class="pages">

                    <a href="https://egonzalez90.github.io/" class="prev_page">&larr;&nbsp;Previous</a>
                    <a href="https://egonzalez90.github.io/page/3" class="next_page">Next&nbsp;&rarr;</a>
                    <span>Page 2 of 32</span>
                </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="https://egonzalez90.github.io/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>
</html>